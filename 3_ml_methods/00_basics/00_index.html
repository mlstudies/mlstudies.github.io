

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>1. Preliminaries &mdash; Study Notes in Machine Learning 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/coloring.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/eqposfix.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2. Basic Neural Networks" href="../02_neural_networks/00_index.html" />
    <link rel="prev" title="Study Notes in Machine Learning" href="../../index.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> Study Notes in Machine Learning
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">1. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#probability-theory-overview">1.1. Probability Theory Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#covariance-matrix">1.2. Covariance Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#property-1-1-classic-representation-of-covariance-by-expectation-or-mean">1.2.1. <span class="ititle">Property 1-1.</span> <span class="bemp">Classic representation of covariance by expectation (or mean).</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#property-1-2-invariance-to-centralization">1.2.2. <span class="ititle">Property 1-2.</span> <span class="bemp">Invariance to centralization.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-1-matrix-arithmetics-of-covariance-matrix">1.2.3. <span class="ititle">Theorem 1-1.</span> <span class="bemp">Matrix arithmetics of covariance matrix.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-2-positive-definiteness-of-covariance-matrix">1.2.4. <span class="ititle">Theorem 1-2.</span> <span class="bemp">Positive definiteness of covariance matrix.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#property-1-3-sample-covariance-represneted-by-rank-1-sum">1.2.5. <span class="ititle">Property 1-3.</span> <span class="bemp">Sample covariance represneted by rank-1 sum.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-3-block-decomposition-of-covariance-matrix">1.2.6. <span class="ititle">Theorem 1-3.</span> <span class="bemp">Block decomposition of covariance matrix.</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#multivariate-gaussian-distribution">1.3. Multivariate Gaussian Distribution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-1-gaussian-maximum-likelihood-estimators">1.3.1. <span class="ititle">Theorem 1-1.</span> <span class="bemp">Gaussian maximum likelihood estimators.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-2-bias-of-gaussian-mle">1.3.2. <span class="ititle">Theorem 1-2.</span> <span class="bemp">Bias of Gaussian MLE.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#property-1-1-shape-of-contours-of-gaussian-density">1.3.3. <span class="ititle">Property 1-1.</span> <span class="bemp">Shape of contours of Gaussian density.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-3-conditional-density-of-multivariate-guassian">1.3.4. <span class="ititle">Theorem 1-3.</span> <span class="bemp">Conditional density of multivariate Guassian.</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../02_neural_networks/00_index.html">2. Basic Neural Networks</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Study Notes in Machine Learning</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</div></li><li>1. Preliminaries</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/3_ml_methods/00_basics/00_index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["color.js","autoload-all.js"] }
  });

      MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
   var color = MathJax.Extension["TeX/color"];
   color.colors["theorem"] = color.getColor('RGB','255,229,153');
       color.colors["result"] = color.getColor('RGB','189,214,238');
       color.colors["fact"] = color.getColor('RGB','255,255,204');
       color.colors["emperical"] = color.getColor('RGB','253,240,207');
       color.colors["comment"] = color.getColor('RGB','204,255,204');
   color.colors["thm"] = color.getColor('RGB','255,229,153');
       color.colors["rlt"] = color.getColor('RGB','189,214,238');
       color.colors["emp"] = color.getColor('RGB','253,240,207');
       color.colors["comm"] = color.getColor('RGB','204,255,204');
       color.colors["conn1"] = color.getColor('RGB','255,0,255');
       color.colors["conn2"] = color.getColor('RGB','237,125,49');
       color.colors["conn3"] = color.getColor('RGB','112,48,160');
      });
</script><div class="section" id="preliminaries">
<h1>1. Preliminaries<a class="headerlink" href="#preliminaries" title="Permalink to this headline">¶</a></h1>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["color.js","autoload-all.js"] }
  });

      MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
   var color = MathJax.Extension["TeX/color"];
   color.colors["theorem"] = color.getColor('RGB','255,229,153');
       color.colors["result"] = color.getColor('RGB','189,214,238');
       color.colors["fact"] = color.getColor('RGB','255,255,204');
       color.colors["emperical"] = color.getColor('RGB','253,240,207');
       color.colors["comment"] = color.getColor('RGB','204,255,204');
   color.colors["thm"] = color.getColor('RGB','255,229,153');
       color.colors["rlt"] = color.getColor('RGB','189,214,238');
       color.colors["emp"] = color.getColor('RGB','253,240,207');
       color.colors["comm"] = color.getColor('RGB','204,255,204');
       color.colors["conn1"] = color.getColor('RGB','255,0,255');
       color.colors["conn2"] = color.getColor('RGB','237,125,49');
       color.colors["conn3"] = color.getColor('RGB','112,48,160');
      });
</script><div class="section" id="probability-theory-overview">
<h2>1.1. Probability Theory Overview<a class="headerlink" href="#probability-theory-overview" title="Permalink to this headline">¶</a></h2>
<p><span style="padding-left:20px"></span> Modern probability theory is built upon measure theory for rigorous
insight of the nature of probabilities, where even an introductory
course needs deep mathematical background and hundreds of pages, way
beyond the scope of this text. Here we overview basic concepts and
results that could help better understand machine learning models that
heavily involve probabilities, like Gaussian process or generative models.</p>
<p><span style="padding-left:20px"></span> In probability theory, a set <span class="math notranslate nohighlight">\(\Omega\)</span> containing all possible outcomes
of an  <span><span class="exdef"> <span class="target" id="index-0"></span>experiment</span></span> is called a  <span><span class="exdef"> <span class="target" id="index-1"></span>sample space</span></span>. For example, in the
experiment of coin flip, we can define <span class="math notranslate nohighlight">\(\Omega = \left\{ 0,1 \right\}\)</span>
where <span class="math notranslate nohighlight">\(0\)</span> represents head and <span class="math notranslate nohighlight">\(1\)</span> represents tail. <span class="math notranslate nohighlight">\(\Omega\)</span> can be
finite, countably infinite or uncountably infinite. <span class="emp">Note</span>
this “experiment” is not a concrete “experiment” we do in machine
learning to measure performance of a model; rather it is an abstract
concept, and can be mathematically viewed as being defined by the sample
space – if we know what the sample space is, then the “experiment” is
completely determined. For example, if we let
<span class="math notranslate nohighlight">\(\Omega = \left\{ 0,1 \right\}\)</span>, then the experiment behind this
<span class="math notranslate nohighlight">\(\Omega\)</span> is mathematically equivalent to the coin-flip experiment.</p>
<p><span style="padding-left:20px"></span> In addition, we define a set <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> that contains subsets of
<span class="math notranslate nohighlight">\(\Omega\)</span> and is named  <span><span class="exdef"> <span class="target" id="index-2"></span>𝜎-algebra</span></span>. It must satisfy three axioms: <span><span class="fact-highlight"> 1)
<span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is non-empty; 2) for any <span class="math notranslate nohighlight">\(A\in \mathcal{ F}\)</span> then
<span class="math notranslate nohighlight">\(\Omega\backslash A\in \mathcal{ F}\)</span>; 3) for countable many
<span class="math notranslate nohighlight">\(A_{\mathrm{1}},A_{2},\text{...},A_{n}\mathcal{,\ldots \in F}\)</span>, then
<span class="math notranslate nohighlight">\(\bigcup A_{k}\in \mathcal{ F}\)</span> </span></span>. The axioms are actually very intuitive
if we view 𝜎-algebra as being “modeling”  <span><span class="exdef"> <span class="target" id="index-3"></span>partitions</span></span> of <span class="math notranslate nohighlight">\(\Omega\)</span> and
“merge” of these partitions. For example, if we have a finite partition
<span class="math notranslate nohighlight">\(\Omega_{1},\ldots,\Omega_{n}\)</span> of <span class="math notranslate nohighlight">\(\Omega\)</span> s.t.
<span class="math notranslate nohighlight">\(\Omega_{i}\bigcap\Omega_{j} = \varnothing,\forall i \neq j\)</span> and
<span class="math notranslate nohighlight">\(\bigcup_{i = 1}^{n}\Omega_{i} = \Omega\)</span>, then the set containing all
possible unions of any subset of <span class="math notranslate nohighlight">\(\Omega_{1},\ldots,\Omega_{n}\)</span>, plus
the empty set, is a 𝜎-algebra. For another example, the  <span><span class="exdef"> <span class="target" id="index-4"></span>power set</span></span> of
<span class="math notranslate nohighlight">\(\Omega\)</span> is a 𝜎-algebra. Each set in <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is referred to as an  <span><span class="exdef"> <span class="target" id="index-5"></span>event</span></span>. An event is also called a  <span><span class="exdef"> <span class="target" id="index-6"></span>measurable set</span></span> in mathematical
text. The pair <span class="math notranslate nohighlight">\(\left( \Omega,\mathcal{F} \right)\)</span> a  <span><span class="exdef"> <span class="target" id="index-7"></span>measurable space</span></span>.</p>
<p><span style="padding-left:20px"></span> A  <span><span class="exdef"> <span class="target" id="index-8"></span>measure</span></span> is mathematical concept that rigorously models essential
real-life concepts like length, area, volume, etc. The most widely used
measure is called  <span><span class="exdef"> <span class="target" id="index-9"></span>Lebesgue measure</span></span>, which is consistent with our
common sense. Under this measure, the length of a line, the area of a
square, or the volume of a ball, etc. is calculated with our usual
formulas. <span><span class="fact-highlight"> All ordinary integrations we learned in calculus are w.r.t. Lebesgue measure </span></span>. A measure <span class="math notranslate nohighlight">\(μ\)</span> is formally defined over a 𝜎-algebra <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> as
a <span class="math notranslate nohighlight">\(\mathcal{F} \rightarrow \lbrack 0, + \infty)\)</span> function s.t. <span><span class="fact-highlight"> 1)
<span class="math notranslate nohighlight">\(\mathbb{P}\left( \varnothing \right) = 0\)</span>; 2)
<span class="math notranslate nohighlight">\(A,B\in \mathcal{ F,}A\bigcap B\mathbb{= \varnothing \Rightarrow P}\left( A \right)\mathbb{+ P}\left( B \right)\mathbb{= P}\left( A\bigcup B \right)\)</span> </span></span>.
A measure has many intuitive properties we would expect from “area” or
“volume”, for example, let <span class="math notranslate nohighlight">\(A,B \in \mathcal{F}\)</span>, then
<span class="math notranslate nohighlight">\(μ\left( A\bigcup B \right) \leq μ\left( A \right) + μ\left( B \right)\)</span>
and <span class="math notranslate nohighlight">\(A \subseteq B \Rightarrow μ\left( A \right) \leq μ\left( B \right)\)</span>
Probability is formally defined a measure over each event in
<span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, called  <span><span class="exdef"> <span class="target" id="index-10"></span>probability measure</span></span>, usually denoted by
<span class="math notranslate nohighlight">\(\mathbb{P}\)</span>. <span><span class="fact-highlight"> A probability measure has all axioms of a measure, and adds a third axiom
that <span class="math notranslate nohighlight">\(\mathbb{P}\left( \Omega \right) = 1\)</span> </span></span>, called the  <span><span class="exdef"> <span class="target" id="index-11"></span>probability normalization axiom</span></span> . For example, if <span class="math notranslate nohighlight">\(\Omega = \left\{ 0,1 \right\}\)</span>,
then <span class="math notranslate nohighlight">\(\left\{ \left\{ 0 \right\},\left\{ 1 \right\},\left\{ 0,1 \right\},\varnothing \right\}\)</span>
is a 𝜎-algebra of <span class="math notranslate nohighlight">\(\Omega\)</span> and we can define
<span class="math notranslate nohighlight">\(\mathbb{P}\left( \left\{ 0 \right\} \right) = 0.3,\mathbb{P}\left( \left\{ 1 \right\} \right) = 0.7,\mathbb{P}\left( \left\{ 0,1 \right\} \right) = 1,\mathbb{P}\left( \varnothing \right) = 0\)</span>.
We call <span class="math notranslate nohighlight">\(\left( \Omega,\mathcal{F}\mathbb{,P} \right)\)</span> a  <span><span class="exdef"> <span class="target" id="index-12"></span>probability space</span></span>.</p>
<p><span style="padding-left:20px"></span> A  <span><span class="exdef"> <span class="target" id="index-13"></span>random variable</span></span> often abbreviated as “RV” in our text, is a
<span class="math notranslate nohighlight">\(\Omega \rightarrow S\)</span> function where <span class="math notranslate nohighlight">\((S\mathcal{,E)}\)</span> is a measurable
space may or may not be the same as <span class="math notranslate nohighlight">\(\left( \Omega,\mathcal{F} \right)\)</span>
<span class="math notranslate nohighlight">\(X\)</span> need to be a  <span><span class="exdef"> <span class="target" id="index-14"></span>measurable function</span></span>, roughly meaning that sets like
<span class="math notranslate nohighlight">\(\left\{ \omega:X\left( \omega \right) = 1 \right\}\)</span>,
<span class="math notranslate nohighlight">\(\left\{ \omega:X\left( \omega \right) \geq 2 \right\}\)</span>, etc. must be
events (i.e. they exist in <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>). <span class="math notranslate nohighlight">\(S\)</span> is named the  <span><span class="exdef"> <span class="target" id="index-15"></span>state space</span></span>. <span class="emp">For example</span>, <span class="math notranslate nohighlight">\(\Omega = \left\{ 1,\ldots,5 \right\}\)</span>,
<span class="math notranslate nohighlight">\(S = \left\{ 0,1 \right\}\)</span>, <span class="math notranslate nohighlight">\(X\left( \omega \right) = \left\{ \begin{matrix} 0 &amp; \omega \leq 3 \\ 1 &amp; \omega &gt; 3 \\ \end{matrix} \right.\ \)</span>. Now if <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> is “uniform” over <span class="math notranslate nohighlight">\(\Omega\)</span>,
i.e. <span class="math notranslate nohighlight">\(\mathbb{P}\left( \left\{ \omega \right\} \right) = \frac{1}{5},\forall\omega \in \Omega\)</span>,
then we have <span class="math notranslate nohighlight">\(\mathbb{P}\left( X = 0 \right) = \frac{3}{5}\)</span> and
<span class="math notranslate nohighlight">\(\mathbb{P}\left( X = 1 \right) = \frac{2}{5}\)</span>. If <span class="math notranslate nohighlight">\(S\)</span> is finite or
countable, then <span class="math notranslate nohighlight">\(X\)</span> is a  <span><span class="exdef"> <span class="target" id="index-16"></span>discrete random variable</span></span> and the
<span class="math notranslate nohighlight">\(S \rightarrow \left\lbrack 0,1 \right\rbrack\)</span> function
<span class="math notranslate nohighlight">\(p\left( x \right):=\mathbb{P}\left( X = x \right),x \in S\)</span> is the  <span><span class="exdef"> <span class="target" id="index-17"></span>discrete distribution</span></span> of <span class="math notranslate nohighlight">\(X\)</span>, and each <span class="math notranslate nohighlight">\(p\left( x \right)\)</span> can be
called the  <span><span class="exdef"> <span class="target" id="index-18"></span>probability mass</span></span> at <span class="math notranslate nohighlight">\(x\)</span>. If <span class="math notranslate nohighlight">\(S\)</span> is uncountable, things
become more complicated, and a theorem called  <span><span class="exdef"> <span class="target" id="index-19"></span>Radom-Nikodym theorem</span></span>
guarantees <span><span class="fact-highlight"> given any measure <span class="math notranslate nohighlight">\(μ\)</span> defined on the <span class="math notranslate nohighlight">\((S,\mathcal{E})\)</span>, there exists a unique corresponding density function
<span class="math notranslate nohighlight">\(f\left( x \right):S \rightarrow \left\lbrack 0, + \infty \right\rbrack\)</span>
consistent with the underlying probability measure <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> in a way that <span class="math notranslate nohighlight">\(\int_{B}^{}f\text{d}μ=\mathbb{P}\left\{ X^{- 1}\left( B \right) \right\}\)</span> </span></span>.
<span><span class="fact-highlight"> If we choose μ as Lebesgue measure, then we have ordinary density functions we commonly use </span></span>.
We also refer to <span class="math notranslate nohighlight">\(f\)</span> as the  <span><span class="exdef"> <span class="target" id="index-20"></span>distribution</span></span> of <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(X\)</span>  <span><span class="exdef"> <span class="target" id="index-21"></span>obeys</span></span> the
distribution <span class="math notranslate nohighlight">\(f\)</span>, and <span class="math notranslate nohighlight">\(f\left( x \right)\)</span> is the  <span><span class="exdef"> <span class="target" id="index-22"></span>density</span></span> of the
distribution at <span class="math notranslate nohighlight">\(x\)</span>. Random-Nikodym applies to both discrete and
continuous RV, so probability mass is a special type of probability
density, but we should <span class="emp">note</span> probability density is not
probability and its value can be much higher than <span class="math notranslate nohighlight">\(1\)</span> (like in a
Gaussian distribution with a high peak). <span class="emp">Every time</span> we mention a random variable, we assume these setups of measurable spaces,
the probability measure and the probability density, although we might not explicitly mention that.</p>
<p><span style="padding-left:20px"></span> A random variable can be a vector, e.g.
<span class="math notranslate nohighlight">\(\left( X_{1},X_{2},X_{3} \right)\)</span>, called a  <span><span class="exdef"> <span class="target" id="index-23"></span>random vector</span></span>; then a  <span><span class="exdef"> <span class="target" id="index-24"></span>marginal random variable</span></span> removes some dimensions of the random
vector, e.g. <span class="math notranslate nohighlight">\(X_{1}\)</span>, or <span class="math notranslate nohighlight">\(\left( X_{1},X_{2} \right)\)</span> or
<span class="math notranslate nohighlight">\(\left( X_{2},X_{3} \right)\)</span>, etc., whose 𝜎-algebra only contains events
not dependent on the removed dimensions. The removal of some dimensions
of a random vector is called  <span><span class="exdef"> <span class="target" id="index-25"></span>marginalization</span></span>. More formally, suppose the original RV is
<span class="math notranslate nohighlight">\(\left( X,Y \right)\)</span> with measurable space
<span class="math notranslate nohighlight">\(\left( \Omega_{X,Y},\mathcal{F}_{X,Y} \right)\)</span>
and density <span class="math notranslate nohighlight">\(f\left( \mathbf{x},\mathbf{y} \right)\)</span> where
<span class="math notranslate nohighlight">\(Y,Y\)</span> are themselves random vectors and we intend to
marginalize out <span class="math notranslate nohighlight">\(Y\)</span>, then the marginal RV <span class="math notranslate nohighlight">\(Y\)</span> has  <span><span class="exdef"> <span class="target" id="index-26"></span>𝜎-algebra closure</span></span> of
<span class="math notranslate nohighlight">\(\left\{ \left\{ Y = \mathbf{x} \right\}\mathbf{:}\left( \mathbf{x},\mathbf{y} \right) \in \Omega_{\mathbf{x},\mathbf{y}} \right\}\)</span>
as its 𝜎-algebra, which is the smallest 𝜎-algebra that contains the set.
The density of <span class="math notranslate nohighlight">\(Y\)</span> is called the  <span><span class="exdef"> <span class="target" id="index-27"></span>marginal density</span></span>, and it can be shown to be exactly
<span class="math notranslate nohighlight">\(f\left( \mathbf{x} \right) = \int_{}^{}{f\left( \mathbf{x},\mathbf{y} \right)d\mathbf{y}}\)</span>
where the integration is w.r.t. the measure chosen for <span class="math notranslate nohighlight">\((S\mathcal{,E)}\)</span>, typically Lebesgue measure.</p>
<p><span style="padding-left:20px"></span> A  <span><span class="exdef"> <span class="target" id="index-28"></span>conditional random variable</span></span> of <span class="math notranslate nohighlight">\(X\)</span> conditioned on an event
<span class="math notranslate nohighlight">\(A\in \mathcal{ F}\)</span>, denoted by <span class="math notranslate nohighlight">\(X|A\)</span>. It can be viewed as a RV defined
on measurable space <span class="math notranslate nohighlight">\(\left( A,\{ B\bigcap A:B\in \mathcal{ F\}} \right)\)</span>.
A conditional random variable conditioned on another <span class="math notranslate nohighlight">\(Y\)</span>, denoted by
<span class="math notranslate nohighlight">\(X|Y\)</span>, can be viewed as a <span class="math notranslate nohighlight">\(\Omega^{2}\mathbb{\rightarrow R}\)</span> function
where an underlying measurable space for <span class="math notranslate nohighlight">\(\Omega^{2}\)</span> is defined. A
conditional RV has its own  <span><span class="exdef"> <span class="target" id="index-29"></span>conditional distribution</span></span>. The  <span><span class="exdef"> <span class="target" id="index-30"></span>conditional density function</span></span> <span class="math notranslate nohighlight">\(f\left( x;y \right)\)</span> we see more often
in practice can either be viewed a density function of conditional RV
<span class="math notranslate nohighlight">\(X|Y = y\)</span> conditioned on event <span class="math notranslate nohighlight">\(Y = y\)</span>, or just the density function of
<span class="math notranslate nohighlight">\(X|Y\)</span>. The  <span><span class="exdef"> <span class="target" id="index-31"></span>expectation</span></span> of a random variable <span class="math notranslate nohighlight">\(X\)</span> is defined as
<span class="math notranslate nohighlight">\(\mathbb{E}X = \int_{\Omega}^{}{\text{Xd}\mathbb{P}}\)</span>, the integration
of function <span class="math notranslate nohighlight">\(X\)</span> over entire sample space under measure <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>. The  <span><span class="exdef"> <span class="target" id="index-32"></span>conditional expectation</span></span> conditioned on an event <span class="math notranslate nohighlight">\(A\in \mathcal{ F}\)</span>
is defined as
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|A \right\rbrack = \int_{A}^{}{\text{Xd}\mathbb{P}}\)</span>,
the integration of function <span class="math notranslate nohighlight">\(X\)</span> over <span class="math notranslate nohighlight">\(A\)</span> under measure <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>. The  <span><span class="exdef"> <span class="target" id="index-33"></span>conditional expectation</span></span> conditioned on another random variable <span class="math notranslate nohighlight">\(Y\)</span>,
denoted as <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|Y \right\rbrack\)</span> is itself a random
variable because <span class="math notranslate nohighlight">\(Y\)</span> is random.</p>
<p><span style="padding-left:20px"></span> A  <span><span class="exdef"> <span class="target" id="index-34"></span>stochastic process</span></span> or a  <span><span class="exdef"> <span class="target" id="index-35"></span>random process</span></span> is a collection of RVs
<span class="math notranslate nohighlight">\(X\left( t \right),t \in T\)</span> where <span class="math notranslate nohighlight">\(T\)</span> is a finite, countable or
uncountable  <span><span class="exdef"> <span class="target" id="index-36"></span>index set</span></span>.” Note the random variables in this general
definition do not have intrinsic order, even though the word “process”
suggests “series”. The order of RVs can be added by concrete
definitions, for example, a  <span><span class="exdef"> <span class="target" id="index-37"></span>Markov chain</span></span> is a stochastic process
where <span class="math notranslate nohighlight">\(T = \left\{ 0,1,2,\ldots \right\}\mathbb{= N}\)</span> s.t. the
distribution of <span class="math notranslate nohighlight">\(X\left( t + 1 \right)\)</span> is only dependent on
<span class="math notranslate nohighlight">\(X\left( t \right)\)</span> for <span class="math notranslate nohighlight">\(t = 1,2\ldots\)</span> In such case when the “process”
is ordered, <span class="math notranslate nohighlight">\(T\)</span> can usually be interpreted as time. We can often see
processes like  <span><span class="exdef"> <span class="target" id="index-38"></span>Gaussian process</span></span>,  <span><span class="exdef"> <span class="target" id="index-39"></span>Poisson process</span></span>,  <span><span class="exdef"> <span class="target" id="index-40"></span>Dirichlet process</span></span>
etc. in probabilistic machine learning, and the pattern of
their definitions are <span class="emp">simply</span> stochastic processes
<span class="math notranslate nohighlight">\(X\left( t \right),t \in T\)</span> s.t. for any finite subset
<span class="math notranslate nohighlight">\(\left\{ t_{1},\ldots,t_{N} \right\}\)</span> of <span class="math notranslate nohighlight">\(T\)</span> of arbitrary
<span class="math notranslate nohighlight">\(N \in \mathbb{N}^{+}\)</span> satisfying certain condition (dependent on the
concrete process definition),
<span class="math notranslate nohighlight">\(\left( X\left( t_{1} \right),\ldots,X\left( t_{N} \right) \right)\)</span> in
some way obeys corresponding distributions (Gaussian distribution,
Poisson distribution, Dirichlet distribution etc.). :emp:<a href="#id1"><span class="problematic" id="id2">``</span></a>For
example`, we may define <span class="math notranslate nohighlight">\(X\left( t \right)\)</span> obeys a
Gaussian process <span class="math notranslate nohighlight">\(\operatorname{GP}\left( μ,\Sigma \right)\)</span> if
<span class="math notranslate nohighlight">\(\forall N \geq 1,t_{1},\ldots,t_{N} \in T\)</span>,
<span class="math notranslate nohighlight">\(μ\left( X\left( t_{1} \right),\ldots,X\left( t_{N} \right) \right)\)</span>
is the  <span><span class="exdef"> <span class="target" id="index-41"></span>mean function</span></span> that yields a vector of length <span class="math notranslate nohighlight">\(N\)</span>, and
<span class="math notranslate nohighlight">\(\Sigma\)</span> is the  <span><span class="exdef"> <span class="target" id="index-42"></span>covariance function</span></span> that yields a <span class="math notranslate nohighlight">\(N \times N\)</span>
matrix. <span class="emp">For another example</span>, the Dirichlet process
assumes the RVs <span class="math notranslate nohighlight">\(X\left( t \right)\)</span> are probability-measure-valued (i.e.
the state space <span class="math notranslate nohighlight">\(S\)</span> of <span class="math notranslate nohighlight">\(X\)</span> are probability measures), and lets the index
set <span class="math notranslate nohighlight">\(T\)</span> be a 𝜎-algebra, and define
<span class="math notranslate nohighlight">\(X\left( t \right)\sim\operatorname{DP}\left( α,H \right)\)</span> for some  <span><span class="exdef"> <span class="target" id="index-43"></span>base probability measure</span></span> <span class="math notranslate nohighlight">\(H\)</span> and  <span><span class="exdef"> <span class="target" id="index-44"></span>concentration parameter</span></span>
<span class="math notranslate nohighlight">\(α\)</span>, if for any finite partition <span class="math notranslate nohighlight">\(t_{1},\ldots,t_{N}\)</span> in <span class="math notranslate nohighlight">\(T\)</span> (a
𝜎-algebra must contain finite partitions due to its axioms),
<span class="math notranslate nohighlight">\(\left( X\left( t_{1} \right),\ldots,X\left( t_{N} \right) \right)\)</span>
obeys Dirichlet distribution
<span class="math notranslate nohighlight">\(\operatorname{Dirichlet}\left(α H \left( t_{1} \right),\ldots,α H\left( t_{N} \right) \right)\)</span>.</p>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["color.js","autoload-all.js"] }
  });

      MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
   var color = MathJax.Extension["TeX/color"];
   color.colors["theorem"] = color.getColor('RGB','255,229,153');
       color.colors["result"] = color.getColor('RGB','189,214,238');
       color.colors["fact"] = color.getColor('RGB','255,255,204');
       color.colors["emperical"] = color.getColor('RGB','253,240,207');
       color.colors["comment"] = color.getColor('RGB','204,255,204');
   color.colors["thm"] = color.getColor('RGB','255,229,153');
       color.colors["rlt"] = color.getColor('RGB','189,214,238');
       color.colors["emp"] = color.getColor('RGB','253,240,207');
       color.colors["comm"] = color.getColor('RGB','204,255,204');
       color.colors["conn1"] = color.getColor('RGB','255,0,255');
       color.colors["conn2"] = color.getColor('RGB','237,125,49');
       color.colors["conn3"] = color.getColor('RGB','112,48,160');
      });
</script></div>
<div class="section" id="covariance-matrix">
<h2>1.2. Covariance Matrix<a class="headerlink" href="#covariance-matrix" title="Permalink to this headline">¶</a></h2>
<p><span style="padding-left:20px"></span> Given two scalar RVs <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, then the covariance
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\operatorname{cov} \left( {X,Y} \right) = \mathbb{E}\left[ {\left( {X - \mathbb{E}X} \right)\left( {Y - \mathbb{E}Y} \right)} \right]$}\)</span>.
Given a RV vector <span class="math notranslate nohighlight">\(X = \left( {\begin{array}{*{20}{c}}{{X_1}} \\   \vdots  \\  {{X_M}}\end{array}} \right)\)</span>,
define <span class="math notranslate nohighlight">\(\mathbb{E}X = \left( {\begin{array}{*{20}{c}}{\mathbb{E}{X_1}} \\\vdots  \\{\mathbb{E}{X_M}}\end{array}} \right)\)</span>
(and similarly the expectation of a RV matrix is to take expectation on each entry of that matrix), then the  <span><span class="def"> <span class="target" id="index-45"></span>covariance matrix</span></span> is defined as the following,</p>
<div class="math notranslate nohighlight" id="equation-eq-rv-cov">
<span class="eqno">(1.1)<a class="headerlink" href="#equation-eq-rv-cov" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
\Sigma \left( X \right) &amp; = \left( {\begin{array}{*{20}{c}}
{\operatorname{cov} \left( {{X_1},{X_1}} \right)}&amp; \cdots &amp;{\operatorname{cov} \left( {{X_1},{X_M}} \right)} \\
\vdots &amp; \ddots &amp; \vdots  \\
{\operatorname{cov} \left( {{X_M},{X_1}} \right)}&amp; \cdots &amp;{\operatorname{cov} \left( {{X_M},{X_M}} \right)}
\end{array}} \right) \hfill \\
&amp; = \left( {\begin{array}{*{20}{c}}
{\mathbb{E}\left[ {\left( {{X_1} - \mathbb{E}{X_1}} \right)\left( {{X_1} - \mathbb{E}{X_1}} \right)} \right]}&amp; \cdots &amp;{\mathbb{E}\left[ {\left( {{X_1} - \mathbb{E}{X_1}} \right)\left( {{X_M} - \mathbb{E}{X_M}} \right)} \right]} \\
\vdots &amp; \ddots &amp; \vdots  \\
{\mathbb{E}\left[ {\left( {{X_M} - \mathbb{E}{X_M}} \right)\left( {{X_1} - \mathbb{E}{X_1}} \right)} \right]}&amp; \cdots &amp;{\mathbb{E}\left[ {\left( {{X_M} - \mathbb{E}{X_M}} \right)\left( {{X_M} - \mathbb{E}{X_M}} \right)} \right]}
\end{array}} \right) \hfill \\
&amp;=\colorbox{fact}{$\mathbb{E}\left[ {\left( {X - \mathbb{E}X} \right){{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}} \right]$} \hfill \\
\end{align}\end{split}\]</div>
<p>where the diagonal elements are  <span><span class="exdef"> <span class="target" id="index-46"></span>variances</span></span> that can be denoted by <span class="math notranslate nohighlight">\(\operatorname{var}\left( X_{i} \right) := \operatorname{cov}\left( X_{i},X_{i} \right),i = 1,\ldots,M\)</span>
. We <span class="emp">note</span> there is difference that for two scalar RVs <span class="math notranslate nohighlight">\(X,Y\)</span>, <span class="math notranslate nohighlight">\(\operatorname{cov}⁡(X,Y)\)</span> is a value, but <span class="math notranslate nohighlight">\(\operatorname{\Sigma}\begin{pmatrix} X \\ Y \\ \end{pmatrix}\)</span> is a <span class="math notranslate nohighlight">\(2×2\)</span> matrix.</p>
<p><span style="padding-left:20px"></span> On the other hand, given two RVs <span class="math notranslate nohighlight">\(X,Y\)</span> and draw samples <span class="math notranslate nohighlight">\({\mathbf{x}} = \left( {{x_1}, \ldots ,{x_N}} \right)\sim X\)</span>
and <span class="math notranslate nohighlight">\({\mathbf{y}} = \left( {{y_1}, \ldots ,{y_N}} \right)\sim Y\)</span>, then we define the  <span><span class="def"> <span class="target" id="index-47"></span>sample covariance</span></span> of them as
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\operatorname{cov} \left( {{\mathbf{x}},{\mathbf{y}}} \right) = \frac{1}{{N - 1}}{\left( {{\mathbf{x}} - {\mathbf{\bar x}}} \right)^{\text{T}}}\left( {{\mathbf{y}} - {\mathbf{\bar y}}} \right)$}\)</span>.
Given a RV vector <span class="math notranslate nohighlight">\(X = \left( {\begin{array}{*{20}{c}}{{X_1}} \\\vdots  \\{{X_M}}\end{array}} \right)\)</span>,
we can draw  <span><span class="def"> <span class="target" id="index-48"></span>samples</span></span> <span class="math notranslate nohighlight">\({\mathbf{x}}_1^{\text{T}} = \left( {{x_{1,1}}, \ldots ,{x_{1,N}}} \right)\sim {X_1}, \ldots , {\mathbf{x}}_M^{\text{T}} = \left( {{x_{M,1}}, \ldots ,{x_{M,M}}} \right)\sim {X_M}\)</span>,
and form a  <span><span class="exdef"> <span class="target" id="index-49"></span>sample matrix</span></span> <span class="math notranslate nohighlight">\({\mathbf{X}} = \left( {\begin{array}{*{20}{c}}{{\mathbf{x}}_1^{\text{T}}} \\\vdots  \\{{\mathbf{x}}_M^{\text{T}}}\end{array}} \right)\)</span>.
In the machine learning context, the rows of <span class="math notranslate nohighlight">\(𝐗\)</span> are also referred to as <span class="tooltip">  <span><span class="def"> <span class="target" id="index-50"></span>feature vectors</span></span> <span class="tooltiptext"> Feature vectors are column vectors even though they are rows in the data matrix.</span></span> because it treats the RVs <span class="math notranslate nohighlight">\(X_1,…,X_M\)</span> as representing <span class="math notranslate nohighlight">\(M\)</span> random features of a data point;
and the columns of <span class="math notranslate nohighlight">\(𝐗\)</span> are called  <span><span class="def"> <span class="target" id="index-51"></span>data entries</span></span>, because they are actually observed data.
Then the  <span><span class="def"> <span class="target" id="index-52"></span>sample covariance matrix</span></span> is defined <span class="red">w.r.t. the feature vectors</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-sample-cov">
<span class="eqno">(1.2)<a class="headerlink" href="#equation-eq-sample-cov" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
{\Sigma }\left( {\mathbf{X}} \right) &amp;= \left( {\begin{array}{*{20}{c}}
{\operatorname{cov} \left( {{{\mathbf{x}}_1},{{\mathbf{x}}_1}} \right)}&amp; \cdots &amp;{\operatorname{cov} \left( {{{\mathbf{x}}_1},{{\mathbf{x}}_M}} \right)} \\
\vdots &amp; \ddots &amp; \vdots  \\
{\operatorname{cov} \left( {{{\mathbf{x}}_M},{{\mathbf{x}}_1}} \right)}&amp; \cdots &amp;{\operatorname{cov} \left( {{{\mathbf{x}}_M},{{\mathbf{x}}_M}} \right)}
\end{array}} \right) \hfill \\
&amp;= \frac{1}{{N - 1}}\left( {\begin{array}{*{20}{c}}
{{{\left( {{{\mathbf{x}}_1} - \overline {{{\mathbf{x}}_1}} } \right)}^{\text{T}}}\left( {{{\mathbf{x}}_1} - \overline {{{\mathbf{x}}_1}} } \right)}&amp; \cdots &amp;{{{\left( {{{\mathbf{x}}_1} - \overline {{{\mathbf{x}}_1}} } \right)}^{\text{T}}}\left( {{{\mathbf{x}}_M} - \overline {{{\mathbf{x}}_M}} } \right)} \\
\vdots &amp; \ddots &amp; \vdots  \\
{{{\left( {{{\mathbf{x}}_M} - \overline {{{\mathbf{x}}_M}} } \right)}^{\text{T}}}\left( {{{\mathbf{x}}_1} - \overline {{{\mathbf{x}}_1}} } \right)}&amp; \cdots &amp;{{{\left( {{{\mathbf{x}}_M} - \overline {{{\mathbf{x}}_M}} } \right)}^{\text{T}}}\left( {{{\mathbf{x}}_M} - \overline {{{\mathbf{x}}_M}} } \right)}
\end{array}} \right) \hfill \\
&amp;= \frac{1}{{N - 1}}\left( {{\mathbf{X}} - {\mathbf{\bar X}}} \right){\left( {{\mathbf{X}} - {\mathbf{\bar X}}} \right)^{\text{T}}} \hfill \\
\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\({\overline {{{\mathbf{x}}_i}} ^{\text{T}}} = \frac{1}{N}\mathop \sum \limits_{j = 1}^N {x_{i,j}}{1^{\text{T}}} = \left( {\frac{1}{N}\mathop \sum \limits_{j = 1}^N {x_{i,j}}, \ldots ,\frac{1}{N}\mathop \sum \limits_{j = 1}^N {x_{i,j}}} \right)\)</span>
(the same mean value repeating itself for <span class="math notranslate nohighlight">\(N\)</span> times)
and <span class="math notranslate nohighlight">\({\mathbf{\bar X}} = \left( {\begin{array}{*{20}{c}}{\overline {{{\mathbf{x}}_1}} } \\\vdots  \\{\overline {{{\mathbf{x}}_M}} }\end{array}} \right)\)</span>,
and the diagonal elements are  <span><span class="exdef"> <span class="target" id="index-53"></span>sample variances</span></span> that can be denoted as <span class="math notranslate nohighlight">\(\operatorname{var}\left( \mathbf{x}_{i} \right) := \operatorname{cov}\left( \mathbf{x}_{i},\mathbf{x}_{i} \right), i=1,...,M\)</span>.
The sum of variances, or the trace of the covariance matrix, is called the  <span><span class="def"> <span class="target" id="index-54"></span>total variance</span></span> of <span class="math notranslate nohighlight">\(X\)</span>.
In addition, <span class="emp">note</span> <span class="math notranslate nohighlight">\(\operatorname{cov} \left( {{\mathbf{x}},{\mathbf{y}}} \right)\)</span> is a value, while <span class="math notranslate nohighlight">\(Σ(x,y)\)</span> is a <span class="math notranslate nohighlight">\(2×2\)</span> matrix.</p>
<p><span style="padding-left:20px"></span>  <span><span class="exdef"> <span class="target" id="index-55"></span>Pearson’s correlation</span></span> is the normalized version of covariance, defined as
<span class="math notranslate nohighlight">\(\operatorname{corr}\left( X,Y \right) = \frac{\operatorname{cov}\left( X,Y \right)}{\sqrt{\operatorname{var}\left( X \right)}\sqrt{\operatorname{var}\left( Y \right)}}\)</span>
for two scalar RVs <span class="math notranslate nohighlight">\(X,Y\)</span>, and
<span class="math notranslate nohighlight">\(\operatorname{corr}\left( \mathbf{x},\mathbf{y} \right) = \frac{\operatorname{cov}\left( \mathbf{x},\mathbf{y} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x} \right)}\sqrt{\operatorname{var}\left( \mathbf{y} \right)}}\)</span>
for two samples <span class="math notranslate nohighlight">\(\mathbf{x},\mathbf{y}\)</span>. A  <span><span class="def"> <span class="target" id="index-56"></span>correlation matrix</span></span> for a
RV vector <span class="math notranslate nohighlight">\(X = \begin{pmatrix} X_{1} \\  \vdots \\ X_{M} \\ \end{pmatrix}\)</span> or a sample matrix <span class="math notranslate nohighlight">\(\mathbf{X} = \begin{pmatrix} \mathbf{x}_{1}^{\rm{T}} \\  \vdots \\ \mathbf{x}_{M}^{\rm{T}} \\ \end{pmatrix}\)</span> are just replacing
<span class="math notranslate nohighlight">\(\operatorname{cov}\left( \cdot , \cdot \right)\)</span> elements in (1?€?1) and
(1?€?2) by corresponding
<span class="math notranslate nohighlight">\(\operatorname{corr}\left( \cdot , \cdot \right)\)</span>. Since
<span class="math notranslate nohighlight">\(\operatorname{corr}\left( X,X \right) = 1\)</span> and
<span class="math notranslate nohighlight">\(\operatorname{corr}\left( \mathbf{x},\mathbf{x} \right) = 1\)</span>, then <span><span class="result-highlight"> the diagonal elements of a correlation matrix will all be 1 </span></span>. Let
<span class="math notranslate nohighlight">\(\Sigma_{\text{corr}}\)</span> denote a correlation matrix, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\operatorname{\Sigma_{\text{corr}}} \left( X \right) = \begin{pmatrix}
1 &amp; \cdots &amp; \frac{\operatorname{cov}\left( X_{1},X_{M} \right)}{\sqrt{\operatorname{var}\left( X_{1} \right)}\sqrt{\operatorname{var}\left( X_{M} \right)}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\operatorname{cov}\left( X_{M},X_{1} \right)}{\sqrt{\operatorname{var}\left( X_{M} \right)}\sqrt{\operatorname{var}\left( X_{1} \right)}} &amp; \cdots &amp; 1 \\
\end{pmatrix} = \begin{pmatrix}
1 &amp; \cdots &amp; \operatorname{E}{\lbrack\frac{\left( X_{1}\mathbb{- E}X_{1} \right)}{\sqrt{\operatorname{var}\left( X_{1} \right)}}\frac{\left( X_{M}\mathbb{- E}X_{M} \right)}{\sqrt{\operatorname{var}\left( X_{M} \right)}}\rbrack} \\
\vdots &amp; \ddots &amp; \vdots \\
\operatorname{E}{\lbrack\frac{\left( X_{M}\mathbb{- E}X_{M} \right)}{\sqrt{\operatorname{var}\left( X_{M} \right)}}\frac{\left( X_{1}\mathbb{- E}X_{1} \right)}{\sqrt{\operatorname{var}\left( X_{1} \right)}}\rbrack} &amp; \cdots &amp; 1 \\
\end{pmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\Sigma_{\text{corr}}\left( \mathbf{X} \right) = \begin{pmatrix}
1 &amp; \cdots &amp; \frac{\operatorname{cov}\left( \mathbf{x}_{1},\mathbf{x}_{M} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{1} \right)}\sqrt{\operatorname{var}\left( \mathbf{x}_{M} \right)}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\operatorname{cov}\left( \mathbf{x}_{M},\mathbf{x}_{1} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{1} \right)}\sqrt{\operatorname{var}\left( \mathbf{x}_{M} \right)}} &amp; \cdots &amp; 1 \\
\end{pmatrix} = \frac{1}{N - 1}\begin{pmatrix}
1 &amp; \cdots &amp; \frac{\left( \mathbf{x}_{1} - \overline{\mathbf{x}_{1}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{1} \right)}}^{\rm{T}}\frac{\left( \mathbf{x}_{M} - \overline{\mathbf{x}_{M}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{M} \right)}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\left( \mathbf{x}_{M} - \overline{\mathbf{x}_{M}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{M} \right)}}^{\rm{T}}\frac{\left( \mathbf{x}_{1} - \overline{\mathbf{x}_{1}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{1} \right)}} &amp; \cdots &amp; 1 \\
\end{pmatrix}\end{split}\]</div>
<p>Given a RV <span class="math notranslate nohighlight">\(X\)</span>, we can define
<span class="math notranslate nohighlight">\(\widetilde{X} = \frac{\left( X - \mathbb{E}X \right)}{\sqrt{\operatorname{var}\left( X \right)}}\)</span>
as the  <span><span class="exdef"> <span class="target" id="index-57"></span>standardized RV</span></span> (or  <span><span class="exdef"> <span class="target" id="index-58"></span>normalized RV</span></span>) of <span class="math notranslate nohighlight">\(X\)</span> of zero
expectation and unit variance; given a sample <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, we can
define
<span class="math notranslate nohighlight">\(\widetilde{\mathbf{x}} = \frac{\left( \mathbf{x} - \overline{\mathbf{x}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x} \right)}}\)</span>
as the  <span><span class="exdef"> <span class="target" id="index-59"></span>standardized sample</span></span> (or  <span><span class="exdef"> <span class="target" id="index-60"></span>normalized sample</span></span>) of
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with zero mean and unit variance. We thus can define  <span><span class="exdef"> <span class="target" id="index-61"></span>standardize RV vector</span></span> <span class="math notranslate nohighlight">\(\widetilde{X} = \begin{pmatrix} {\widetilde{X}}_{1} \\  \vdots \\ {\widetilde{X}}_{M} \\ \end{pmatrix}\)</span> and  <span><span class="exdef"> <span class="target" id="index-62"></span>standardized sample matrix</span></span>
<span class="math notranslate nohighlight">\(\widetilde{\mathbf{X}} = \begin{pmatrix} {\widetilde{\mathbf{x}}}_{1}^{\rm{T}} \\  \vdots \\ {\widetilde{\mathbf{x}}}_{M}^{\rm{T}} \\ \end{pmatrix}\)</span>, and therefore
<span><span class="result-highlight"> <span class="math notranslate nohighlight">\(\operatorname{}\left( X \right) = \operatorname{E}{\lbrack{\widetilde{X}\widetilde{X}}^{\rm{T}}\rbrack},\Sigma_{\text{corr}}\left( \mathbf{X} \right) = \frac{1}{N - 1}\widetilde{\mathbf{X}}{\widetilde{\mathbf{X}}}^{\rm{T}}\)</span>,
where we see correlation matrix is just the concept of covariance matrix applied on standardize (normalized) RV or sample.</span></span></p>
<p><span style="padding-left:20px"></span>  <span><span class="def"> <span class="target" id="index-63"></span>Cross-covariance matrix</span></span> and  <span><span class="def"> <span class="target" id="index-64"></span>cross-correlation matrix</span></span> is a generalized concept of covariance matrix and correlation matrix that consider two RV vectors <span class="math notranslate nohighlight">\(X,Y\)</span> or two sample matrices <span class="math notranslate nohighlight">\(\mathbf{X},\mathbf{Y}\)</span>. For example, suppose <span class="math notranslate nohighlight">\(X,Y\)</span> are of lengths <span class="math notranslate nohighlight">\(M_{1},M_{2}\)</span>, following <a class="reference internal" href="#equation-eq-rv-cov">Eq.1.1</a>, the cross-covariance matrix for <span class="math notranslate nohighlight">\(X,Y\)</span> is defined as a <span class="math notranslate nohighlight">\(M_{1} \times M_{2}\)</span> matrix (<span class="emp">note</span> need not be a square matrix),</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned} \operatorname{\Sigma}\left( X,Y \right)
&amp;= \begin{pmatrix}
\operatorname{cov}{(X_{1},Y_{1})} &amp; \cdots &amp; \operatorname{cov}{(X_{1},Y_{M_{2}})} \\
\vdots &amp; \ddots &amp; \vdots \\
\operatorname{cov}{(X_{M_{1}},Y_{1})} &amp; \cdots &amp; \operatorname{cov}{(X_{M_{1}},Y_{M_{2}})} \\
\end{pmatrix} \\
&amp;= \begin{pmatrix}
\operatorname{E}{\lbrack\left( X_{1}\mathbb{- E}X_{1} \right)\left( Y_{1}\mathbb{- E}Y_{1} \right)\rbrack} &amp; \cdots &amp; \operatorname{E}{\lbrack\left( X_{1}\mathbb{- E}X_{1} \right)\left( Y_{M_{2}}\mathbb{- E}Y_{M_{2}} \right)\rbrack} \\
\vdots &amp; \ddots &amp; \vdots \\
\operatorname{E}{\lbrack\left( X_{M_{1}}\mathbb{- E}X_{M_{1}} \right)\left( Y_{1}\mathbb{- E}Y_{1} \right)\rbrack} &amp; \cdots &amp; \operatorname{E}{\lbrack\left( X_{M_{1}}\mathbb{- E}X_{M_{1}} \right)\left( Y_{M_{2}}\mathbb{- E}Y_{M_{2}} \right)\rbrack} \\
\end{pmatrix} \\
&amp;= \colorbox{fact}{$\operatorname{E}{\lbrack{\left( X - \mathbb{E}X \right)\left( Y - \mathbb{E}Y \right)}^{\rm{T}}\rbrack}$}
\end{aligned}\end{split}\]</div>
<p>And all others can be defined in the same way, summarized as <span class="math notranslate nohighlight">\(\colorbox{fact}{$\Sigma\left( \mathbf{X,Y} \right) = \frac{1}{N - 1}\left( \mathbf{X} - \overline{\mathbf{X}} \right)\left( \mathbf{Y} - \overline{\mathbf{Y}} \right)^{\rm{T}}$}\)</span>,
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\operatorname{}\left( X,Y \right) = \operatorname{E}{\lbrack{\widetilde{X}\widetilde{Y}}^{\rm{T}}\rbrack}$}\)</span>, and <span class="math notranslate nohighlight">\(\colorbox{fact}{$\Sigma_{\text{corr}}\left( \mathbf{X,Y} \right) = \frac{1}{N - 1}\widetilde{\mathbf{X}}{\widetilde{\mathbf{Y}}}^{\rm{T}}$}\)</span></p>
<div class="admonition caution">
<p class="first admonition-title">Caution</p>
<p>In machine learning problems, we are often given a data matrix <span class="math notranslate nohighlight">\({\mathbf{X}} = \left( {{{\mathbf{x}}_1}, \ldots ,{{\mathbf{x}}_N}} \right)\)</span>
with the columns <span class="math notranslate nohighlight">\({{\mathbf{x}}_1}, \ldots ,{{\mathbf{x}}_N}\)</span> as data entries.
The bold small-letter symbol “<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>” very often represents a data entry in the content of machine learning,
but in statistics it often instead represents a feature vector (as in above discussion), and sometimes this causes confusion.
Therefore, we note it is necessary to understand what “<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>” represents in the context.</p>
<p class="last">The other possible confusion is about the “samples”. It is possible both the data entries and feature vectors are referred to as samples in different contexts.
We again <span class="emp">note</span> <span class="red">sample covariance is always w.r.t. the feature vectors, not data entries</span>.
Therefore, the “samples” in <a class="reference internal" href="#equation-eq-sample-cov">Eq.1.2</a> refers to feature vectors.</p>
</div>
<ul style="left-margin:20px">
<li>
<div class="section" id="property-1-1-classic-representation-of-covariance-by-expectation-or-mean">
<span id="property-cov-to-expectation"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Property 1-1.</span> <span class="bemp">Classic representation of covariance by expectation (or mean).</span><a class="headerlink" href="#property-1-1-classic-representation-of-covariance-by-expectation-or-mean" title="Permalink to this headline">¶</a></h3>Using the fact that <span class="math notranslate nohighlight">\(\colorbox{fact}{$\operatorname{cov}\left( X,Y \right)\mathbb{= E}XY - \mathbb{E}X\mathbb{E}Y$}\)</span> for scalar RVs <span class="math notranslate nohighlight">\(X,Y\)</span>, the covariance matrix has another form
<div class="math notranslate nohighlight">
\[\begin{split}\operatorname{\Sigma}\left( X \right) = \begin{pmatrix}
\mathbb{E}X_{1}^{2} - \mathbb{E}^{2}X_{1} &amp; \cdots &amp; \mathbb{E}{X_{1}X_{M}}\mathbb{- E}X_{1}\mathbb{E}X_{M} \\
\vdots &amp; \ddots &amp; \vdots \\
\mathbb{E}{X_{M}X_{1}}\mathbb{- E}X_{M}\mathbb{E}X_{1} &amp; \cdots &amp; \mathbb{E}X_{M}^{2} - \mathbb{E}^{2}X_{M} \\
\end{pmatrix} = \colorbox{rlt}{$\mathbb{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}X\mathbb{E}^{\mathrm{T}}X$}\end{split}\]</div>
<p>For two samples <span class="math notranslate nohighlight">\(\mathbf{x}\sim X,\mathbf{y}\sim Y\)</span> where
<span class="math notranslate nohighlight">\(\mathbf{x =}\left( x_{1}\mathbf{,\ldots,}x_{N} \right)\mathbf{,}\mathbf{y = (}y_{1}\mathbf{,\ldots,}y_{N}\mathbf{)}\)</span>,
we have</p>
<div class="math notranslate nohighlight">
\[\left( \mathbf{x} - \overline{\mathbf{x}} \right)^{\mathrm{T}}\left( \mathbf{y} - \overline{\mathbf{y}} \right) = \mathbf{x}^{\mathrm{T}}\mathbf{y} - \mathbf{x}^{\mathrm{T}}\overline{\mathbf{y}} - {\overline{\mathbf{x}}}^{\mathrm{T}}\mathbf{y} + {\overline{\mathbf{x}}}^{\mathrm{T}}\overline{\mathbf{y}}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\overline{x} = \frac{\sum_{i = 1}^{N}{\mathbf{x(}i\mathbf{)}}}{N}\)</span>
and <span class="math notranslate nohighlight">\(\overline{y} = \frac{\sum_{i = 1}^{N}{\mathbf{y(}i\mathbf{)}}}{N}\)</span>,
then</p>
<div class="math notranslate nohighlight">
\[{\mathbf{x}^{\mathrm{T}}\overline{\mathbf{y}} = \sum_{i = 1}^{N}{\overline{y}\mathbf{x(}i\mathbf{)}} = \overline{y}\sum_{i = 1}^{N}{\mathbf{x(}i\mathbf{)}} = N\overline{x}\overline{y}}\]</div>
<div class="math notranslate nohighlight">
\[{{\overline{\mathbf{x}}}^{\mathrm{T}}\mathbf{y} = \sum_{i = 1}^{N}{\overline{x}\mathbf{y(}i\mathbf{)}} = \overline{x}\sum_{i = 1}^{N}{\mathbf{y(}i\mathbf{)}} = N\overline{x}\overline{y}}\]</div>
<div class="math notranslate nohighlight">
\[{{\overline{\mathbf{x}}}^{\mathrm{T}}\overline{\mathbf{y}} = \sum_{i = 1}^{N}{\overline{x}\overline{y}} = N\overline{x}\overline{y}}\]</div>
<p>Thus</p>
<div class="math notranslate nohighlight">
\[\left( \mathbf{x} - \overline{\mathbf{x}} \right)^{\mathrm{T}}\left( \mathbf{y} - \overline{\mathbf{y}} \right) = \mathbf{x}^{\mathrm{T}}\mathbf{x +}N\overline{x}\overline{y} - 2N\overline{x}\overline{y} = \mathbf{x}^{\mathrm{T}}\mathbf{x -}N\overline{x}\overline{y} = \mathbf{x}^{\mathrm{T}}\mathbf{y -}{\overline{\mathbf{x}}}^{\mathrm{T}}\overline{\mathbf{y}}\]</div>
<p>which implies</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Sigma(\mathbf{X}) = \frac{1}{N - 1}\begin{pmatrix}
\mathbf{x}_{1}^{\mathrm{T}}\mathbf{x}_{1}{\bf -}{\overline{\mathbf{x}_{1}}}^{\mathrm{T}}\overline{\mathbf{x}_{1}} &amp; \cdots &amp; \mathbf{x}_{1}^{\mathrm{T}}\mathbf{x}_{M}{\bf -}{\overline{\mathbf{x}_{1}}}^{\mathrm{T}}\overline{\mathbf{x}_{M}} \\
\vdots &amp; \ddots &amp; \vdots \\
\mathbf{x}_{M}^{\mathrm{T}}\mathbf{x}_{1}{\bf -}{\overline{\mathbf{x}_{M}}}^{\mathrm{T}}\overline{\mathbf{x}_{1}} &amp; \cdots &amp; \mathbf{x}_{M}^{\mathrm{T}}\mathbf{x}_{M}{\bf -}{\overline{\mathbf{x}_{M}}}^{\mathrm{T}}\overline{\mathbf{x}_{M}} \\
\end{pmatrix} = \colorbox{rlt}{$\frac{1}{N - 1}\left( \mathbf{X}\mathbf{X}^{\mathrm{T}}{\bf -}\overline{\mathbf{X}}{\overline{\mathbf{X}}}^{\mathrm{T}} \right)$}\end{split}\]</div>
<p>We can verify above inference directly works for cross-covariance, and therefore</p>
<div class="math notranslate nohighlight">
\[\colorbox{result}{$\operatorname{\Sigma}\left( X,Y \right) = \mathbb{E}\mathrm{\lbrack}XY^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}X\mathbb{E}^{\rm{T}}Y,\Sigma\left( \mathbf{X,Y} \right) = \frac{1}{N - 1}\left( \mathbf{X}\mathbf{Y}^{\rm{T}}\mathbf{-}\overline{\mathbf{X}}{\overline{\mathbf{Y}}}^{\rm{T}} \right)$}\]</div>
</div></li><li>

<div class="section" id="property-1-2-invariance-to-centralization">
<span id="property-cov-invariant-to-centralization"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Property 1-2.</span> <span class="bemp">Invariance to centralization.</span><a class="headerlink" href="#property-1-2-invariance-to-centralization" title="Permalink to this headline">¶</a></h3>For any random vector <span class="math notranslate nohighlight">\(X\)</span>, we have <span class="math notranslate nohighlight">\(\colorbox{result}{$\Sigma\left( X - \mathbb{E}X \right) = \Sigma(X)$}\)</span>,
since <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack = \mathbf{0}\)</span> and
<div class="math notranslate nohighlight">
\[\Sigma\left( X - \mathbb{E}X \right)
= \mathbb{E}{\lbrack{\left( X - \mathbb{E}X - \mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack \right)\left( X - \mathbb{E}X - \mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack \right)}^{\mathrm{T}}\rbrack}
= \mathbb{E}{\lbrack{\left( X - \mathbb{E}X \right)\left( X - \mathbb{E}X \right)}^{\mathrm{T}}\rbrack} = \Sigma(X)\]</div>
<p>Similarly <span class="math notranslate nohighlight">\(\colorbox{result}{$\Sigma\left( \mathbf{X} - \overline{\mathbf{X}} \right) = \Sigma\left( \mathbf{X} \right)$}\)</span>,
because <span class="math notranslate nohighlight">\(\mathbf{x} - \overline{\mathbf{x}} - \overline{\mathbf{x} - \overline{\mathbf{x}}} = \mathbf{x} - \overline{\mathbf{x}}\)</span>
for any sample <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, and the result following by applying this on <a class="reference internal" href="#equation-eq-sample-cov">Eq.1.2</a>. For cross-covariance matrix, we have
<span class="math notranslate nohighlight">\(\colorbox{result}{$\Sigma\left( X - \mathbb{E}X,Y - \mathbb{E}Y \right) = \Sigma\left( X,Y \right)$}\)</span>
and <span class="math notranslate nohighlight">\(\colorbox{result}{$\Sigma\left( \mathbf{X} - \overline{\mathbf{X}},\mathbf{Y} - \overline{\mathbf{Y}} \right) = \Sigma\left( \mathbf{X,}\mathbf{Y} \right)$}\)</span>
for exactly the same reason.</p>
</div></li><li>

<div class="section" id="theorem-1-1-matrix-arithmetics-of-covariance-matrix">
<span id="theorem-cov-matrix-arithmetic-rules"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-1.</span> <span class="bemp">Matrix arithmetics of covariance matrix.</span><a class="headerlink" href="#theorem-1-1-matrix-arithmetics-of-covariance-matrix" title="Permalink to this headline">¶</a></h3><span><span class="theorem-highlight"> Given <span class="math notranslate nohighlight">\(X = \left( X_{1},\ldots,X_{n} \right)^{\mathrm{T}}\)</span>,
<span class="math notranslate nohighlight">\(\operatorname{var}\left( \mathbf{α}^{\rm{T}}X \right) = \operatorname{var}\left( X^{\rm{T}}\mathbf{α} \right) = \mathbf{α}^{\rm{T}}\operatorname{\Sigma}\left( X \right)\mathbf{α}\)</span> </span></span>.
<span class="emp">Note</span> <span class="math notranslate nohighlight">\(α^{\rm{T}}\Sigma{X}\)</span> is a scalar random variable, and
<div class="math notranslate nohighlight">
\[\mathbb{E}\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)\mathbb{= E}\left( X^{\mathrm{T}}\mathrm{\mathbf{α}} \right) = \mathrm{\mathbf{α}}^{\mathrm{T}}\mathbb{E}X = \left( \mathbb{E}^{\mathrm{T}}X \right)\mathrm{\mathbf{α}}\]</div>
<p>Also <span class="math notranslate nohighlight">\(\mathrm{\mathbf{α}}^{\mathrm{T}}X\)</span> is a scalar RV, and so
<span class="math notranslate nohighlight">\(\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)^{2}={\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)}^{\mathrm{T}} = \mathrm{\mathbf{α}}^{\mathrm{T}}XX^{\mathrm{T}}\mathrm{\mathbf{α}}\)</span>.
Recall <span class="math notranslate nohighlight">\(\operatorname{var} \left( X \right)=\mathbb{E}X^{2} - \mathbb{E}^{2}X\)</span>, then using <a class="reference internal" href="__02_cov.html#property-cov-to-expectation"><span class="std std-ref">Property 1-1</span></a>, we have</p>
<div class="math notranslate nohighlight">
\[\operatorname{var} \left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right) = \mathbb{E}\mathrm{\lbrack}\mathrm{\mathbf{α}}^{\mathrm{T}}X\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}\left\lbrack \mathrm{\mathbf{α}}^{\mathrm{T}}X \right\rbrack\mathbb{E}^{\mathrm{T}}\left\lbrack \mathrm{\mathbf{α}}^{\mathrm{T}}X \right\rbrack = \mathrm{\mathbf{α}}^{\mathrm{T}}\mathbb{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack}\mathrm{\mathbf{α}} - \mathrm{\mathbf{α}}^{\mathrm{T}}\mathbb{E}X\mathbb{E}^{\mathrm{T}}X\mathrm{\mathbf{α}}=\mathrm{\mathbf{α}}^{\mathrm{T}}\mathbf{(}\mathbb{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}X\mathbb{E}^{\mathrm{T}}X\mathrm{)}\mathrm{\mathbf{α}}=\mathrm{\mathbf{α}}^{\mathrm{T}}\operatorname{\Sigma}\left( X \right)\mathrm{\mathbf{α}}\]</div>
<p>Similarly, using
<span class="math notranslate nohighlight">\(\operatorname{cov} \left( X,Y \right)\mathbb{= E}XY - \mathbb{E}X\mathbb{E}Y\)</span>, we have
<span class="math notranslate nohighlight">\(\colorbox{theorem}{$\operatorname{cov} \left( \mathrm{\mathbf{α}}^{\mathrm{T}}X,\mathbf{β}^\mathrm{T}Y \right) = \mathrm{\mathbf{α}}^\mathrm{T}\operatorname{\Sigma}\left( X \right)\mathbf{β}$}\)</span>.
Further, if we let
<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{= (}\mathbf{a}_{1}\mathbf{,\ldots,}\mathbf{a}_{n}\mathbf{)}\)</span>,
then
<span class="math notranslate nohighlight">\(\colorbox{theorem}{$\Sigma\left( \mathbf{A}^\mathrm{T}X \right) = \mathbf{A}^\mathrm{T}\operatorname{\Sigma}\left( X \right)\mathbf{A}$}\)</span>,
since
<span class="math notranslate nohighlight">\(\Sigma\left( \mathrm{\mathbf{α}}_{i}X\mathbf{,}\mathrm{\mathbf{α}}_{j}X \right) = \mathrm{\mathbf{α}}_{i}^\mathrm{T}\operatorname{\Sigma}\left( X \right)\mathrm{\mathbf{α}}_{j}\)</span>; for the same reason, we have
<span class="math notranslate nohighlight">\(\Sigma\left( \mathbf{A}^{\rm{T}}X,\mathbf{B}^{\rm{T}}Y \right) = \mathbf{A}^{\rm{T}}\operatorname{\Sigma}\left( X \right)\mathbf{B}\)</span>
for cross-covariance.</p>
<p>On the other hand, given
<span class="math notranslate nohighlight">\(\mathbf{X}=(\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\mathbf{)}\)</span>, we have that
<span class="math notranslate nohighlight">\(\colorbox{theorem}{$\operatorname{var}\left( \mathbf{α}^{\rm{T}}\mathbf{X} \right) = \operatorname{var}\left( \mathbf{X}\mathbf{α} \right) = \mathbf{α}^{\rm{T}}\operatorname{\Sigma}\left( \mathbf{X} \right)\mathbf{α}$}\)</span>.
First check</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\{ \begin{matrix}
\overline{\mathbf{\text{Xα}}} = \overline{\sum_{i = 1}^{n}{\mathrm{\mathbf{α}}\left( i \right)\mathbf{x}_{i}}} = \frac{1}{m}\sum_{j = 1}^{m}{\sum_{i = 1}^{n}{\mathrm{\mathbf{α}}\left( i \right)\mathbf{x}_{i}(j)}} \\
\overline{\mathbf{X}}\mathrm{\mathbf{α}}=\sum_{i = 1}^{n}{\mathrm{\mathbf{α}}\left( i \right)\overline{\mathbf{x}_{i}}} = \sum_{i = 1}^{n}\left( \mathrm{\mathbf{α}}\left( i \right) \times \frac{1}{m}\sum_{j = 1}^{m}{\mathbf{x}_{i}\left( j \right)} \right) = \frac{1}{m}\sum_{i = 1}^{n}\left( \sum_{j = 1}^{m}{\mathrm{\mathbf{α}}\left( i \right)\mathbf{x}_{i}\left( j \right)} \right) \\
\end{matrix} \Rightarrow \colorbox{rlt}{$\overline{\mathbf{\text{Xα}}} = \overline{\mathbf{X}}\mathrm{\mathbf{α}}$} \right.\end{split}\]</div>
<p>Then we have</p>
<div class="math notranslate nohighlight">
\[\operatorname{var} \left( \mathbf{\text{Xα}} \right) = \frac{1}{n + 1}\left( \left( \mathbf{\text{Xα}} \right)\mathrm{T}\left( \mathbf{\text{Xα}} \right)\mathbf{-}{\overline{\mathbf{\text{Xα}}}}^\mathrm{T}\overline{\mathbf{\text{Xα}}} \right) = \frac{1}{n + 1}\left( \mathrm{\mathbf{α}}^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{Xα -}\mathrm{\mathbf{α}}^\mathrm{T}{\overline{\mathbf{X}}}^\mathrm{T}\overline{\mathbf{X}}\mathrm{\mathbf{α}} \right) = \mathrm{\mathbf{α}}^\mathrm{T}\Sigma\left( \mathbf{X} \right)\mathrm{\mathbf{α}}\]</div>
<p>By similar calculation,
<span class="math notranslate nohighlight">\(\operatorname{cov} \left( \mathbf{Xα,Xβ} \right) = \mathrm{\mathbf{α}}^\mathrm{T}\Sigma\left( \mathbf{X} \right)\mathbf{β}\)</span>.
Let <span class="math notranslate nohighlight">\(\mathbf{Y} = \mathbf{\text{XA}}\)</span> for any matrix
<span class="math notranslate nohighlight">\(\mathbf{A} = (\mathbf{a}_{1}\mathbf{,\ldots,}\mathbf{a}_{n}\mathbf{)}\)</span>,
then
<span class="math notranslate nohighlight">\(\colorbox{theorem}{$\Sigma(\mathbf{\text{A}}^{\rm{T}}\mathbf{\mathrm{X}}) = \mathbf{A}^\mathrm{T}\Sigma(\mathbf{X})\mathbf{A}$}\)</span>,
since <span class="math notranslate nohighlight">\(\ \Sigma\left( \mathbf{a}_{i}^{\rm{T}}\mathbf{X}\mathbf{,}\mathbf{a}_{j}^{\rm{T}}\mathbf{X} \right) = \mathbf{a}_{i}^{\rm{T}}\operatorname{\Sigma}\left( \mathbf{X} \right)\mathbf{a}_{j}\)</span>;
of course we also have <span><span class="theorem-highlight"> <span class="math notranslate nohighlight">\(\Sigma(\mathbf{A}^{\text{T}}\mathbf{X,}\mathbf{B}^{\rm{T}}\mathbf{Y}) = \mathbf{A}^{\rm{T}}\Sigma(\mathbf{X,Y})\mathbf{B}\)</span> for cross-covariance </span></span>.</p>
<p>For the same reason, <span><span class="theorem-highlight"> a square cross-covariance matrix is positive-semidefinite</span></span>.</p>
</div></li><li>

<div class="section" id="theorem-1-2-positive-definiteness-of-covariance-matrix">
<span id="theorem-cov-semipositiveness"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-2.</span> <span class="bemp">Positive definiteness of covariance matrix.</span><a class="headerlink" href="#theorem-1-2-positive-definiteness-of-covariance-matrix" title="Permalink to this headline">¶</a></h3><span><span class="theorem-highlight"> Covariance matrix is clearly symmetric, and moreover they are semi-positive definite</span></span>, since for any constant vector <span class="math notranslate nohighlight">\(\mathbf{α}\)</span>, using <a class="reference internal" href="__02_cov.html#theorem-cov-matrix-arithmetic-rules"><span class="std std-ref">Theorem 1-1</span></a>, we have
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
{{\mathbf{α }}^{\text{T}}}\left( {\Sigma \left( X \right)} \right){\mathbf{α }} &amp;= \Sigma \left( {{{\mathbf{α }}^{\text{T}}}X} \right) \hfill \\
&amp;= \mathbb{E}\left[ {\left( {{{\mathbf{α }}^{\text{T}}}X - \mathbb{E}{{\mathbf{α }}^{\text{T}}}X} \right){{\left( {{{\mathbf{α }}^{\text{T}}}X - \mathbb{E}{{\mathbf{α }}^{\text{T}}}X} \right)}^{\text{T}}}} \right] = \mathbb{E}\left[ {\left( {{{\mathbf{α }}^{\text{T}}}X - {{\mathbf{α }}^{\text{T}}}\mathbb{E}X} \right){{\left( {{{\mathbf{α }}^{\text{T}}}X - {{\mathbf{α }}^{\text{T}}}\mathbb{E}X} \right)}^{\text{T}}}} \right] \hfill \\
&amp;= \mathbb{E}\left[ {{{\mathbf{α }}^{\text{T}}}\left( {X - \mathbb{E}X} \right){{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}{\mathbf{α }}} \right] = \mathbb{E}\left[ {{{\left( {{{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}{\mathbf{α }}} \right)}^{\text{T}}}\left( {{{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}{\mathbf{α }}} \right)} \right] \geqslant 0 \hfill \\
\end{aligned}\end{split}\]</div>
<p>For sample covariance matrix, check that (omiting the coefficient)</p>
<div class="math notranslate nohighlight">
\[\mathbf{α}^{\rm{T}}\left( \Sigma\left( \mathbf{X} \right) \right)\mathbf{α}
=\Sigma\left( \mathbf{\text{Xα}} \right) \propto \left( \mathbf{\text{Xα}} - \overline{\mathbf{\text{Xα}}} \right)^{\rm{T}}\left( \mathbf{\text{Xα}} - \overline{\mathbf{\text{Xα}}} \right) \geq 0\]</div>
</div></li><li>

<div class="section" id="property-1-3-sample-covariance-represneted-by-rank-1-sum">
<span id="property-cov-as-rank1-sum"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Property 1-3.</span> <span class="bemp">Sample covariance represneted by rank-1 sum.</span><a class="headerlink" href="#property-1-3-sample-covariance-represneted-by-rank-1-sum" title="Permalink to this headline">¶</a></h3>Recall <span class="math notranslate nohighlight">\(\mathbf{X} = \begin{pmatrix} \mathbf{x}_{1}^{\rm{T}} \\  \vdots \\ \mathbf{x}_{M}^{\rm{T}} \\ \end{pmatrix}\)</span> are feature vectors, and the covariance matrix in <a class="reference internal" href="#equation-eq-sample-cov">Eq.1.2</a> is defined w.r.t. the feature vectors. Suppose
<span class="math notranslate nohighlight">\(\mathbf{X} = \left( \mathbf{𝓍}_{1},\ldots,\mathbf{𝓍}_{N} \right)\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{𝓍}_{1},\ldots,\mathbf{𝓍}_{N}\)</span> are columns of
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> as data entries, and similarly <span class="math notranslate nohighlight">\(\mathbf{Y} = \left( \mathbf{𝓎}_{1},\ldots,\mathbf{𝓎}_{N} \right)\)</span>
and let <span class="math notranslate nohighlight">\(\overline{\mathbf{𝓍}} = \frac{1}{N}\sum_{i = 1}^{N}\mathbf{𝓍}_{i}\)</span> and <span class="math notranslate nohighlight">\(\overline{\mathbf{𝓎}} = \frac{1}{N}\sum_{i = 1}^{N}\mathbf{𝓎}_{j}\)</span> be the mean vector of all data entries. Then we can show
<span class="math notranslate nohighlight">\(\Sigma\left( \mathbf{X} \right)\)</span> or <span class="math notranslate nohighlight">\(\Sigma\left( \mathbf{X,Y} \right)\)</span> can also be represented by
sum of rank-1 addends dependent on the data entries (rather than the feature vectors) as
<div class="math notranslate nohighlight" id="equation-eq-cov-rank1-data-entry-representation">
<span class="eqno">(1.3)<a class="headerlink" href="#equation-eq-cov-rank1-data-entry-representation" title="Permalink to this equation">¶</a></span>\[\Sigma\left( \mathbf{X,Y} \right) = \frac{1}{N - 1}\sum_{k = 1}^{N}{\left( \mathbf{𝓍}_{k} - \overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓎}_{k} - \overline{\mathbf{𝓎}} \right)^{\rm{T}}}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{\Sigma} := \Sigma\left( \mathbf{X},\mathbf{Y} \right)\)</span> for convenience.
By <a class="reference internal" href="#equation-eq-sample-cov">Eq.1.2</a>, we have (omitting the coefficient)</p>
<div class="math notranslate nohighlight">
\[\mathbf{\Sigma}\left( i,j \right) \propto \left( \mathbf{x}_{i} - \overline{\mathbf{x}_{i}} \right)^{\rm{T}}\left( \mathbf{y}_{j} - \overline{\mathbf{y}_{j}} \right)\]</div>
<p>Note <span class="math notranslate nohighlight">\(\mathbf{𝓍}_{j}\left( i \right) = x_{i,j} = \mathbf{x}_{i}\left( j \right)\)</span>
and <span class="math notranslate nohighlight">\(\overline{\mathbf{𝓍}}\left( i \right)\mathbf{=}\overline{x_{i}} = \frac{1}{N}\sum_{k = 1}^{N}x_{i,k}\)</span>, <span class="math notranslate nohighlight">\(\overline{\mathbf{𝓎}}\left( j \right)\mathbf{=}\overline{y_{j}} = \frac{1}{N}\sum_{j = 1}^{N}y_{j,k}\)</span>,
we have</p>
<div class="math notranslate nohighlight" id="equation-eq-cov-elementwise-representation-by-data-entries">
<span class="eqno">(1.4)<a class="headerlink" href="#equation-eq-cov-elementwise-representation-by-data-entries" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
&amp; \mathbf{\Sigma}\left( i,j \right) \propto \left( \mathbf{x}_{i} - \overline{\mathbf{x}_{i}} \right)^{\rm{T}}\left( \mathbf{y}_{j} - \overline{\mathbf{y}_{j}} \right)
= \sum_{k = 1}^{N}{\left( \mathbf{x}_{i}\left( k \right) - \overline{x_{i}} \right)\left( \mathbf{y}_{j}\left( k \right) - \overline{y_{j}} \right)} \\
&amp;= \sum_{k = 1}^{N}{\left( \mathbf{𝓍}_{k}\left( i \right) - \overline{x_{i}} \right)\left( \mathbf{𝓎}_{k}\left( j \right) - \overline{y_{j}} \right)}
= \sum_{k = 1}^{N}{\left( \mathbf{𝓍}_{k}\left( i \right) - \overline{\mathbf{𝓍}}\left( i \right) \right)\left( \mathbf{𝓎}_{k}\left( j \right) - \overline{\mathbf{𝓎}}\left( j \right) \right)} \\
&amp;= \sum_{k = 1}^{N}{\left( \left( \mathbf{𝓍}_{k} - \overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓎}_{k} - \overline{\mathbf{𝓎}} \right)^{\rm{T}} \right)\left( i,j \right)} \end{aligned}\end{split}\]</div>
<p>The above identity immediately implies <a class="reference internal" href="#equation-eq-cov-rank1-data-entry-representation">Eq.1.3</a>.</p>
<p id="corollary-cov-as-mixed"><span class="ititle2">Corollary 1-1.</span> <span class="math notranslate nohighlight">\(\colorbox{result}{$\mathbf{X}\mathbf{X}^{\rm{T}} = \left( N - 1 \right)\Sigma\left( \mathbf{X} \right) + N\overline{\mathbf{𝓍}}{\overline{\mathbf{𝓍}}}^{\rm{T}}$}\)</span>.
Check that <span class="math notranslate nohighlight">\(\mathbf{\text{XX}}^{\rm{T}} = \begin{pmatrix} \mathbf{x}_{1}^{\rm{T}}\mathbf{}_{1} &amp; \cdots &amp; \mathbf{x}_{1}^{\rm{T}}\mathbf{x}_{M} \\  \vdots &amp; \ddots &amp; \vdots \\ \mathbf{x}_{M}^{\rm{T}}\mathbf{x}_{1} &amp; \cdots &amp; \mathbf{x}_{M}^{  T}\mathbf{x}_{M} \\ \end{pmatrix}\)</span>, and by the same inference as <a class="reference internal" href="#equation-eq-cov-elementwise-representation-by-data-entries">Eq.1.4</a>
we have <span class="tooltip"> <span class="math notranslate nohighlight">\(\mathbf{\text{XX}}^{\rm{T}} = \sum_{k = 1}^{N}{\mathbf{𝓍}_{k}{\mathbf{𝓍}_{k}}^{\rm{T}}}\)</span> <span class="tooltiptext"> This is the rank-1 decomposition of any symmetric matrix in linear algebra.</span></span>,
and then use <a class="reference internal" href="#equation-eq-cov-rank1-data-entry-representation">Eq.1.3</a>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned} \mathbf{\text{XX}}^{\rm{T}} &amp;= \sum_{k = 1}^{N}{\mathbf{𝓍}_{k}{\mathbf{𝓍}_{k}}^{\rm{T}}} \\
&amp;= \sum_{k = 1}^{N}{\left( \mathbf{𝓍}_{k} - \overline{\mathbf{𝓍}} \right)({\mathbf{𝓍}_{k} - \overline{\mathbf{𝓍}})}^{\rm{T}}} + \sum_{k = 1}^{N}\left( \overline{\mathbf{𝓍}}\mathbf{𝓍}_{k}^{\rm{T}} + \mathbf{𝓍}_{k}{\overline{\mathbf{𝓍}}}^{\rm{T}} - \overline{\mathbf{𝓍}}{\overline{\mathbf{𝓍}}}^{\rm{T}} \right) \\
&amp;= \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)\mathbf{+}\overline{\mathbf{𝓍}}\left( \sum_{k = 1}^{N}\mathbf{𝓍}_{k}^{\rm{T}} \right) + \left( \sum_{k = 1}^{N}\mathbf{𝓍}_{k} \right){\overline{\mathbf{𝓍}}}^{\rm{T}} - N\overline{\mathbf{𝓍}}{\overline{\mathbf{𝓍}}}^{\rm{T}} \\
&amp;= \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)\mathbf{+}N\overline{\mathbf{𝓍}}{\overline{\mathbf{𝓍}}}^{\rm{T}} + N\overline{\mathbf{𝓍}}{\overline{\mathbf{𝓍}}}^{\rm{T}} - N\overline{\mathbf{𝓍}}{\overline{\mathbf{𝓍}}}^{\rm{T}} \\
&amp;= \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)\mathbf{+}N\overline{\mathbf{𝓍}} {\overline{\mathbf{𝓍}}}^{\rm{T}} \end{aligned}\end{split}\]</div>
</div></li><li>

<div class="section" id="theorem-1-3-block-decomposition-of-covariance-matrix">
<span id="theorem-cov-as-rank1-sum"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-3.</span> <span class="bemp">Block decomposition of covariance matrix.</span><a class="headerlink" href="#theorem-1-3-block-decomposition-of-covariance-matrix" title="Permalink to this headline">¶</a></h3>Again consider <span class="math notranslate nohighlight">\(\mathbf{X} = \left( \mathbf{𝓍}_{1},\ldots,\mathbf{𝓍}_{N} \right)\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{𝓍}_{1},\ldots,\mathbf{𝓍}_{N}\)</span> are data entries, and
<span class="math notranslate nohighlight">\(\overline{\mathbf{𝓍}} = \frac{1}{N}\sum_{i = 1}^{N}\mathbf{𝓍}_{i}\)</span>.
Suppose <span class="math notranslate nohighlight">\(\mathbf{𝓍}_{1},\ldots,\mathbf{𝓍}_{N}\)</span> are categorized into
<span class="math notranslate nohighlight">\(K\)</span> non-overlapping groups <span class="math notranslate nohighlight">\(G_{1},\ldots,G_{k}\)</span>. Let
<span class="math notranslate nohighlight">\(N_{1},\ldots,N_{k}\)</span> be the size of the groups, and
<span class="math notranslate nohighlight">\({\overline{\mathbf{𝓍}}}^{k} = \frac{1}{N_{k}}\sum_{\mathbf{𝓍} \in G_{k}}^{}\mathbf{𝓍}\)</span>, then
<div class="math notranslate nohighlight" id="equation-eq-cov-block-decomposition">
<span class="eqno">(1.5)<a class="headerlink" href="#equation-eq-cov-block-decomposition" title="Permalink to this equation">¶</a></span>\[\colorbox{theorem}{$\left( N - 1 \right)\Sigma\left( \mathbf{X} \right)
= \color{conn1}{\sum_{k = 1}^{K}{\left( N_{k} - 1 \right)\Sigma\left( \mathbf{X}_{k} \right)}}
+ \color{conn2}{\sum_{k = 1}^{K}{N_{k}\left( {\overline{\mathbf{𝓍}}}^{k} - \overline{\mathbf{𝓍}} \right)\left( {\overline{\mathbf{𝓍}}}^{k} - \overline{\mathbf{𝓍}} \right)^{\rm{T}}}}$}\]</div>
<p>This is because</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\left( N - 1 \right)\Sigma\left( \mathbf{X} \right)
&amp;= \sum_{j = 1}^{N}{\left( \mathbf{𝓍}_{j} - \overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓍}_{j} - \overline{\mathbf{𝓍}} \right)^{\rm{T}}} \\
&amp;= \sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( \mathbf{𝓍} - \overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓍} - \overline{\mathbf{𝓍}} \right)^{\rm{T}}}} \\
&amp;= \sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k}\mathbf{+}{\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k}\mathbf{+}{\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}}}} \\
&amp;= \sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)^{\rm{T}}}} + \sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}}}} + \sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}}}} + \sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)^{\rm{T}}}}
\end{aligned}\end{split}\]</div>
<p>where we have</p>
<div class="math notranslate nohighlight">
\[\sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)^{\rm{T}}}}
= \color{conn1}{\sum_{k = 1}^{K}{\left( N_{k} - 1 \right)\Sigma\left( \mathbf{X}_{k} \right)}}\]</div>
<div class="math notranslate nohighlight">
\[\sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}}}}
= \color{conn2}{\sum_{k = 1}^{K}{N_{k}\left( {\overline{\mathbf{𝓍}}}^{k} - \overline{\mathbf{𝓍}} \right)\left( {\overline{\mathbf{𝓍}}}^{k} - \overline{\mathbf{𝓍}} \right)^{\rm{T}}}}\]</div>
<p>The other two summations are zero matrices. For example,</p>
<div class="math notranslate nohighlight">
\[\sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}}}}
= \sum_{k = 1}^{K}\left( \left( \sum_{\mathbf{𝓍} \in G_{k}}^{}\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right) \right)\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}} \right)
= \sum_{k = 1}^{K}{\left( N_{k}{\overline{\mathbf{𝓍}}}^{k}\mathbf{-}N_{k}{\overline{\mathbf{𝓍}}}^{k} \right)\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}}}
= \mathbf{O}\]</div>
<p>Now the identity of <a class="reference internal" href="#equation-eq-cov-block-decomposition">Eq.1.5</a> is obvious.</p>
</li></ul>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["color.js","autoload-all.js"] }
  });

      MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
   var color = MathJax.Extension["TeX/color"];
   color.colors["theorem"] = color.getColor('RGB','255,229,153');
       color.colors["result"] = color.getColor('RGB','189,214,238');
       color.colors["fact"] = color.getColor('RGB','255,255,204');
       color.colors["emperical"] = color.getColor('RGB','253,240,207');
       color.colors["comment"] = color.getColor('RGB','204,255,204');
   color.colors["thm"] = color.getColor('RGB','255,229,153');
       color.colors["rlt"] = color.getColor('RGB','189,214,238');
       color.colors["emp"] = color.getColor('RGB','253,240,207');
       color.colors["comm"] = color.getColor('RGB','204,255,204');
       color.colors["conn1"] = color.getColor('RGB','255,0,255');
       color.colors["conn2"] = color.getColor('RGB','237,125,49');
       color.colors["conn3"] = color.getColor('RGB','112,48,160');
      });
</script></div>
</div>
<div class="section" id="multivariate-gaussian-distribution">
<h2>1.3. Multivariate Gaussian Distribution<a class="headerlink" href="#multivariate-gaussian-distribution" title="Permalink to this headline">¶</a></h2>
<p>An <span class="math notranslate nohighlight">\(m\)</span>-dimensional Multivariate Gaussian distribution is defined by an
<span class="math notranslate nohighlight">\(m\)</span>-dimensional mean <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span> and a <span class="math notranslate nohighlight">\(m \times m\)</span> non-singular
covariance <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>, denoted by
<span class="math notranslate nohighlight">\(\operatorname{Gaussian}\left( \mathbf{μ},\mathbf{\Sigma} \right)\)</span>,
whose density function can be written as</p>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-density">
<span class="eqno">(1.6)<a class="headerlink" href="#equation-eq-gaussian-density" title="Permalink to this equation">¶</a></span>\[p\left( \mathbf{x} \right)
= \frac{1}{\left( 2\pi \right)^{\frac{m}{2}}}\frac{1}{\left| \mathbf{\Sigma} \right|^{\frac{1}{2}}}\exp\left\{ - \frac{1}{2}\left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{\Sigma}^{- 1}\left( \mathbf{x} - \mathbf{μ} \right) \right\}
= \frac{1}{\left( 2\pi \right)^{\frac{m}{2}}}\frac{1}{\left| \mathbf{\Sigma} \right|^{\frac{1}{2}}}\kappa\left( \mathbf{x},\mathbf{μ} \right)
= \frac{1}{\left( 2\pi \right)^{\frac{m}{2}}}\frac{1}{\left| \mathbf{\Sigma} \right|^{\frac{1}{2}}}\exp\left\{ - \frac{1}{2}𝒹_{M}\left( \mathbf{x};\mathbf{μ,}\mathbf{\Sigma} \right) \right\}\]</div>
<p>where <span class="math notranslate nohighlight">\(\left| \mathbf{\Sigma} \right|\)</span> is the determinant of
<span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>, <span class="math notranslate nohighlight">\(\kappa\)</span> is the  <span><span class="exdef"> <span class="target" id="index-65"></span>Gaussian kernel</span></span>, and
<span class="math notranslate nohighlight">\(𝒹_{M}\left( \mathbf{x};\mathbf{μ,\Sigma} \right) = \left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{\Sigma}^{- 1}\left( \mathbf{x} - \mathbf{μ} \right)\)</span>
is called the  <span><span class="exdef"> <span class="target" id="index-66"></span>Mahalanobis distance</span></span>. <span class="math notranslate nohighlight">\(\kappa\)</span> and <span class="math notranslate nohighlight">\(𝒹_{M}\)</span> measure the similarity/distance
between an observation <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and a distribution with
mean <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span> and covariance <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span> (not necessarily
Gaussian). <span class="emp">Note</span> <span class="math notranslate nohighlight">\(𝒹_{M}\)</span> is reduced to
Euclidean distance from observation <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to mean <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span>
when the covariance is identity. Recall the basic fact that co-variance
matrix is symmetric and positive semidefinite, and in the case of
Gaussian <span class="tooltip"> it must be positive definite <span class="tooltiptext"> This is because <span class="math notranslate nohighlight">\(|Σ|≠0\)</span>; otherwise the Gaussian density of <a class="reference internal" href="#equation-eq-gaussian-density">Eq.1.6</a> would be invalid. Therefore, all eigenvalues of Σ must be positive, because the determinant equals to the product of all eigenvalues. </span></span>, thus
<span class="math notranslate nohighlight">\(𝒹_{M}\left( \mathbf{x};\mathbf{μ,\Sigma} \right) \geq 0\)</span>
and <span class="math notranslate nohighlight">\(𝒹_{M}\left( \mathbf{x};\mathbf{μ,\Sigma} \right) = 0\)</span>
iff <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{μ}\)</span>. The inverse of covariance matrix
<span class="math notranslate nohighlight">\(\mathbf{\Sigma}^{- 1}\)</span> is also named the  <span><span class="def"> <span class="target" id="index-67"></span>precision matrix</span></span>, denoted
by :math:<a href="#id3"><span class="problematic" id="id4">`</span></a>mathbf{Ⲗ}:=mathbf{Sigma}^{- 1}`, and the density can be written in terms of precision
matrix as</p>
<div class="math notranslate nohighlight">
\[p\left( \mathbf{x} \right) = \frac{\left| \mathbf{Ⲗ} \right|^{\frac{1}{2}}}{\left( 2\pi \right)^{\frac{m}{2}}}\exp\left\{ - \frac{1}{2}\left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{Ⲗ}\left( \mathbf{x} - \mathbf{μ} \right) \right\}\]</div>
<p>The <span class="emp">limitation</span> of Gaussian is <span><span class="comment-highlight"> its quadratic number of parameters which could be a problem for high-dimensional computation,
and its very limited unimodal shape which could not represent complicated real-world distributions.</span></span></p>
<ul style="left-margin:20px">
<li><p> We state useful gradient results from matrix derivatives, which are frequently used in finding analytical solution for optimization
problems. They are soon applied to prove the maximum likelihood of multivariate Gaussian.</p>
<p id="fact-gradient-affine-transform"><span class="ititle2">Fact 1-1.</span> Affine transformation
<span class="math notranslate nohighlight">\(\mathbf{a}^{\rm{T}}\mathbf{x} + b\mathbf{:}\mathbb{R}^{n}\mathbb{\rightarrow R}\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{a} \in \mathbb{R}^{n},b\mathbb{\in R}\)</span> has gradient
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\nabla\left( \mathbf{a}^{\rm{T}}\mathbf{x} + b \right) = \nabla\left( \mathbf{x}^{\rm{T}}\mathbf{a} + b \right) = \mathbf{a}$}\)</span>.</p>
<p id="fact-gradient-determinant"><span class="ititle2">Fact 1-2.</span> Determinant
<span class="math notranslate nohighlight">\(\left| \mathbf{X} \right|:\mathbb{R}^{n \times n}\mathbb{\rightarrow R}\)</span>
is a scalar-valued matrix function, and we have
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\nabla\log\left| \mathbf{X} \right| = \mathbf{X}^{-T}$}\)</span>.</p>
<p id="fact-gradient-matrix-of-matrix-product"><span class="ititle2">Fact 1-3.</span> The gradient of
<span class="math notranslate nohighlight">\(f\left( \mathbf{X} \right)=\mathbf{u}^{\rm{T}}\mathbf{\text{Xv}}:\mathbb{R}^{m \times n}\mathbb{\rightarrow R}\)</span>,
where <span class="math notranslate nohighlight">\(\mathbf{u} \in \mathbb{R}^{m},\mathbf{v} \in \mathbb{R}^{n}\)</span> are
constant vectors, is <span class="math notranslate nohighlight">\(\colorbox{fact}{$\nabla\mathbf{u}^{\rm{T}}\mathbf{\text{Xv}} \equiv \mathbf{u}\mathbf{v}^{\rm{T}}$}\)</span>.</p>
<p id="fact-gradient-vector-of-matrix-product"><span class="ititle2">Fact 1-4.</span> The gradient of <span class="math notranslate nohighlight">\(f\left( \mathbf{x} \right) = \mathbf{x}^{\rm{T}}\mathbf{\text{Ux}}:\mathbf{x} \in \mathbb{R}^{n}\mathbb{\rightarrow R}\)</span>,
where <span class="math notranslate nohighlight">\(\mathbf{U} \in \mathbb{R}^{n \times n}\)</span> is a constant matrix, is <span class="math notranslate nohighlight">\(\colorbox{fact}{$\nabla\mathbf{x}^{\rm{T}}\mathbf{\text{Ux}} = (\mathbf{U} + \mathbf{U}^{\rm{T}})\mathbf{x}$}\)</span>.</p>
<div class="section" id="theorem-1-1-gaussian-maximum-likelihood-estimators">
<span id="theorem-guassian-mle"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-1.</span> <span class="bemp">Gaussian maximum likelihood estimators.</span><a class="headerlink" href="#theorem-1-1-gaussian-maximum-likelihood-estimators" title="Permalink to this headline">¶</a></h3>Given a data matrix <span class="math notranslate nohighlight">\(\mathbf{X}=\left( \mathbf{x}_{1},\ldots,\mathbf{x}_{N} \right)\)</span>,
we can view its columns <span class="math notranslate nohighlight">\(\mathbf{x}_{1},\ldots,\mathbf{x}_{N}\)</span> as being
drawn i.i.d. drawn from a multivariate Gaussian distribution, then the
log-likelihood is
<div class="math notranslate nohighlight">
\[L \propto \sum_{i = 1}^{N}{- \frac{1}{2}\log\left| \mathbf{\Sigma} \right| - \frac{1}{2}\left( \mathbf{x}_{i} - \mathbf{μ} \right)^{\rm{T}}\mathbf{\Sigma}^{- 1}\left( \mathbf{x}_{i} - \mathbf{μ} \right)}
= \frac{N}{2}\log\left| \mathbf{\Sigma}^{- 1} \right| - \frac{1}{2}\sum_{i = 1}^{N}{\left( \mathbf{x}_{i} - \mathbf{μ} \right)^{\rm{T}}\mathbf{\Sigma}^{- 1}\left( \mathbf{x}_{i} - \mathbf{μ} \right)}\]</div>
<p>Using <a class="reference internal" href="__03_gaussian.html#fact-gradient-determinant"><span class="std std-ref">Fact 1-2</span></a> and <a class="reference internal" href="__03_gaussian.html#fact-gradient-matrix-of-matrix-product"><span class="std std-ref">Fact 1-3</span></a>, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-mle-inf">
<span class="eqno">(1.7)<a class="headerlink" href="#equation-eq-gaussian-mle-inf" title="Permalink to this equation">¶</a></span>\[\frac{\partial L}{\partial\mathbf{\Sigma}^{- 1}}
= \frac{N}{2}\mathbf{\Sigma}^{\rm{T}} - \frac{1}{2}\sum_{i = 1}^{N}{\left( \mathbf{x}_{i} - \mathbf{μ} \right)\left( \mathbf{x}_{i} - \mathbf{μ} \right)^{\rm{T}}}
= 0 \Rightarrow \mathbf{\Sigma}=\frac{1}{N}\sum_{i = 1}^{N}{\left( \mathbf{x}_{i} - \mathbf{μ} \right)\left( \mathbf{x}_{i} - \mathbf{μ} \right)^{\rm{T}}}\]</div>
<p>Then for <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span>, using <a class="reference internal" href="__03_gaussian.html#fact-gradient-vector-of-matrix-product"><span class="std std-ref">Fact 1-4</span></a>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp; L \propto \sum_{i = 1}^{N}{\mathbf{x}_{i}^{\rm{T}}\mathbf{\Sigma}^{- 1}\mathbf{μ} - \frac{1}{2}\mathbf{μ}^{\rm{T}}\mathbf{\Sigma}^{- 1}\mathbf{μ}} \\
&amp; \Rightarrow \frac{\partial L}{\partial\mathbf{μ}} = \sum_{i = 1}^{N}{\mathbf{x}_{i}^{\rm{T}}\mathbf{\Sigma}^{- 1} - \mathbf{\Sigma}^{- 1}\mathbf{μ}} = \mathbf{0} \\
&amp; \Rightarrow \mathbf{\Sigma}^{- 1}\left( \sum_{i = 1}^{N}\mathbf{x}_{i} \right) = N\mathbf{\Sigma}^{- 1}\mathbf{μ} \Rightarrow \colorbox{thm}{$\mathbf{μ}_{\text{ML}}=\frac{\sum_{i = 1}^{N}\mathbf{x}_{i}}{N} = \overline{\mathbf{x}}$} \end{aligned}\end{split}\]</div>
<p>Plug back to <span class="math notranslate nohighlight">\(\mathbf{\Sigma}^{- 1}\)</span> in (1?€?7) and we have</p>
<div class="math notranslate nohighlight">
\[\mathbf{\Sigma}_{\text{ML}}=\frac{1}{N}\sum_{i = 1}^{N}{\left( \mathbf{x}_{i} - \overline{\mathbf{x}} \right)\left( \mathbf{x}_{i} - \overline{\mathbf{x}} \right)^{\rm{T}}}\]</div>
<p>By <a class="reference internal" href="__02_cov.html#property-cov-as-rank1-sum"><span class="std std-ref">Property 1-3</span></a>, <span><span class="result-highlight"> <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{\text{ML}}\)</span> equals the biased sample covariance</span></span>, or <span><span class="result-highlight"> <span class="math notranslate nohighlight">\(\frac{N}{N - 1}\mathbf{\Sigma}_{\text{ML}}\)</span> equals the sample covariance matrix</span></span></p>
</div></li><li> Treating samples <span class="math notranslate nohighlight">\(\mathbf{x,y}\)</span> drawn from
<span class="math notranslate nohighlight">\(\operatorname{Gaussian}\left( \mathbf{μ},\mathbf{\Sigma} \right)\)</span> as
two RV vector where
<span class="math notranslate nohighlight">\(\mathbb{E}\mathbf{x} = \mathbb{E}\mathbf{y} = \mathbf{μ}\)</span>, recall we have <span class="math notranslate nohighlight">\(\mathbf{\Sigma =}\mathbb{E}\left\lbrack \mathbf{x}\mathbf{y}^{\rm{T}} \right\rbrack-\mathbf{μ}\mathbf{μ}^{\rm{T}}\)</span>
or <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack \mathbf{x}\mathbf{y}^{\rm{T}} \right\rbrack=\mathbf{μ}\mathbf{μ}^{\rm{T}}\mathbf{+ \Sigma}\)</span> by <a class="reference internal" href="__02_cov.html#property-cov-to-expectation"><span class="std std-ref">Property 1-1</span></a>. When <span class="math notranslate nohighlight">\(\mathbf{x},\mathbf{y}\)</span> are independent, we have
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack \mathbf{x}\mathbf{y}^{\rm{T}} \right\rbrack=\mathbf{μ}\mathbf{μ}^{\rm{T}}\)</span>.

<div class="section" id="theorem-1-2-bias-of-gaussian-mle">
<span id="theorem-guassian-mle-bias"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-2.</span> <span class="bemp">Bias of Gaussian MLE.</span><a class="headerlink" href="#theorem-1-2-bias-of-gaussian-mle" title="Permalink to this headline">¶</a></h3>Now given i.i.d. RVs <span class="math notranslate nohighlight">\(\mathbf{x}_{1},\ldots,\mathbf{x}_{N}\)</span> with mean <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span> and covariance <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>, note
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}\left\lbrack \mathbf{x}_{i}\mathbf{x}_{j}^{\rm{T}} \right\rbrack = \left\{ \begin{matrix}
\mathbf{μ}\mathbf{μ}^{\rm{T}} &amp; i \neq j \\
\mathbf{μ}\mathbf{μ}^{\rm{T}}\mathbf{+}\mathbf{\Sigma} &amp; i = j \\
\end{matrix} \right.\end{split}\]</div>
<p>Then we have</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left\lbrack \mathbf{x}_{i}{\overline{\mathbf{x}}}^{\rm{T}} \right\rbrack = \frac{1}{N}\sum_{i = 1}^{N}{\mathbb{E}\left\lbrack \mathbf{x}_{i}\mathbf{x}_{j}^{\rm{T}} \right\rbrack} = \frac{1}{N}\left( N\mathbf{μ}\mathbf{μ}^{\rm{T}} + \mathbf{\Sigma} \right) = \mathbf{μ}\mathbf{μ}^{\rm{T}} + \frac{\mathbf{\Sigma}}{N}\mathbb{= E}\left\lbrack \overline{\mathbf{x}}\mathbf{x}_{i}^{\rm{T}} \right\rbrack\]</div>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left\lbrack \overline{\mathbf{x}}{\overline{\mathbf{x}}}^{\rm{T}} \right\rbrack
= \frac{1}{N^{2}}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\mathbb{E}\left\lbrack \mathbf{x}_{i}\mathbf{x}_{j}^{\rm{T}} \right\rbrack}}
= \frac{1}{N^{2}}\left(N^{2}\mathbf{μ}\mathbf{μ}^{\rm{T}} + N\mathbf{\Sigma} \right)
= \mathbf{μ}\mathbf{μ}^{\rm{T}} + \frac{\mathbf{\Sigma}}{N}\]</div>
<p>For Gaussian i.i.d. RVs <span class="math notranslate nohighlight">\(\mathbf{x}_{1},\ldots,\mathbf{x}_{N}\)</span> as in</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\mathbf{E}\left\lbrack \mathbf{\Sigma}_{\text{ML}} \right\rbrack
&amp;=\frac{1}{N}\sum_{i = 1}^{N}{\mathbb{E}\left( \mathbf{x}_{i} - \overline{\mathbf{x}} \right)\left( \mathbf{x}_{i} - \overline{\mathbf{x}} \right)^{\rm{T}}}
= \frac{1}{N}\sum_{i = 1}^{N}{\mathbb{E}\left\lbrack \mathbf{x}_{i}\mathbf{x}_{i}^{\rm{T}} - \mathbf{x}_{i}{\overline{\mathbf{x}}}^{\rm{T}} - \overline{\mathbf{x}}\mathbf{x}_{i}^{\rm{T}} + \overline{\mathbf{x}}{\overline{\mathbf{x}}}^{\rm{T}} \right\rbrack} \\
&amp;= \frac{1}{N}\sum_{i = 1}^{N}\left( \cancel{\mathbf{μ}\mathbf{μ}^{\rm{T}}}\mathbf{+ \Sigma} - 2\left( \cancel{\mathbf{μ}\mathbf{μ}^{\rm{T}}} + \frac{\mathbf{\Sigma}}{N} \right) + \cancel{\mathbf{μ}\mathbf{μ}^{\rm{T}}} + \frac{\mathbf{\Sigma}}{N} \right)
= \frac{N - 1}{N}\mathbf{\Sigma}\end{aligned}\end{split}\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{\text{ML}} = \frac{N - 1}{N}\mathbf{\Sigma}\)</span> is a biased estimator. <span class="math notranslate nohighlight">\(\mathbf{μ}_{\text{ML}}\)</span> is trivially an unbiased
estimator, since
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack \mathbf{μ}_{\text{ML}} \right\rbrack=\mathbb{E}\left\lbrack \overline{\mathbf{x}} \right\rbrack = \mathbf{μ}\)</span>.</p>
</div></li><li>
<p id="fact-matrix-as-rank1-sum"><span class="ititle2">Fact 1-5.</span> Every matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> s.t.
<span class="math notranslate nohighlight">\(\operatorname{rank}\mathbf{A} = k\)</span> can be written as the sum of <span class="math notranslate nohighlight">\(k\)</span> rank-1 matrices, i.e.
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\mathbf{A} = \sum_{i = 1}^{k}{σ_{i}^{2}\mathbf{u}_{i}\mathbf{v}_{i}^{\rm{T}}}$}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{u}_{i}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}_{i}\)</span> are columns from two
matrices <span class="math notranslate nohighlight">\(\mathbf{U},\mathbf{V}\)</span> that come from the  <span><span class="exdef"> <span class="target" id="index-68"></span>reduced SVD</span></span> <span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{\text{U}\Lambda}\mathbf{V}^{\rm{T}}\)</span>; in particular, if
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is symmetric and positive semidefinite, then the singular
values are eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, and the reduced SVD becomes
reduced eigen-decomposition
<span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{\text{Q}\Lambda}\mathbf{Q}^{\rm{T}}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>
consists of <span class="math notranslate nohighlight">\(k\)</span> orthonormal eigenvectors
<span class="math notranslate nohighlight">\(\mathbf{q}_{1},\ldots,\mathbf{q}_{k}\)</span>, and thus
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\mathbf{A =}\sum_{i = 1}^{k}{𝜆_{i}\mathbf{q}_{i}\mathbf{q}_{i}^{\rm{T}}}$}\)</span>;
moreover, <span class="math notranslate nohighlight">\(\colorbox{fact}{$\mathbf{A}^{- 1}=\sum_{i = 1}^{k}{𝜆_{i}^{- 1}\mathbf{q}_{i}\mathbf{q}_{i}^{\rm{T}}}$}\)</span>
because <span class="math notranslate nohighlight">\(\mathbf{A}^{- 1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> share the same eigenspace for each eigenvalue.</p>

<div class="section" id="property-1-1-shape-of-contours-of-gaussian-density">
<span id="property-guassian-ellipsoid-contour"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Property 1-1.</span> <span class="bemp">Shape of contours of Gaussian density.</span><a class="headerlink" href="#property-1-1-shape-of-contours-of-gaussian-density" title="Permalink to this headline">¶</a></h3>Recall the  <span><span class="exdef"> <span class="target" id="index-69"></span>Mahalanobis distance</span></span>
<span class="math notranslate nohighlight">\(𝒹_{M}\left( \mathbf{x};\mathbf{μ,\Sigma} \right) = \left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{\Sigma}^{- 1}\left( \mathbf{x} - \mathbf{μ} \right)\)</span>.
Let <span class="math notranslate nohighlight">\(𝜆_{1},\ldots,𝜆_{m}\)</span> be the eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>, then
<span class="math notranslate nohighlight">\(\mathbf{\Sigma}^{- 1} = \sum_{i = 1}^{k}{𝜆_{i}^{- 1}\mathbf{q}_{i}\mathbf{q}_{i}^{\rm{T}}}\)</span> as a result of <a class="reference internal" href="__03_gaussian.html#fact-matrix-as-rank1-sum"><span class="std std-ref">Fact 1-5</span></a>, and
<div class="math notranslate nohighlight">
\[𝒹_{M}\left( \mathbf{x};\mathbf{μ,\Sigma} \right) = \sum_{i = 1}^{k}{𝜆_{i}^{- 1}\left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{q}_{i}\mathbf{q}_{i}^{\rm{T}}}\left( \mathbf{x} - \mathbf{μ} \right) = \sum_{i = 1}^{k}{\left( \frac{1}{\sqrt{𝜆_{i}}}\mathbf{q}_{i}^{\rm{T}}\left( \mathbf{x} - \mathbf{μ} \right) \right)^{\rm{T}}\left( \frac{1}{\sqrt{𝜆_{i}}}\mathbf{q}_{i}^{\rm{T}}\left( \mathbf{x} - \mathbf{μ} \right) \right)}\]</div>
<p>Therefore,
<span><span class="result-highlight"> <span class="math notranslate nohighlight">\(𝒹_{M}\left( \mathbf{x};\mathbf{μ,\Sigma} \right) = \sum_{i = 1}^{k}z_{i}^{2} = \mathbf{z}^{\rm{T}}\mathbf{z}\)</span>
where <span class="math notranslate nohighlight">\(z_{i} = \frac{1}{\sqrt{𝜆_{i}}}\mathbf{q}_{i}^{\rm{T}}\left( \mathbf{x} - \mathbf{μ} \right)\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{z} = \mathbf{\Lambda}^{- \frac{1}{2}}\mathbf{Q}\left( \mathbf{x} - \mathbf{μ} \right) = \left( z_{1},\ldots,z_{m} \right)^{\rm{T}}\)</span>,
and <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{\Lambda}^{\frac{1}{2}}\mathbf{Q}^{\rm{T}}\mathbf{z}\mathbf{+}\mathbf{μ}\)</span>. </span></span></p>
<p>Recall an orthonormal matrix represents a rotation (oriented rotation, to be exact), then
<span class="math notranslate nohighlight">\(\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{Q}^{\rm{T}}\left( \cdot \right)\mathbf{+}\mathbf{μ}\)</span>
geometrically transforms the standard frame on a unit circle to a frame on an ellipsoid centered at <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is the
coordinate of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> w.r.t. to the ellipsoid frame. Conversely,
given a contour level <span class="math notranslate nohighlight">\(p\left( \mathbf{x} \right) = p_{0}\)</span>, we have
<span class="math notranslate nohighlight">\(\mathbf{z}^{\rm{T}}\mathbf{z =}c \Rightarrow \left\| \mathbf{z} \right\|_{2} = \sqrt{c}\)</span>
for some constant <span class="math notranslate nohighlight">\(c\)</span> (i.e. the contour is a circle w.r.t. the ellipsoid frame), and then any <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> s.t.
<span class="math notranslate nohighlight">\(\left\| \mathbf{z} \right\|_{2} = \sqrt{c}\)</span> will be on an ellipsoid centered at <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span>.
Therefore, <span><span class="result-highlight"> the contours of multivariate Gaussian density are ellipsoids </span></span>.</p>
</div></li><li>
<p id="lemma-guassian-special-exponential-term-format"><span class="ititle">Lemma 1-1.</span> <span><span class="result-highlight"> If a density function
<span class="math notranslate nohighlight">\(p\left( \mathbf{x} \right) \propto \exp\left\{ - \frac{1}{2}\mathbf{x}^{\rm{T}}\mathbf{Ax +}\mathbf{x}^{\rm{T}}\mathbf{\text{Ay}} \right\}\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is symmetric positive semidefinite, then <span class="math notranslate nohighlight">\(p\)</span> must be Gaussian with precision <span class="math notranslate nohighlight">\(\mathbf{Ⲗ}=\mathbf{A}\)</span> and mean
<span class="math notranslate nohighlight">\(\mathbf{μ} = \mathbf{y}\)</span> </span></span>. This is simply we can rearrange it as</p>
<div class="math notranslate nohighlight">
\[p\left( \mathbf{x} \right) \propto \exp\left\{ - \frac{1}{2}\left( \mathbf{x - y} \right)^{\rm{T}}\mathbf{A}\left( \mathbf{x}-\mathbf{y} \right)-\mathbf{y}^{\rm{T}}\mathbf{\text{Ay}} \right\} \propto \exp\left\{ - \frac{1}{2}\left( \mathbf{x - y} \right)^{\rm{T}}\mathbf{A}\left( \mathbf{x}-\mathbf{y} \right) \right\}\]</div>
<p>There is no need to worry about normalization, since we have assumed <span class="math notranslate nohighlight">\(p\)</span> is a density, where its normalization is guaranteed.</p>
<p id="fact-matrix-block-inverse"><span class="ititle2">Fact 1-6.</span> If a block matrix <span class="math notranslate nohighlight">\(\begin{pmatrix} \mathbf{A} &amp; \mathbf{B} \\ \mathbf{C} &amp; \mathbf{D} \\ \end{pmatrix}\)</span> is inversible, then if <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is non-singular, we
have the following with all inverses valid</p>
<div class="math notranslate nohighlight">
\[\begin{split}\colorbox{fact}{$\begin{pmatrix}
\mathbf{A} &amp; \mathbf{B} \\
\mathbf{C} &amp; \mathbf{D} \\
\end{pmatrix}^{- 1} = \begin{pmatrix}
\mathbf{A}^{- 1}\left( \mathbf{I}\mathbf{+}\mathbf{\text{BMC}}\mathbf{A}^{- 1} \right) &amp; - \mathbf{A}^{- 1}\mathbf{\text{BM}} \\
- \mathbf{\text{MC}}\mathbf{A}^{- 1} &amp; \mathbf{M} \\
\end{pmatrix},\mathbf{M} = \left( \mathbf{D} - \mathbf{C}\mathbf{A}^{- 1}\mathbf{B} \right)^{- 1}$}\end{split}\]</div>
<p>and if <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> is non-singular, we have the following with all
inverses valid,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\colorbox{fact}{$\begin{pmatrix}
\mathbf{A} &amp; \mathbf{B} \\
\mathbf{C} &amp; \mathbf{D} \\
\end{pmatrix}^{- 1} = \begin{pmatrix}
\mathbf{M} &amp; - \mathbf{\text{MB}}\mathbf{D}^{- 1} \\
- \mathbf{D}^{- 1}\mathbf{\text{CM}} &amp; \mathbf{D}^{- 1}\left( \mathbf{I} + \mathbf{\text{CMB}}\mathbf{D}^{- 1} \right) \\
\end{pmatrix},\mathbf{M} = \left( \mathbf{A} - \mathbf{B}\mathbf{D}^{- 1}\mathbf{C} \right)^{- 1}$}\end{split}\]</div>

<div class="section" id="theorem-1-3-conditional-density-of-multivariate-guassian">
<span id="theorem-gaussian-conditional-density"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-3.</span> <span class="bemp">Conditional density of multivariate Guassian.</span><a class="headerlink" href="#theorem-1-3-conditional-density-of-multivariate-guassian" title="Permalink to this headline">¶</a></h3>Given
<span class="math notranslate nohighlight">\(\mathbf{x}\sim\operatorname{Gaussian}\left( \mathbf{μ},\mathbf{\Sigma} \right)\)</span>,
WLOG, partition <span class="math notranslate nohighlight">\(\mathbf{x} = \begin{pmatrix} \mathbf{x}_{a} \\ \mathbf{x}_{b} \\ \end{pmatrix}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{x}_{a} \in \mathbb{R}^{d}\)</span> is the
unknown, and <span class="math notranslate nohighlight">\(\mathbf{x}_{b} \in \mathbb{R}^{m - d}\)</span> is the condition, and we want to find the conditional density
<span class="math notranslate nohighlight">\(p\left( \mathbf{x}_{a}\mathbf{|}\mathbf{x}_{b} \right)\)</span>. Partition the mean and covariance accordingly as <span class="math notranslate nohighlight">\(\mathbf{μ} = \begin{pmatrix} \mathbf{μ}_{a} \\ \mathbf{μ}_{b} \\ \end{pmatrix}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{\Sigma} = \begin{pmatrix} \mathbf{\Sigma}_{{aa}} &amp; \mathbf{\Sigma}_{{ab}} \\ \mathbf{\Sigma}_{{ba}} &amp; \mathbf{\Sigma}_{{bb}} \\ \end{pmatrix}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Ⲗ} = \begin{pmatrix} \mathbf{Ⲗ}_{{aa}} &amp; \mathbf{Ⲗ}_{{ab}} \\ \mathbf{Ⲗ}_{{ba}} &amp; \mathbf{Ⲗ}_{{bb}} \\ \end{pmatrix}\)</span>.
Assume <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{{aa}}\)</span> is non-singular, then we have
<div class="math notranslate nohighlight" id="equation-eq-gaussian-conditional-mean-key-inference">
<span class="eqno">(1.8)<a class="headerlink" href="#equation-eq-gaussian-conditional-mean-key-inference" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned} - \frac{1}{2}\left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{Ⲗ}\left( \mathbf{x} - \mathbf{μ} \right) &amp;= \begin{pmatrix} \mathbf{x}_{a}-\mathbf{μ}_{a} \\ \mathbf{x}_{b}-\mathbf{μ}_{b} \\ \end{pmatrix}^{\rm{T}}\begin{pmatrix} \mathbf{Ⲗ}_{{aa}} &amp; \mathbf{Ⲗ}_{{ab}} \\
\mathbf{Ⲗ}_{{ba}} &amp; \mathbf{Ⲗ}_{{bb}} \\ \end{pmatrix}\begin{pmatrix} \mathbf{x}_{a}-\mathbf{μ}_{a} \\ \mathbf{x}_{b}-\mathbf{μ}_{b} \\ \end{pmatrix} \\
&amp;= - \frac{1}{2}\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right)^{\rm{T}}\mathbf{Ⲗ}_{{aa}}\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right) - \left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right)^{\rm{T}}\mathbf{Ⲗ}_{{ab}}\left( \mathbf{x}_{b}-\mathbf{μ}_{b} \right)-\frac{1}{2}\left( \mathbf{x}_{b}-\mathbf{μ}_{b} \right)^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\left( \mathbf{x}_{b}-\mathbf{μ}_{b} \right) \\
&amp;= - \frac{1}{2}\mathbf{x}_{a}^{\rm{T}}\mathbf{Ⲗ}_{{aa}}\mathbf{x}_{a} + \mathbf{x}_{a}^{\rm{T}}\mathbf{Ⲗ}_{{aa}}\mathbf{μ}_{a}-\mathbf{x}_{a}^{\rm{T}}\mathbf{Ⲗ}_{{ab}}\left( \mathbf{x}_{b}-\mathbf{μ}_{b} \right)\mathbf{+}\text{constant} \\
&amp;= - \frac{1}{2}\mathbf{x}_{a}^{\rm{T}}\mathbf{Ⲗ}_{{aa}}\mathbf{x}_{a} + \mathbf{x}_{a}^{\rm{T}}\mathbf{Ⲗ}_{{aa}}\left( \mathbf{μ}_{a}-\mathbf{Ⲗ}_{{aa}}^{- 1}\mathbf{Ⲗ}_{{ab}}\left( \mathbf{x}_{b}-\mathbf{μ}_{b} \right) \right)\mathbf{+}\text{constant} \end{aligned}\end{split}\]</div>
<p>By <a class="reference internal" href="__03_gaussian.html#lemma-guassian-special-exponential-term-format"><span class="std std-ref">Lemma 1-1</span></a>, it follows that <span><span class="theorem-highlight"> if <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{{aa}}\)</span> is
non-singular, then <span class="math notranslate nohighlight">\(p\left( \mathbf{x}_{a}\mathbf{|}\mathbf{x}_{b} \right)\)</span> is Gaussian </span></span>.
Denote <span class="math notranslate nohighlight">\(\mathbf{x}_{a}\mathbf{|}\mathbf{x}_{b}\mathbf{\sim}\operatorname{Gaussian}\left( \mathbf{μ}_{a|b}\mathbf{,}\mathbf{\Sigma}_{a|b} \right)\)</span>, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-conditional-mean-and-cov-a">
<span class="eqno">(1.9)<a class="headerlink" href="#equation-eq-gaussian-conditional-mean-and-cov-a" title="Permalink to this equation">¶</a></span>\[\colorbox{theorem}{$\mathbf{μ}_{a|b}=\mathbf{μ}_{a}-\mathbf{Ⲗ}_{{aa}}^{- 1}\mathbf{Ⲗ}_{{ab}}\left( \mathbf{x}_{b}-\mathbf{μ}_{b} \right)\mathbf{,}\mathbf{\Sigma}_{a|b}=\mathbf{Ⲗ}_{{aa}}^{- 1}$}\]</div>
<p>If in addition <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{{bb}}\)</span> is non-singular, using
<a class="reference internal" href="__03_gaussian.html#fact-matrix-block-inverse"><span class="std std-ref">Fact 1-6</span></a> and <span class="math notranslate nohighlight">\(\begin{pmatrix} \mathbf{\Sigma}_{{aa}} &amp; \mathbf{\Sigma}_{{ab}} \\ \mathbf{\Sigma}_{{ba}} &amp; \mathbf{\Sigma}_{{bb}} \\ \end{pmatrix}^{- 1} = \begin{pmatrix} \mathbf{Ⲗ}_{{aa}} &amp; \mathbf{Ⲗ}_{{ab}} \\ \mathbf{Ⲗ}_{{ba}} &amp; \mathbf{Ⲗ}_{{bb}} \\ \end{pmatrix}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\{ {\begin{array}{*{20}{l}}
{{Ⲗ _{aa}} = {{\left( {{{\mathbf{\Sigma }}_{aa}} - {{\mathbf{\Sigma }}_{ab}}{\mathbf{\Sigma }}_{bb}^{ - 1}{{\mathbf{\Sigma }}_{ba}}} \right)}^{ - 1}}} \\
{{Ⲗ _{ab}} =  - {Ⲗ _{aa}}{{\mathbf{\Sigma }}_{ab}}{\mathbf{\Sigma }}_{bb}^{ - 1}}
\end{array}} \right. \Rightarrow \left\{ {\begin{array}{*{20}{l}}
{Ⲗ _{aa}^{ - 1}{Ⲗ _{ab}} =  - {{\mathbf{\Sigma }}_{ab}}{\mathbf{\Sigma }}_{bb}^{ - 1}} \\
{Ⲗ _{aa}^{ - 1} = {{\mathbf{\Sigma }}_{aa}} - {{\mathbf{\Sigma }}_{ab}}{\mathbf{\Sigma }}_{bb}^{ - 1}{{\mathbf{\Sigma }}_{ba}}}
\end{array}} \right.\end{split}\]</div>
<p>Plug into <a class="reference internal" href="#equation-eq-gaussian-conditional-mean-and-cov-a">Eq.1.9</a> we have</p>
<div class="math notranslate nohighlight">
\[\colorbox{theorem}{${{\mathbf{μ }}_{a|b}}
= {{\mathbf{μ }}_a} + {{\mathbf{\Sigma }}_{ab}}{\mathbf{\Sigma }}_{bb}^{ - 1}\left( {{{\mathbf{x}}_b} - {{\mathbf{μ }}_b}} \right),{{\mathbf{\Sigma }}_{a|b}}
= {{\mathbf{\Sigma }}_{aa}} - {{\mathbf{\Sigma }}_{ab}}{\mathbf{\Sigma }}_{bb}^{ - 1}{{\mathbf{\Sigma }}_{ba}}$}\]</div>
<p>Similarly, if <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{{bb}}\)</span> is non-singular, then <span class="math notranslate nohighlight">\(p\left( \mathbf{x}_{b}|\mathbf{x}_{a} \right)\)</span> is Gaussian. Following
<a class="reference internal" href="#equation-eq-gaussian-conditional-mean-key-inference">Eq.1.8</a> we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned} - \frac{1}{2}\left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{Ⲗ}\left( \mathbf{x} - \mathbf{μ} \right) &amp;= - \frac{1}{2}\mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\mathbf{x}_{b} + \mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\mathbf{μ}_{b}-\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right)^{\rm{T}}\mathbf{Ⲗ}_{{ab}}\mathbf{x}_{b}\mathbf{+}\text{constant} \\
&amp;= - \frac{1}{2}\mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\mathbf{x}_{b} + \mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\mathbf{μ}_{b}-\mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{ba}}\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right)\mathbf{+}\text{constant} \\
&amp;= - \frac{1}{2}\mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\mathbf{x}_{b} + \mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\left( \mathbf{μ}_{b}-\mathbf{Ⲗ}_{{bb}}^{- 1}\mathbf{Ⲗ}_{{ba}}\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right) \right)\mathbf{+}\text{constant}\end{aligned}\end{split}\]</div>
<p>If in addition <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{{aa}}\)</span> is non-singular, using
<a class="reference internal" href="__03_gaussian.html#fact-matrix-block-inverse"><span class="std std-ref">Fact 1-6</span></a> we have</p>
<div class="math notranslate nohighlight">
\[\mathbf{Ⲗ}_{{bb}}^{- 1}\mathbf{Ⲗ}_{{ba}}\mathbf{= -}\mathbf{\Sigma}_{{ba}}\mathbf{\Sigma}_{{aa}}^{- 1}\mathbf{,}\mathbf{Ⲗ}_{{bb}}^{- 1}=\mathbf{\Sigma}_{{bb}}-\mathbf{\Sigma}_{{ba}}\mathbf{\Sigma}_{{aa}}^{- 1}\mathbf{\Sigma}_{{ab}}\]</div>
<p>Finally,</p>
<div class="math notranslate nohighlight">
\[\colorbox{theorem}{$\mathbf{μ}_{b|a}=\mathbf{μ}_{b}-\mathbf{Ⲗ}_{{bb}}^{- 1}\mathbf{Ⲗ}_{{ba}}\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right)=\mathbf{μ}_{b}\mathbf{+}\mathbf{\Sigma}_{{ba}}\mathbf{\Sigma}_{{aa}}^{- 1}\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right)$}\]</div>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-conditional-covariance-b">
<span class="eqno">(1.10)<a class="headerlink" href="#equation-eq-gaussian-conditional-covariance-b" title="Permalink to this equation">¶</a></span>\[\colorbox{theorem}{$\mathbf{\Sigma}_{b|a}=\mathbf{Ⲗ}_{{bb}}^{- 1}=\mathbf{\Sigma}_{{bb}}-\mathbf{\Sigma}_{{ba}}\mathbf{\Sigma}_{{aa}}^{- 1}\mathbf{\Sigma}_{{ab}}$}\]</div>
<p>We <span class="emp">note</span> 1) <span><span class="comment-highlight"> the conditional mean and variance can be simpler in terms of precision, as seen in <a class="reference internal" href="#equation-eq-gaussian-conditional-mean-and-cov-a">Eq.1.9</a> and <a class="reference internal" href="#equation-eq-gaussian-conditional-covariance-b">Eq.1.10</a> </span></span>;
2) <span><span class="comment-highlight"> the conditional mean <span class="math notranslate nohighlight">\(\mathbf{μ}_{a|b}\)</span> is independent of <span class="math notranslate nohighlight">\(\mathbf{x}_{a}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{μ}_{b|a}\)</span> is independent of
<span class="math notranslate nohighlight">\(\mathbf{x}_{b}\)</span>, and both <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{a|b}\mathbf{,}\mathbf{\Sigma}_{b|a}\)</span> are independent of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> </span></span>, which is expected because the conditional mean and covariance should be determined by the distribution itself and the conditions,
but should not be related to the “unknown” RVs.</p>
</li></ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../02_neural_networks/00_index.html" class="btn btn-neutral float-right" title="2. Basic Neural Networks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../../index.html" class="btn btn-neutral" title="Study Notes in Machine Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Tony Chen, Drexel University.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'0.0.1',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>