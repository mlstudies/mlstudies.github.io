

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>1. Preliminaries &mdash; Study Notes in Machine Learning 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/coloring.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/eqposfix.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2. Basic Neural Networks" href="../02_neural_networks/00_index.html" />
    <link rel="prev" title="Study Notes in Machine Learning" href="../../index.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> Study Notes in Machine Learning
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">1. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview-of-probability-measure-theory">1.1. Overview of Probability Measure Theory</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#measure-probability-space-random-variable-radon-nikodym-derivative-and-stochastic-process">1.1.1. Measure, Probability Space, Random Variable, Radon-Nikodym Derivative and Stochastic Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="#marginalization-conditional-expectation-conditional-probablity-and-bayes-rules">1.1.2. Marginalization, Conditional Expectation, Conditional Probablity and Bayes’ Rules</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#covariance-matrix">1.2. Covariance Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#property-1-4-classic-representation-of-covariance-by-expectation-or-mean">1.2.1. <span class="ititle">Property 1-4.</span> <span class="bemp">Classic representation of covariance by expectation (or mean).</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#property-1-5-invariance-to-centralization">1.2.2. <span class="ititle">Property 1-5.</span> <span class="bemp">Invariance to centralization.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-5-matrix-arithmetics-of-covariance-matrix">1.2.3. <span class="ititle">Theorem 1-5.</span> <span class="bemp">Matrix arithmetics of covariance matrix.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-6-positive-definiteness-of-covariance-matrix">1.2.4. <span class="ititle">Theorem 1-6.</span> <span class="bemp">Positive definiteness of covariance matrix.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#property-1-6-sample-covariance-represneted-by-rank-1-sum">1.2.5. <span class="ititle">Property 1-6.</span> <span class="bemp">Sample covariance represneted by rank-1 sum.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-7-block-decomposition-of-covariance-matrix">1.2.6. <span class="ititle">Theorem 1-7.</span> <span class="bemp">Block decomposition of covariance matrix.</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#multivariate-gaussian-distribution">1.3. Multivariate Gaussian Distribution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-8-gaussian-maximum-likelihood-estimators">1.3.1. <span class="ititle">Theorem 1-8.</span> <span class="bemp">Gaussian maximum likelihood estimators.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-9-bias-of-gaussian-mle">1.3.2. <span class="ititle">Theorem 1-9.</span> <span class="bemp">Bias of Gaussian MLE.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#property-1-7-shape-of-contours-of-gaussian-density">1.3.3. <span class="ititle">Property 1-7.</span> <span class="bemp">Shape of contours of Gaussian density.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-10-conditional-density-of-multivariate-guassian">1.3.4. <span class="ititle">Theorem 1-10.</span> <span class="bemp">Conditional density of multivariate Guassian.</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../02_neural_networks/00_index.html">2. Basic Neural Networks</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Study Notes in Machine Learning</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</div></li><li>1. Preliminaries</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/3_ml_methods/00_basics/00_index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["color.js","autoload-all.js"] }
  });

      MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
   var color = MathJax.Extension["TeX/color"];
   color.colors["theorem"] = color.getColor('RGB','255,229,153');
       color.colors["result"] = color.getColor('RGB','189,214,238');
       color.colors["fact"] = color.getColor('RGB','255,255,204');
       color.colors["emperical"] = color.getColor('RGB','253,240,207');
       color.colors["comment"] = color.getColor('RGB','204,255,204');
   color.colors["thm"] = color.getColor('RGB','255,229,153');
       color.colors["rlt"] = color.getColor('RGB','189,214,238');
       color.colors["emp"] = color.getColor('RGB','253,240,207');
       color.colors["comm"] = color.getColor('RGB','204,255,204');
       color.colors["conn1"] = color.getColor('RGB','255,0,255');
       color.colors["conn2"] = color.getColor('RGB','237,125,49');
       color.colors["conn3"] = color.getColor('RGB','112,48,160');
      });
</script><div class="section" id="preliminaries">
<h1>1. Preliminaries<a class="headerlink" href="#preliminaries" title="Permalink to this headline">¶</a></h1>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["color.js","autoload-all.js"] }
  });

      MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
   var color = MathJax.Extension["TeX/color"];
   color.colors["theorem"] = color.getColor('RGB','255,229,153');
       color.colors["result"] = color.getColor('RGB','189,214,238');
       color.colors["fact"] = color.getColor('RGB','255,255,204');
       color.colors["emperical"] = color.getColor('RGB','253,240,207');
       color.colors["comment"] = color.getColor('RGB','204,255,204');
   color.colors["thm"] = color.getColor('RGB','255,229,153');
       color.colors["rlt"] = color.getColor('RGB','189,214,238');
       color.colors["emp"] = color.getColor('RGB','253,240,207');
       color.colors["comm"] = color.getColor('RGB','204,255,204');
       color.colors["conn1"] = color.getColor('RGB','255,0,255');
       color.colors["conn2"] = color.getColor('RGB','237,125,49');
       color.colors["conn3"] = color.getColor('RGB','112,48,160');
      });
</script><div class="section" id="overview-of-probability-measure-theory">
<h2>1.1. Overview of Probability Measure Theory<a class="headerlink" href="#overview-of-probability-measure-theory" title="Permalink to this headline">¶</a></h2>
<p><span style="padding-left:20px"></span> Modern  <span><span class="exdef"> <span class="target" id="index-0"></span>probability theory</span></span> is built upon  <span><span class="exdef"> <span class="target" id="index-1"></span>measure theory</span></span> for rigorous
insight into the nature of probabilities, where even an introductory
course needs hundreds of pages’ discourse and deep background from at least  <span><span class="exdef"> <span class="target" id="index-2"></span>real analysis</span></span>
(topology, limits, convergence, bounded variation, measurability, integration, differentiation, <span class="math notranslate nohighlight">\(\mathcal{L}^p\)</span> space, …). It is way
beyond the scope of this text and most of the theories and proofs do not have close connection with real-world problems.
Here we concisely overview basic constructions and
results from modern probability measure theory which could potentially help with a
better understanding of machine learning models that heavily involve probabilities, like Gaussian process or generative models.
We treat it as rigorously as we can although it is an overview, and we provide the best intuition we can conceive.</p>
<div class="section" id="measure-probability-space-random-variable-radon-nikodym-derivative-and-stochastic-process">
<h3>1.1.1. Measure, Probability Space, Random Variable, Radon-Nikodym Derivative and Stochastic Process<a class="headerlink" href="#measure-probability-space-random-variable-radon-nikodym-derivative-and-stochastic-process" title="Permalink to this headline">¶</a></h3>
<p><span style="padding-left:20px"></span> In probability theory, a set <span class="math notranslate nohighlight">\(\Omega\)</span> containing all possible outcomes
of an  <span><span class="def"> <span class="target" id="index-3"></span>experiment</span></span> is called a  <span><span class="def"> <span class="target" id="index-4"></span>sample space</span></span>. <span class="emp">For example</span>, in the
experiment of coin flip, we can define <span class="math notranslate nohighlight">\(\Omega = \left\{ 0,1 \right\}\)</span>
where <span class="math notranslate nohighlight">\(0\)</span> represents head and <span class="math notranslate nohighlight">\(1\)</span> represents tail. <span class="math notranslate nohighlight">\(\Omega\)</span> can be
finite, countably infinite or uncountably infinite. <span class="emp">Note</span>
this “experiment” is not a concrete “experiment” we do in machine
learning to measure performance of a model; rather it is an abstract
concept, and can be mathematically viewed as being defined by the sample
space – if we know what the sample space is, then the “experiment” is
completely determined. <span class="emp">For example</span>, if we let
<span class="math notranslate nohighlight">\(\Omega = \left\{ 0,1 \right\}\)</span>, then the experiment behind this
<span class="math notranslate nohighlight">\(\Omega\)</span> is mathematically equivalent to the coin-flip experiment.</p>
<p><span style="padding-left:20px"></span> In addition, we define a set <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> that contains subsets of
<span class="math notranslate nohighlight">\(\Omega\)</span> and is named  <span><span class="def"> <span class="target" id="index-5"></span>𝜎-algebra</span></span>. It must satisfy three axioms: <strong>1)</strong>
<span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is non-empty; <strong>2)</strong> for any <span class="math notranslate nohighlight">\(A\in\mathcal{F}\)</span> then
<span class="math notranslate nohighlight">\(\Omega\backslash A\in\mathcal{F}\)</span>; <strong>3)</strong> for countable many
<span class="math notranslate nohighlight">\(A_{\mathrm{1}},A_{2},\text{...},A_{n}\mathcal{,\ldots \in F}\)</span>, then
<span class="math notranslate nohighlight">\(\bigcup A_{k}\in\mathcal{F}\)</span>.
The last axiom actually implies <span><span class="result-highlight"> the union of any finite number of sets
<span class="math notranslate nohighlight">\(A_{1},\ldots,A_{N}\)</span> is in <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> </span></span> because we can expand it to a
countable sequence <span class="math notranslate nohighlight">\(A_{1},\ldots,A_{N},A_{N + 1}\ldots\)</span> where
<span class="math notranslate nohighlight">\(A_{N + 1} = A_{N + 2} = \ldots = \Omega\backslash\left\{ \bigcup_{k = 1}^{N}A_{k} \right\}\)</span>
and then
<span class="math notranslate nohighlight">\(\bigcup_{k = 1}^{N}A_{k} = \Omega\backslash\left\{ \bigcup_{k = N + 1}^{\infty}A_{k} \right\}\in\mathcal{F}\)</span>.
It is also easy to check <span><span class="result-highlight"> if <span class="math notranslate nohighlight">\(\Omega\)</span> is
finite or countable, then 𝜎-algebra is exactly the <a href="#id1"><span class="problematic" id="id2">*</span></a><span class="exdef">power set</span>
of <span class="math notranslate nohighlight">\(\Omega\)</span> </span></span>. 𝜎-algebra is the generalization of the concept of
power set of a finite set to an  <span><span class="exdef"> <span class="target" id="index-6"></span>uncountable set</span></span>. Not every set of in
the power set of an uncountable set is mathematically measurable; as a
result, length, area or volume, or generally a <a class="reference internal" href="#measure">measure</a> (see
below) cannot be properly defined for such set (see  <span><span class="exdef"> <span class="target" id="index-7"></span>Vitali set</span></span> if
interested). 𝜎-algebra excludes these unmeasurable sets from the
power set. Each set in <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is called a  <span><span class="def"> <span class="target" id="index-8"></span>measurable set</span></span> in
general mathematical context, and is referred to as an  <span><span class="def"> <span class="target" id="index-9"></span>event</span></span> in a
probabilistic context. The tuple <span class="math notranslate nohighlight">\(\left( \Omega,\mathcal{F} \right)\)</span> is
called a  <span><span class="def"> <span class="target" id="index-10"></span>measurable space</span></span>.</p>
<p><span style="padding-left:20px"></span> Given a sample space <span class="math notranslate nohighlight">\(\Omega\)</span> and a set <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> of subsets
of <span class="math notranslate nohighlight">\(\Omega\)</span>, then we say <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is a  <span><span class="def"> <span class="target" id="index-11"></span>σ-algebra generated by event sets</span></span> <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> if <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is
constructed as according to the axioms: <strong>1)</strong> let
<span class="math notranslate nohighlight">\(\Omega\mathcal{,\varnothing \in F}\)</span>; <strong>2)</strong> <span class="math notranslate nohighlight">\(\forall A \in \mathcal{A}\)</span>,
let <span class="math notranslate nohighlight">\(\Omega\backslash A \in \mathcal{F}\)</span>; <strong>3)</strong> for any countable many
<span class="math notranslate nohighlight">\(A_{1},A_{2}\mathcal{,\ldots \in A}\)</span>, let
<span class="math notranslate nohighlight">\(\bigcup_{k = 1}^{\infty}A_{k}\in\mathcal{F}\)</span>. We denote
<span class="math notranslate nohighlight">\(\mathcal{F =}σ\left( \mathcal{A} \right)\)</span>. We also say
<span class="math notranslate nohighlight">\(σ\left( \mathcal{A} \right)\)</span> is the  <span><span class="def"> <span class="target" id="index-12"></span>σ-closure</span></span> of a set
– the minimum σ-algebra that contain all sets in <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>.
<span class="emp">For a concrete example</span>, suppose the sample space is
<span class="math notranslate nohighlight">\(\Omega = \left\{ 1,2,3,4,5,6 \right\}\)</span> (e.g. the experiment of throwing
a dice), and then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp; σ \left( \left\{ \left\{ 1,3 \right\},\left\{ 5 \right\},\left\{ 2 \right\},\left\{ 4,6 \right\} \right\} \right) \\
&amp;= \left\{ \Omega,\left\{ 1,3 \right\},\left\{ 5 \right\},\left\{ 2 \right\},\left\{ 4,6 \right\},
\overset{\Omega\backslash\left\{ 1,3 \right\}}{\overbrace{\left\{ 2,4,5,6 \right\}}},
\overset{\Omega\backslash\left\{ 5 \right\}}{\overbrace{\left\{ 1,2,3,4,6 \right\}}},\ldots,
\overset{\left\{ 1,3 \right\}\bigcup\left\{ 5 \right\}}{\overbrace{\left\{ 1,3,5 \right\}}},
\overset{\left\{ 1,3 \right\}\bigcup\left\{ 2 \right\}}{\overbrace{\left\{ 1,2,3 \right\}}},\ldots,\varnothing \right\} \end{aligned}\end{split}\]</div>
<p>An important generated σ-algebra is the  <span><span class="def"> <span class="target" id="index-13"></span>Borel σ-algebra</span></span>. For <span class="math notranslate nohighlight">\(\mathbb{R}^{n}\)</span>, we denote its Borel σ-algebra
as <span class="math notranslate nohighlight">\(\mathcal{B}\left( \mathbb{R}^{n} \right)\)</span>, defined
as the σ-algebra generated by all  <span><span class="exdef"> <span class="target" id="index-14"></span>open sets</span></span> of
<span class="math notranslate nohighlight">\(\mathbb{R}^{n}\)</span>. It is the same for a general set <span class="math notranslate nohighlight">\(S\)</span>, whose Borel σ-algebra as <span class="math notranslate nohighlight">\(\mathcal{B}\left( S \right)\)</span> is the
σ-algebra generated by all  <span><span class="exdef"> <span class="target" id="index-15"></span>open sets</span></span> defined for <span class="math notranslate nohighlight">\(S\)</span>. The
concept of open sets for <span class="math notranslate nohighlight">\(\mathbb{R}^{n}\)</span> or <span class="math notranslate nohighlight">\(S\)</span> is taught in real
analysis and general topology, and is beyond the scope of this text.</p>
<p id="measure"><span style="padding-left:20px"></span> A measure is mathematical concept that rigorously models essential
real-life concepts like length, area, volume, etc. The most widely used
measure is called  <span><span class="exdef"> <span class="target" id="index-16"></span>Lebesgue measure</span></span>, which is consistent with our
common sense. Under this measure, the length of a line, the area of a
square, or the volume of a ball, etc. is calculated with our usual
formulas. A  <span><span class="def"> <span class="target" id="index-17"></span>measure</span></span> <span class="math notranslate nohighlight">\(μ\)</span> is formally defined over a 𝜎-algebra <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> as
a <span class="math notranslate nohighlight">\(\mathcal{F} \rightarrow \lbrack 0, + \infty)\)</span> function s.t. <span><span class="fact-highlight"> <strong>1)</strong>
<span class="math notranslate nohighlight">\(\mathbb{P}\left( \varnothing \right) = 0\)</span>; <strong>2)</strong>
<span class="math notranslate nohighlight">\(A,B\in\mathcal{F,}A\bigcap B\mathbb{= \varnothing \Rightarrow P}\left( A \right)\mathbb{+ P}\left( B \right)=\mathbb{P}\left( A\bigcup B \right)\)</span> </span></span>.
A measure has many intuitive properties we would expect from “area” or
“volume”, for example, let <span class="math notranslate nohighlight">\(A,B \in \mathcal{F}\)</span>, then
<span class="math notranslate nohighlight">\(μ\left( A\bigcup B \right) \leq μ\left( A \right) + μ\left( B \right)\)</span>
and <span class="math notranslate nohighlight">\(A \subseteq B \Rightarrow μ\left( A \right) \leq μ\left( B \right)\)</span>.
If for any countable many measurable sets
<span class="math notranslate nohighlight">\(A_{i}\in \mathcal{ F,}i = 1,2,\ldots\)</span> we have
<span class="math notranslate nohighlight">\(μ\left( \bigcup_{i}^{}A_{i} \right) &lt; + \infty\)</span>, we say <span class="math notranslate nohighlight">\(μ\)</span> is a  <span><span class="def"> <span class="target" id="index-18"></span>𝜎-finite measure</span></span>.
Probability is formally defined a measure over each event in
<span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, called  <span><span class="def"> <span class="target" id="index-19"></span>probability measure</span></span>, usually denoted by
<span class="math notranslate nohighlight">\(\mathbb{P}\)</span>. <span><span class="fact-highlight"> A probability measure has all axioms of a measure, and adds a third axiom
that <span class="math notranslate nohighlight">\(\mathbb{P}\left( \Omega \right) = 1\)</span> </span></span>, called the  <span><span class="def"> <span class="target" id="index-20"></span>probability normalization axiom</span></span> . For example, if <span class="math notranslate nohighlight">\(\Omega = \left\{ 0,1 \right\}\)</span>,
then <span class="math notranslate nohighlight">\(\left\{ \left\{ 0 \right\},\left\{ 1 \right\},\left\{ 0,1 \right\},\varnothing \right\}\)</span>
is a 𝜎-algebra of <span class="math notranslate nohighlight">\(\Omega\)</span> and we can define
<span class="math notranslate nohighlight">\(\mathbb{P}\left( \left\{ 0 \right\} \right) = 0.3,\mathbb{P}\left( \left\{ 1 \right\} \right) = 0.7,\mathbb{P}\left( \left\{ 0,1 \right\} \right) = 1,\mathbb{P}\left( \varnothing \right) = 0\)</span>.
We call <span class="math notranslate nohighlight">\(\left( \Omega,\mathcal{F,}μ \right)\)</span> a  <span><span class="def"> <span class="target" id="index-21"></span>measure space</span></span> for
a general measure <span class="math notranslate nohighlight">\(μ\)</span>, can call
<span class="math notranslate nohighlight">\(\left( \Omega,\mathcal{F}\mathbb{,P} \right)\)</span> a  <span><span class="def"> <span class="target" id="index-22"></span>probability space</span></span>.</p>
<p><span style="padding-left:20px"></span> Given two measurable spaces <span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right)\)</span> and
<span class="math notranslate nohighlight">\(\left( S,\mathcal{E} \right)\)</span>, a function <span class="math notranslate nohighlight">\(f:\Omega \rightarrow S\)</span> is
called a  <span><span class="def"> <span class="target" id="index-23"></span>measurable function</span></span> if
<span class="math notranslate nohighlight">\(f^{- 1}\left( B \right)\in\mathcal{F,}\forall B \in \mathcal{E}\)</span>. We also say <span class="math notranslate nohighlight">\(f\)</span> is a
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right) \rightarrow (S\mathcal{,E)}\)</span> function.
When we choose
<span class="math notranslate nohighlight">\(\left( S,\mathcal{E} \right) = \left( \mathbb{R,}\mathcal{B}\left( \mathbb{R} \right) \right)\)</span>,
we also simply say a
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right) \rightarrow \left( \mathbb{R,}\mathcal{B}\left( \mathbb{R} \right) \right)\)</span>
function <span class="math notranslate nohighlight">\(f\)</span> as a  <span><span class="def"> <span class="target" id="index-24"></span>ℱ-measurable function</span></span>. <span class="emp">To make an intuition</span> of what a measurable function should look like,
think about an at most countable partition
<span class="math notranslate nohighlight">\(\mathcal{P =}\left\{ A_{1},A_{2},\ldots \right\}\)</span> of <span class="math notranslate nohighlight">\(\Omega\)</span>. <span class="emp">Note</span> for any set <span class="math notranslate nohighlight">\(A\)</span> in the generated σ-algebra
<span class="math notranslate nohighlight">\(σ\left( \mathcal{P} \right)\)</span> and any <span class="math notranslate nohighlight">\(A_{i}\)</span>, it is easy to verify
either <span class="math notranslate nohighlight">\(A\bigcap A_{i} = \varnothing\)</span> or <span class="math notranslate nohighlight">\(A_{i} \subseteq A\)</span>, and there
is no other possibility. Then <span><span class="result-highlight"> a <span class="math notranslate nohighlight">\(σ\left( \mathcal{P} \right)\)</span>-measurable real-valued function <span class="math notranslate nohighlight">\(f\)</span>
must be a piecewise constant function </span></span> s.t.
<span class="math notranslate nohighlight">\(f\left( x \right) \equiv c_{i},\forall x \in A_{i},i = 1,2,\ldots\)</span>
Suppose not, then there exists at least
<span class="math notranslate nohighlight">\(x_{i}^{\left( 1 \right)},x_{i}^{\left( 2 \right)} \in A_{i}\)</span> for some
<span class="math notranslate nohighlight">\(i\)</span> s.t.
<span class="math notranslate nohighlight">\(f\left( x_{i}^{\left( 1 \right)} \right) \neq f\left( x_{i}^{\left( 2 \right)} \right)\)</span>,
then <span class="math notranslate nohighlight">\(f^{- 1}\left( x_{i}^{\left( 1 \right)} \right)\)</span> is a set in
<span class="math notranslate nohighlight">\(σ\left( \mathcal{P} \right)\)</span> containing only part of <span class="math notranslate nohighlight">\(A_{i}\)</span>,
contradiction. Therefore, the concept of measurable function can be
viewed as generalizing the piecewise constant function. <span><span class="comment-highlight"> Many counter-intuitive statements in probability theory
involving σ-algebra can make more sense if we check the special case of a σ-algebra generated by a countable partition</span></span>.
<span class="emp">The technical purpose in plain words</span> for measurable
function is to ensure any preimage a measurable set in <span class="math notranslate nohighlight">\(\mathcal{E}\)</span> can
be found in <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> in order to be “measured” by a measure defined
on <span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right)\)</span>, we can see this more clearly later.</p>
<ul style="margin-left:20px">
<p><li style="margin-top:10px"></p>
<p id="fact-prob-measurable-function-arithmetic-rules"><span class="ititle2">Fact 1-1.</span> Basic arithmetic facts for measurable functions: <span><span class="fact-highlight"> <strong>1)</strong> if <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>-measurable
and <span class="math notranslate nohighlight">\(c\in\mathbb{R}\)</span>, then <span class="math notranslate nohighlight">\(f + c,\ cf\)</span> are <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>-measurable;
<strong>2)</strong> if <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>-measurable, then <span class="math notranslate nohighlight">\(f + g\)</span>,
<span class="math notranslate nohighlight">\(\text{fg}\)</span> are both <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>-measurable; <strong>3)</strong> if <span class="math notranslate nohighlight">\(g \neq 0\)</span>,
then <span class="math notranslate nohighlight">\(\frac{f}{g}\)</span> is <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>-measurable; <strong>4)</strong> if <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>-measurable, <span class="math notranslate nohighlight">\(g\)</span> is
<span class="math notranslate nohighlight">\(\mathcal{B}\left( \mathbb{R} \right)\)</span>-measurable, then the composition
<span class="math notranslate nohighlight">\(f \circ g\)</span> is <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>-measurable</span></span>.</p>
</li>
<p><li style="margin-top:10px"></p>
<p id="lemma-prob-measurable-function-basic-measurable-sets"><span class="ititle">Lemma 1-1.</span> If <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>-measurable, then <span><span class="result-highlight"> for any <span class="math notranslate nohighlight">\(a\mathbb{\in R}\)</span>,
<span class="math notranslate nohighlight">\(\left\{ x \in \Omega:f\left( x \right) \geq a \right\}\)</span>, denoted as
<span class="math notranslate nohighlight">\(\left\{ f \geq a \right\}\)</span>, can be found in <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> </span></span>, simply because <span class="math notranslate nohighlight">\(\left\{ f \geq a \right\}\)</span> is a closed set,
and thus <span class="math notranslate nohighlight">\(\left\{ f \geq a\right\}\in\mathcal{B}\left(\mathbb{R}\right)\)</span> since the open set
<span class="math notranslate nohighlight">\(\left\{ f &lt; a\right\}\in\mathcal{B}\left(\mathbb{R}\right)\)</span> by definition of Borel <span class="math notranslate nohighlight">\(σ\)</span>-algebra and that
<span class="math notranslate nohighlight">\(\left\{ f \geq a \right\} = \Omega\backslash\left\{ f &lt; a \right\}\)</span> is
the complment of the open set. <span class="emp">As a result</span>,
<span><span class="result-highlight"> <span class="math notranslate nohighlight">\(\left\{ f \in I \right\}\in\mathcal{F}\)</span> if <span class="math notranslate nohighlight">\(I\)</span> is any open, closed,
semi-open interval <span class="math notranslate nohighlight">\(I\)</span> or any closed and open half-line</span></span>.</p>
</li></ul>
<p><span style="padding-left:20px"></span> If a real-valued function <span class="math notranslate nohighlight">\(f\)</span> is a
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right) \rightarrow \left( \mathbb{R,}\mathcal{B}\left( \mathbb{R} \right) \right)\)</span>
measurable function, then the  <span><span class="exdef"> <span class="target" id="index-25"></span>integration</span></span> of <span class="math notranslate nohighlight">\(f\)</span> w.r.t. some measure <span class="math notranslate nohighlight">\(μ\)</span> over some
<span class="math notranslate nohighlight">\(A \in \mathcal{F}\)</span> is denoted by <span class="math notranslate nohighlight">\(\int_{A}^{}{f\operatorname{d}μ}\)</span>. The rigorous
mathematical construction of the integration is beyond our scope, but
basically can still be understood as the integration we see in calculus.
The integration w.r.t. the Lebesgue measure is called  <span><span class="exdef"> <span class="target" id="index-26"></span>Lebesgue integration</span></span>,
and <span><span class="comment-highlight"> the  <span><span class="exdef"> <span class="target" id="index-27"></span>Riemann integration</span></span> we learned in calculus is a special case of Lebesgue integration</span></span>
because <span><span class="fact-highlight"> every Riemann integrable function is Lebesgue integrable</span></span> (but the reverse is not true).
If <span class="math notranslate nohighlight">\(\int_{A}^{}{f\operatorname{d}μ}\mathbb{\in R}\)</span>, we say <span class="math notranslate nohighlight">\(f\)</span> is <strong>integrable</strong> over
<span class="math notranslate nohighlight">\(A\in\mathcal{F}\)</span> under <span class="math notranslate nohighlight">\(μ\)</span>, denoted by
<span class="math notranslate nohighlight">\(f\in\mathcal{L}\left( A,μ \right)\)</span> (or equivalently
<span class="math notranslate nohighlight">\(\left| f \right|\in\mathcal{L}\left( A,μ \right)\)</span> due to 1) of <a class="reference internal" href="__01_probability.html#fact-integration-arithemetics"><span class="std std-ref">Fact 1-2</span></a>,
where the notation <span class="math notranslate nohighlight">\(\mathcal{L}\left( A,μ \right)\)</span> can be
understood as the set of all integrable functions over <span class="math notranslate nohighlight">\(A\)</span>. <span class="math notranslate nohighlight">\(f\)</span> is said
to be  <span><span class="def"> <span class="target" id="index-28"></span>Lp-integrable</span></span> over a set <span class="math notranslate nohighlight">\(A\in\mathcal{F}\)</span> if
<span class="math notranslate nohighlight">\(\int_{A}^{}{\left| f \right|^{p}\operatorname{d}μ}\mathbb{\in R}\)</span>, denoted by
<span class="math notranslate nohighlight">\(f \in \mathcal{L}^{p}\left( A,μ \right)\)</span>, where
<span class="math notranslate nohighlight">\(\mathcal{L}^{p}\left( A,μ \right)\)</span> is the set of all
<span class="math notranslate nohighlight">\(\mathcal{L}^{p}\)</span>-integrable functions and has a famous name in
mathematics called  <span><span class="def"> <span class="target" id="index-29"></span>Lp space</span></span>. When <span class="math notranslate nohighlight">\(A = \Omega\)</span> and
the measure <span class="math notranslate nohighlight">\(μ\)</span> is Lebesgue measure, we can denote
<span class="math notranslate nohighlight">\(f\in\mathcal{L}:=f\in\mathcal{L}\left( \Omega,μ \right)\)</span> and
<span class="math notranslate nohighlight">\(f \in \mathcal{L}^{p} := f \in \mathcal{L}^{p}\left( \Omega,μ \right)\)</span>
for simplicity. In addition, we say <span class="math notranslate nohighlight">\(\int_{A}^{}{f\operatorname{d}μ}\)</span>  <span><span class="def"> <span class="target" id="index-30"></span>integration exists</span></span> if
<span class="math notranslate nohighlight">\(\int_{A}^{}{f\operatorname{d}μ} \in \left\lbrack - \infty, + \infty \right\rbrack\)</span>,
in contrast to the case when the integration is not defined;
<span class="math notranslate nohighlight">\(\int_{A}^{}{f\operatorname{d}μ}\)</span> is  <span><span class="def"> <span class="target" id="index-31"></span>integration not defined</span></span> if
<span class="math notranslate nohighlight">\(\int_{A}^{}{f^{+}\operatorname{d}μ} = + \infty,\int_{A}^{}{f^{-}\operatorname{d}μ} = + \infty\)</span>
where <span class="math notranslate nohighlight">\(f^{+} = \left\{ \begin{matrix} f\left( x \right) &amp; f\left( x \right) \geq 0 \\ 0 &amp; f\left( x \right) &lt; 0 \\ \end{matrix} \right.\ \)</span> and <span class="math notranslate nohighlight">\(f^{-} = \left\{ \begin{matrix}  - f\left( x \right) &amp; f\left( x \right) &lt; 0 \\ 0 &amp; f\left( x \right) \geq 0 \\ \end{matrix} \right.\ \)</span>.</p>
<ul style="margin-left:20px">
<p><li style="margin-top:10px"></p>
<p id="fact-integration-arithemetics"><span class="ititle2">Fact 1-2.</span> <span class="bemp">Arithmetic properties of integration.</span>
<strong>1)</strong><span class="math notranslate nohighlight">\(\colorbox{fact}{$f \in L\left( E,μ \right) \Leftrightarrow |f| \in L\left( E,μ \right)$}\)</span>;
<strong>2) monotonicity</strong>: <span><span class="fact-highlight"> if <span class="math notranslate nohighlight">\(f_{1} \leq f_{2}\)</span>, then <span class="math notranslate nohighlight">\(\int_{A}^{}{f_{1}\operatorname{d}μ} \leq \int_{A}^{}{f_{2}\operatorname{d}μ}\)</span> if
both integrals exist</span></span>; <strong>3) linearity</strong>: <span class="math notranslate nohighlight">\(\colorbox{fact}{$\int_{A}^{}{(af_{1} + bf_{2})\operatorname{d}μ} = a\int_{A}^{}{f_{1}\operatorname{d}μ} + b\int_{A}^{}{f_{2}\operatorname{d}μ}$ if $f_{1},f_{2}\in\mathcal{L}\left( A,μ \right)$}\)</span>; <strong>4) finite extension</strong>: <span><span class="fact-highlight"> if <span class="math notranslate nohighlight">\(A_{1}\)</span> and <span class="math notranslate nohighlight">\(A_{2}\)</span> are disjoint sets, then
<span class="math notranslate nohighlight">\(\int_{A_{1}\bigcup A_{2}}^{}f\operatorname{d}μ = \int_{A_{1}}^{}f\operatorname{d}μ + \int_{A_{2}}^{}f\operatorname{d}μ\)</span> </span></span>;
<strong>4)</strong> <span><span class="fact-highlight"> <span class="math notranslate nohighlight">\(\left| \int_{A}^{}f\operatorname{d}μ \right| \leq \int_{A}^{}{|f|\operatorname{d}μ}\)</span> if <span class="math notranslate nohighlight">\(\int_{A}^{}f\operatorname{d}μ\)</span> exists</span></span>.
The expectation of a random variable in probability theory is an integration, so it inherits all these properties.</p>
</li>
<p><li style="margin-top:10px"></p>
<p id="fact-integration-change-of-variable-image-measure"><span class="ititle2">Fact 1-3.</span> <span class="bemp">Change of variable for image measure.</span> If <span class="math notranslate nohighlight">\(g\)</span> is a
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right) \rightarrow \left( S,\mathcal{E} \right)\)</span>
measurable function, then it is easy to verify <span class="math notranslate nohighlight">\(μ \circ g^{- 1}\)</span> is a
measure defined on <span class="math notranslate nohighlight">\(\left( S,\mathcal{E} \right)\)</span>, which is named the  <span><span class="def"> <span class="target" id="index-32"></span>image measure</span></span> and can be denoted by <span class="math notranslate nohighlight">\(μ_{g}\)</span>. <span><span class="fact-highlight"> If
<span class="math notranslate nohighlight">\(h:S \rightarrow \mathbb{R}\)</span> is
<span class="math notranslate nohighlight">\(\mathcal{L}^{1}\left( B,μ \circ g^{- 1} \right)\)</span> for some
<span class="math notranslate nohighlight">\(B\in \mathcal{E}\)</span>, then
<span class="math notranslate nohighlight">\(\int_{B}^{}{h\operatorname{d}\left( μ \circ g^{- 1} \right)} = \int_{g^{- 1}\left( B \right)}^{}{h\operatorname{d}μ}\)</span> </span></span>.</p>
</li></ul>
<p><span style="padding-left:20px"></span> Given two measurable spaces <span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right)\)</span> and
<span class="math notranslate nohighlight">\(\left( S,\mathcal{E} \right)\)</span>, a  <span><span class="def"> <span class="target" id="index-33"></span>random variable</span></span>, often abbreviated
as “RV” in our text, is a
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right) \rightarrow (S\mathcal{,E)}\)</span>
measurable function. <span><span class="comment-highlight"> The choice of <span class="math notranslate nohighlight">\(\left( S,\mathcal{E} \right)\)</span> for our real-world problems
is usually <span class="math notranslate nohighlight">\(S = \mathbb{R}^{n}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{E = B}\left( \mathbb{R}^{n} \right)\)</span> being the  <span><span class="def"> <span class="target" id="index-34"></span>Borel σ-algebra</span></span> of <span class="math notranslate nohighlight">\(\mathbb{R}^{n}\)</span> </span></span>.
<span class="math notranslate nohighlight">\(X\)</span> need to be a measurable function, i.e. <span class="math notranslate nohighlight">\(X^{- 1}\left( B \right)\in\mathcal{F,}\forall B \in \mathcal{E}\)</span>;
<span class="math notranslate nohighlight">\(S\)</span> is named the  <span><span class="def"> <span class="target" id="index-35"></span>state space</span></span>, and a subset <span class="math notranslate nohighlight">\(\left\{ x \in S:X^{- 1}\left( S \right) \neq \varnothing \right\}\)</span> is
called the  <span><span class="def"> <span class="target" id="index-36"></span>support</span></span> of <span class="math notranslate nohighlight">\(X\)</span>. <span class="emp">The technical purpose</span> for <span class="math notranslate nohighlight">\(X\)</span> to be a measurable
function, as mentioned above, is to make sure every measurable subset in
<span class="math notranslate nohighlight">\(\mathcal{E}\)</span> of the state space <span class="math notranslate nohighlight">\(S\)</span> can find its preimage in
<span class="math notranslate nohighlight">\(\mathcal{F}\)</span> and hence can be properly “measured” by probability
measure <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>, so the classic concepts like distribution and
density of <span class="math notranslate nohighlight">\(X\)</span> can follow. <span class="emp">As a convention</span>, we use <span class="math notranslate nohighlight">\(\left\{ X = x \right\}\)</span> to denote the event of
<span class="math notranslate nohighlight">\(X^{- 1}\left( x \right)\)</span>, and <span class="math notranslate nohighlight">\(\left\{ X \in B \right\}\)</span> to denote the
event of <span class="math notranslate nohighlight">\(X^{- 1}\left( B \right)\)</span>, and likewise <span class="math notranslate nohighlight">\(\mathbb{P}\left( X = x \right)=\mathbb{P}\left( X^{- 1}\left( x \right) \right)\)</span>,
and <span class="math notranslate nohighlight">\(\mathbb{P}\left( X \in B \right)=\mathbb{P}\left( X^{- 1}\left( B \right) \right)\)</span>. <span class="emp">For example</span>, <span class="math notranslate nohighlight">\(\Omega = \left\{ 1,\ldots,5 \right\}\)</span>,
<span class="math notranslate nohighlight">\(S = \left\{ 0,1 \right\}\)</span>, <span class="math notranslate nohighlight">\(X\left( \omega \right) = \left\{ \begin{matrix} 0 &amp; \omega \leq 3 \\ 1 &amp; \omega &gt; 3 \\ \end{matrix} \right.\ \)</span>.
Now if <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> is “uniform” over <span class="math notranslate nohighlight">\(\Omega\)</span>,
i.e. <span class="math notranslate nohighlight">\(\mathbb{P}\left( \left\{ \omega \right\} \right) = \frac{1}{5},\forall\omega \in \Omega\)</span>,
then we have <span class="math notranslate nohighlight">\(\mathbb{P}\left( X = 0 \right) = \frac{3}{5}\)</span> and
<span class="math notranslate nohighlight">\(\mathbb{P}\left( X = 1 \right) = \frac{2}{5}\)</span>. Given a RV <span class="math notranslate nohighlight">\(X:\left( \Omega\mathcal{,F} \right) \rightarrow \left( S,\mathcal{E} \right)\)</span>,
we say <span class="math notranslate nohighlight">\(σ\left( X \right) := \left\{ X^{- 1}\left( B \right):B \in \mathcal{E} \right\}\)</span> is the  <span><span class="def"> <span class="target" id="index-37"></span>σ-algebra generated by a random variable</span></span>.
It is trivial to check <span class="math notranslate nohighlight">\(\colorbox{result}{$X$ is measurable w.r.t. $\left( \Omega,σ\left( X \right) \right)$}\)</span>.
During one experiment, <span class="math notranslate nohighlight">\(X\left( \omega \right)\)</span> accepts an input
<span class="math notranslate nohighlight">\(\omega\)</span>, and if <span class="math notranslate nohighlight">\(x = X\left( \omega \right)\)</span>, then <span class="math notranslate nohighlight">\(x\)</span> is called a  <span><span class="def"> <span class="target" id="index-38"></span>realization</span></span> of <span class="math notranslate nohighlight">\(X\)</span>; if you repeat the experiment, you may have
multiple realizations</p>
<ul style="margin-left:20px">
<p><li style="margin-top:10px"></p>
<p id="lemma-prob-random-variable-arithmetic-rules"><span class="ititle">Lemma 1-2.</span> Consider
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right) \rightarrow \left( \mathbb{R}\mathcal{,B}\left( \mathbb{R} \right) \right)\)</span>
RVs. Due to <a class="reference internal" href="__01_probability.html#fact-prob-measurable-function-arithmetic-rules"><span class="std std-ref">Fact 1-1</span></a>, if <strong>1)</strong> <span><span class="result-highlight"> <span class="math notranslate nohighlight">\(X\)</span> is a RV and <span class="math notranslate nohighlight">\(c \in \mathbb{R}\)</span>, then
<span class="math notranslate nohighlight">\(\text{cX}\)</span> is a RV</span></span>; <strong>2)</strong> <span><span class="result-highlight"> if both <span class="math notranslate nohighlight">\(X,Y\)</span> are RVs, then <span class="math notranslate nohighlight">\(X + Y,XY\)</span> are RVs</span></span>;
<strong>3)</strong> <span><span class="result-highlight"> if <span class="math notranslate nohighlight">\(X,Y\)</span> are RVs and <span class="math notranslate nohighlight">\(Y \neq 0\)</span>, then <span class="math notranslate nohighlight">\(\frac{X}{Y}\)</span> is a RV</span></span>; <strong>4)</strong> <span><span class="result-highlight"> if
<span class="math notranslate nohighlight">\(X\)</span> is a RV, and <span class="math notranslate nohighlight">\(f\)</span> is a
<span class="math notranslate nohighlight">\(\mathcal{B}\left( \mathbb{R} \right)\)</span>-measurable function, then
<span class="math notranslate nohighlight">\(f\left( X \right)\)</span> is a RV</span></span>.</p>
</li></ul>
<p><span style="padding-left:20px"></span> We should <span class="emp">note</span> for a general measure space
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F,}μ \right)\)</span> or a probability space
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F,}\mathbb{P} \right)\)</span>, <span><span class="result-highlight"> a non-empty set may have
non-zero measure, a non-empty event may have zero probability measure,
and of course <span class="math notranslate nohighlight">\(\mathbb{P}\left( A \right) = 1\)</span> does not mean
<span class="math notranslate nohighlight">\(A = \Omega\)</span> </span></span>. <span class="emp">For example</span>, if we throw a point <span class="math notranslate nohighlight">\(X\)</span>
uniformly into the interval <span class="math notranslate nohighlight">\(\left\lbrack 0,1 \right\rbrack\)</span> on the real
line, then the event that <span class="math notranslate nohighlight">\(X = x\)</span> for any
<span class="math notranslate nohighlight">\(x \in \left\lbrack 0,1 \right\rbrack\)</span> has zero probability; actually,
any continuous RV has zero probability for any event like
<span class="math notranslate nohighlight">\(\left\{ X = x \right\}\)</span>. In probabilistic discussion, we may frequently
encounter a statement involving an event or a random variable is true
“almost surely”, abbreviated as “a.s.”. If an event <span class="math notranslate nohighlight">\(A\)</span> happens  <span><span class="def"> <span class="target" id="index-39"></span>almost surely</span></span>, it means <span class="math notranslate nohighlight">\(\mathbb{P}\left( A \right) = 1\)</span>, but note it is
possible <span class="math notranslate nohighlight">\(A \neq \Omega\)</span>; if a statement is true  <span><span class="def"> <span class="target" id="index-40"></span>almost surely</span></span>, that
means the probability of the statement being true is 1, while there
could be violations. <span class="emp">For example</span>, if we say a RV <span class="math notranslate nohighlight">\(X\)</span>
satisfying some condition is unique a.s., we mean there could be another
<span class="math notranslate nohighlight">\(Y \neq X\)</span>, but <span class="math notranslate nohighlight">\(\mathbb{P}\left( \left\{ \omega:X\left( \omega \right) \neq Y\left( \omega \right) \right\} \right) = 0\)</span>.</p>
<p><span style="padding-left:20px"></span> Consider RV
<span class="math notranslate nohighlight">\(X:\left( \Omega\mathcal{,F} \right) \rightarrow \left( S,\mathcal{E} \right)\)</span>.
If <span class="math notranslate nohighlight">\(S\)</span> is finite or
countable, then <span class="math notranslate nohighlight">\(X\)</span> is a  <span><span class="def"> <span class="target" id="index-41"></span>discrete random variable</span></span> and the
<span class="math notranslate nohighlight">\(S \rightarrow \left\lbrack 0,1 \right\rbrack\)</span> function
<span class="math notranslate nohighlight">\(p\left( x \right):=\mathbb{P}\left( X = x \right),x \in S\)</span> is the  <span><span class="def"> <span class="target" id="index-42"></span>discrete distribution</span></span> of <span class="math notranslate nohighlight">\(X\)</span>, and each <span class="math notranslate nohighlight">\(p\left( x \right)\)</span> can be
called the  <span><span class="def"> <span class="target" id="index-43"></span>probability mass</span></span> at <span class="math notranslate nohighlight">\(x\)</span>. If <span class="math notranslate nohighlight">\(S\)</span> is uncountable, things
become non-trivial,</p>
<p><ol style="margin-left:20px"></p>
<li value="1" style="margin-top:10px"><p>Suppose <span class="math notranslate nohighlight">\(μ\)</span> and <span class="math notranslate nohighlight">\(𝜆\)</span> are two measures for a measurable space <span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right)\)</span>, it is said <span class="math notranslate nohighlight">\(μ\)</span>
is  <span><span class="def"> <span class="target" id="index-44"></span>absolutely consitnuous</span></span> w.r.t. <span class="math notranslate nohighlight">\(𝜆\)</span>, denoted by <span class="math notranslate nohighlight">\(μ \ll 𝜆\)</span>,
if <span class="math notranslate nohighlight">\(𝜆\left( A \right) = 0 \Rightarrow μ\left( A \right) = 0,\forall A \in \mathcal{F}\)</span>.
A measure <span class="math notranslate nohighlight">\(μ\)</span> is said to be an  <span><span class="def"> <span class="target" id="index-45"></span>absolutely continuous measure</span></span>
if it is absolutely continuous w.r.t. the Lebesgue measure (again, note Lebesgue measure reflect our common sense of length, area,
volume, etc.).</p>
</li>
<li value="2" style="margin-top:10px"><p id="fact-prob-randon-nikodym-theorem"><span class="ititle2">Fact 1-4.</span> The  <span><span class="exdef"> <span class="target" id="index-46"></span>Radon-Nikodym theorem</span></span> guarantees <span><span class="fact-highlight"> if <span class="math notranslate nohighlight">\(μ\)</span> and <span class="math notranslate nohighlight">\(𝜆\)</span> are two measures on <span class="math notranslate nohighlight">\((\Omega,\mathcal{F})\)</span> s.t. <span class="math notranslate nohighlight">\(μ \ll 𝜆\)</span>, then there exists an
<span class="math notranslate nohighlight">\(𝜆\)</span>-a.s. unique <span class="math notranslate nohighlight">\(f:\left( \Omega,\mathcal{F} \right) \rightarrow \left\lbrack 0, + \infty \right)\)</span> measurable function s.t.</span></span></p>
<div class="math notranslate nohighlight" id="equation-eq-randon-nikodym">
<span class="eqno">(1.1)<a class="headerlink" href="#equation-eq-randon-nikodym" title="Permalink to this equation">¶</a></span>\[\colorbox{fact}{$μ\left( A \right) = \int_{A}^{}{f \operatorname{d}𝜆},\forall A \in \mathcal{F}$}\]</div>
<p>and also say <span class="math notranslate nohighlight">\(f\)</span> is the  <span><span class="exdef"> <span class="target" id="index-47"></span>Radon-Nikodym derivative</span></span> of measure <span class="math notranslate nohighlight">\(μ\)</span>
w.r.t. measure <span class="math notranslate nohighlight">\(𝜆\)</span>, denoted by <span class="math notranslate nohighlight">\(f = \frac{\operatorname{d}μ}{\operatorname{d}𝜆}\)</span>.
Here, the “<span class="math notranslate nohighlight">\(𝜆\)</span>-a.s. unique” means if there is another <span class="math notranslate nohighlight">\(g\)</span>
satisfying <a class="reference internal" href="#equation-eq-randon-nikodym">Eq.1.1</a>, then <span class="math notranslate nohighlight">\(𝜆\left( \left\{ x \in \Omega:f\left( x \right) \neq g\left( x \right) \right\} \right) = 0\)</span>;
remember <span class="math notranslate nohighlight">\(μ \ll 𝜆\)</span>, then “<span class="math notranslate nohighlight">\(𝜆\)</span>-a.s.” implies <span class="math notranslate nohighlight">\(μ\)</span>-a.s. A change of variable rule is frequently used in probabilistic and
statistical text, which says <span><span class="fact-highlight"> if <span class="math notranslate nohighlight">\(f = \frac{\operatorname{d}μ}{\operatorname{d}𝜆}\)</span>, and
function <span class="math notranslate nohighlight">\(g:\Omega\mathbb{\rightarrow R}\)</span> is
<span class="math notranslate nohighlight">\(\mathcal{L}^{1}\left( A,μ \right)\)</span> for some <span class="math notranslate nohighlight">\(A \in \mathcal{F}\)</span>, then
<span class="math notranslate nohighlight">\(\int_{A}^{}{g\operatorname{d}μ} = \int_{A}^{}{gf\operatorname{d}𝜆}\)</span> </span></span>.</p>
<p></li> </ol></p>
<p>Now consider measurable space <span class="math notranslate nohighlight">\(\left( S,\mathcal{E} \right)\)</span>, by
Radon-Nikodym, we immediately have <span><span class="fact-highlight"> given any measure <span class="math notranslate nohighlight">\(μ\)</span> defined on the <span class="math notranslate nohighlight">\((S,\mathcal{E})\)</span>, there exists a unique corresponding density function
<span class="math notranslate nohighlight">\(f\left( x \right):S \rightarrow \left\lbrack 0, + \infty \right\rbrack\)</span>
consistent with the underlying probability measure <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> in a way that
<span class="math notranslate nohighlight">\(\int_{B} f\operatorname{d}μ = \mathbb{P}\left\{ X^{- 1}\left( B \right) \right\},\forall B \in \mathcal{E}\)</span> </span></span>.
We can easily verify that <span class="math notranslate nohighlight">\(\colorbox{result}{$\mathbb{P \circ}X^{- 1}$ is a probability measure on the state space $\left( S,\mathcal{E} \right)$}\)</span>, and we call this measure the  <span><span class="def"> <span class="target" id="index-48"></span>probability law</span></span> or  <span><span class="def"> <span class="target" id="index-49"></span>distribution</span></span> of RV <span class="math notranslate nohighlight">\(X\)</span>,
and <span class="math notranslate nohighlight">\(f\)</span> is the  <span><span class="def"> <span class="target" id="index-50"></span>density function</span></span> or  <span><span class="def"> <span class="target" id="index-51"></span>Randon-Nikodym derivative</span></span> of the
distribution, <span class="math notranslate nohighlight">\(f\left( x \right)\)</span> is the  <span><span class="def"> <span class="target" id="index-52"></span>density</span></span> of the distribution at <span class="math notranslate nohighlight">\(x\)</span>.
<span><span class="comment-highlight"> If we choose μ as Lebesgue measure
and <span class="math notranslate nohighlight">\(\left( S,\mathcal{E} \right) = \left( \mathbb{R}^{n}\mathcal{,B}\left( \mathbb{R}^{n} \right) \right)\)</span>,
then we have ordinary density functions we commonly use</span></span>.
In the future <span class="emp">we denote</span> “<span class="math notranslate nohighlight">\(\int_{B}f\operatorname{d}μ\)</span>” as
“<span class="math notranslate nohighlight">\(\int_{B}^{}f\)</span>” for simplicity if Lebesgue measure is assumed for <span class="math notranslate nohighlight">\(μ\)</span>.
Random-Nikodym applies to both discrete and
continuous RV, so probability mass is a special type of probability
density, but we should <span class="emp">note</span> probability density is not
probability and its value can be much higher than <span class="math notranslate nohighlight">\(1\)</span> (like in a
Gaussian distribution with a high peak). <span class="emp">Every time</span> we mention a random variable, we assume these setups of measurable spaces,
the probability measure and the probability density, although we might not explicitly mention that.</p>
<p><span style="padding-left:20px"></span> A  <span><span class="def"> <span class="target" id="index-53"></span>stochastic process</span></span> or a  <span><span class="def"> <span class="target" id="index-54"></span>random process</span></span> is a collection of RVs
<span class="math notranslate nohighlight">\(X\left( t \right),t \in T\)</span> where <span class="math notranslate nohighlight">\(T\)</span> is a finite, countable or
uncountable  <span><span class="def"> <span class="target" id="index-55"></span>index set</span></span>. <span class="emp">Note</span> the random variables in this general
definition do not have intrinsic order, even though the word meaning of “process”
can suggest “series”. The order of RVs can be added by concrete
definitions, for example, a  <span><span class="exdef"> <span class="target" id="index-56"></span>Markov chain</span></span> is a stochastic process
where <span class="math notranslate nohighlight">\(T = \left\{ 0,1,2,\ldots \right\}\mathbb{= N}\)</span> s.t. the
distribution of <span class="math notranslate nohighlight">\(X\left( t + 1 \right)\)</span> is only dependent on
<span class="math notranslate nohighlight">\(X\left( t \right)\)</span> for <span class="math notranslate nohighlight">\(t = 1,2\ldots\)</span> In such case when the “process”
is ordered, <span class="math notranslate nohighlight">\(T\)</span> can usually be interpreted as time. We can often see
processes like  <span><span class="exdef"> <span class="target" id="index-57"></span>Gaussian process</span></span>,  <span><span class="exdef"> <span class="target" id="index-58"></span>Poisson process</span></span>,  <span><span class="exdef"> <span class="target" id="index-59"></span>Dirichlet process</span></span>
etc. in probabilistic machine learning, and the scheme of
their definitions are <span class="emp">simply</span> stochastic processes
<span class="math notranslate nohighlight">\(X\left( t \right),t \in T\)</span> s.t. for any finite subset
<span class="math notranslate nohighlight">\(\left\{ t_{1},\ldots,t_{N} \right\}\)</span> of <span class="math notranslate nohighlight">\(T\)</span> of arbitrary
<span class="math notranslate nohighlight">\(N \in \mathbb{N}^{+}\)</span> satisfying certain condition (dependent on the
concrete process definition),
<span class="math notranslate nohighlight">\(\left( X\left( t_{1} \right),\ldots,X\left( t_{N} \right) \right)\)</span> in
some way obeys corresponding distributions (Gaussian distribution,
Poisson distribution, Dirichlet distribution etc.). <span class="emp">For example</span>, we may define <span class="math notranslate nohighlight">\(X\left( t \right)\)</span> obeys a
Gaussian process <span class="math notranslate nohighlight">\(\operatorname{GP}\left( μ,\Sigma \right)\)</span> if
<span class="math notranslate nohighlight">\(\forall N \geq 1,t_{1},\ldots,t_{N} \in T\)</span> we have</p>
<div class="math notranslate nohighlight">
\[\left( X\left( t_{1} \right),\ldots,X\left( t_{N} \right) \right)
\sim\operatorname{Gaussian}\left( μ\left( X\left( t_{1} \right),\ldots,X\left( t_{N} \right) \right),
\Sigma\left( X\left( t_{1} \right),\ldots,X\left( t_{N} \right) \right) \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(μ\left( X\left( t_{1} \right),\ldots,X\left( t_{N} \right) \right)\)</span>
is the  <span><span class="exdef"> <span class="target" id="index-60"></span>mean function</span></span> that yields a vector of length <span class="math notranslate nohighlight">\(N\)</span>, and
<span class="math notranslate nohighlight">\(\Sigma\)</span> is the  <span><span class="exdef"> <span class="target" id="index-61"></span>covariance function</span></span> that yields a <span class="math notranslate nohighlight">\(N \times N\)</span>
matrix. <span class="emp">For another example</span>, the Dirichlet process
assumes the RVs <span class="math notranslate nohighlight">\(X\left( t \right)\)</span> are probability-measure-valued (i.e.
the state space <span class="math notranslate nohighlight">\(S\)</span> of <span class="math notranslate nohighlight">\(X\)</span> are probability measures), and let the index
set <span class="math notranslate nohighlight">\(T\)</span> be a 𝜎-algebra of the sample space <span class="math notranslate nohighlight">\(\Omega\)</span>. We say <span class="math notranslate nohighlight">\(X\left( t \right)\sim\operatorname{DP}\left( α,H \right)\)</span>
for the  <span><span class="exdef"> <span class="target" id="index-62"></span>base probability measure</span></span> <span class="math notranslate nohighlight">\(H\)</span> and the  <span><span class="exdef"> <span class="target" id="index-63"></span>concentration parameter</span></span> <span class="math notranslate nohighlight">\(α\)</span>, if we always have</p>
<div class="math notranslate nohighlight">
\[\left( X\left( t_{1} \right),\ldots,X\left( t_{N} \right) \right)\sim
\operatorname{Dirichlet}\left( α H \left( t_{1} \right),\ldots,α H\left( t_{N} \right) \right)\]</div>
<p>for any finite partition of the sample space
<span class="math notranslate nohighlight">\(\left\{ t_{1},\ldots,t_{N} \right\} \subset T\)</span> (i.e.
<span class="math notranslate nohighlight">\(\bigcup_{i = 1}^{N}t_{i} = \Omega\)</span>, a σ-algebra must contain such a
finite partition of the sample space due to its axioms).</p>
  <div class="admonition note">
<p class="first admonition-title">Note: σ-Algebra Is The Lowest Mathematical Construction for Information</p>
<p><span style="padding-left:20px"></span> σ-algebra can be understood as the lowest-level mathematical
construction for information, on top of which concepts to quantify
information, such like <a href="#id5"><span class="problematic" id="id6">entropy_</span></a>, can be
defined. Given two σ-algebras <span class="math notranslate nohighlight">\(\mathcal{F}_{1},\mathcal{F}_{2}\)</span>
defined on a sample space <span class="math notranslate nohighlight">\(\Omega\)</span> and if
<span class="math notranslate nohighlight">\(\mathcal{F}_{1} \subset \mathcal{F}_{2}\)</span>, then we can say
<span class="math notranslate nohighlight">\(\mathcal{F}_{2}\)</span> contains more “information”, because a RV defined
w.r.t. <span class="math notranslate nohighlight">\(\mathcal{F}_{2}\)</span> (recall a RV must be measurable and thus it is
required <span class="math notranslate nohighlight">\(X^{- 1}\left( x \right)\in\mathcal{F,}\forall x \in X\left( \Omega \right)\)</span>)
can have a larger support, or the “realization” of <span class="math notranslate nohighlight">\(X\)</span> has a higher variety.</p>
<p><span style="padding-left:20px"></span> <span class="emp">For example</span>, recall the sample space of a coin flip is
<span class="math notranslate nohighlight">\(\Omega = \left\{ \text{head},\text{tail} \right\}\)</span> and the finest
σ-algebra for this is
<span class="math notranslate nohighlight">\(\mathcal{F}_{2}\mathcal{= P}\left( \Omega \right) = \left\{ \left\{ \text{head},\text{tail} \right\},\left\{ \text{head} \right\},\left\{ \text{tail} \right\},\varnothing \right\}\)</span>
which contains all possible events that can happen for a coin-flip
experiment, and by common sense we assign a probability measure</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\left( \left\{ \text{head},\text{tail} \right\} \right) = 1,\mathbb{P}\left( \left\{ \text{head} \right\} \right)=\mathbb{P}\left( \left\{ \text{tail} \right\} \right) = \frac{1}{2}\mathbb{,P}\left( \varnothing \right) = 0\]</div>
<p>Any real-valued random variable <span class="math notranslate nohighlight">\(X\)</span> defined on this probability space
can have <span class="math notranslate nohighlight">\(X^{- 1}\)</span> be any one of
<span class="math notranslate nohighlight">\(\left\{ \text{head},\text{tail} \right\},\left\{ \text{head} \right\},\left\{ \text{tail} \right\},\varnothing\)</span>,
with the most intuitive random variable being
<span class="math notranslate nohighlight">\(X\left( \omega \right) = \left\{ \begin{matrix}   0 &amp; \omega = \text{head} \\   1 &amp; \omega = \text{tail} \\   \end{matrix} \right.\ \)</span> with
<span class="math notranslate nohighlight">\(X^{- 1}\left( 0 \right) = \left\{ \text{head} \right\}\)</span>,
<span class="math notranslate nohighlight">\(X^{- 1}\left( 1 \right) = \left\{ \text{tail} \right\}\)</span> and
<span class="math notranslate nohighlight">\(X^{- 1}\left( \omega \right) = \varnothing,\forall\omega \notin \left\{ 0,1 \right\}\)</span>.
Such RV contains more information because its realization value tells you
what happed in the coin-flip experiment. Of course, we can define
another trivial RV w.r.t. <span class="math notranslate nohighlight">\(\mathcal{F}_{2}\)</span> s.t.
<span class="math notranslate nohighlight">\(X\left( \omega \right) \equiv 0\)</span> and then
<span class="math notranslate nohighlight">\(X^{- 1}\left( 0 \right) = \left\{ \text{head},tail \right\}\)</span> and
<span class="math notranslate nohighlight">\(X^{- 1}\left( \omega \right) = \varnothing,\forall\omega \neq 0\)</span>.
<span class="emp">However</span>, there is a valid σ-algebra that is
coarser than <span class="math notranslate nohighlight">\(\mathcal{F}_{2}\)</span>, i.e.
<span class="math notranslate nohighlight">\(\mathcal{F}_{1} = \left\{ \left\{ \text{head},\text{tail} \right\},\varnothing \right\} \subseteq \mathcal{F}_{2}\)</span>,
which is the trivial σ-algebra. On this σ-algebra, any RV
can only be a constant function, like <span class="math notranslate nohighlight">\(X\left( \omega \right) \equiv 0\)</span>.
Such RV is meaningless, and its realization value tells you nothing about
what happened in the coin-flip experiment. The inability of the RV is
due to loss of information by the coarser σ-algebra.</p>
<p><span style="padding-left:20px"></span> <span class="emp">For a likewise example</span>, one can think of throwing a
dice. The sample space is <span class="math notranslate nohighlight">\(\Omega = \left\{ 1,2,3,4,5,6 \right\}\)</span>. Consider the following σ-algebras,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{F}_{3} &amp;= \mathcal{P}\left( \Omega \right) \\
\mathcal{F}_{2} &amp;= σ\left( \left\{ \left\{ 1,3 \right\},\left\{ 5 \right\},\left\{ 2 \right\},\left\{ 4,6 \right\} \right\} \right) \\
\mathcal{F}_{1}^{\left( 1 \right)} &amp;= σ\left( \left\{ \left\{ 1,3,5 \right\},\left\{ 2,4,6 \right\} \right\} \right) \\
\mathcal{F}_{1}^{\left( 2 \right)} &amp;= σ\left( \left\{ \left\{ 1,2,3 \right\},\left\{ 4,5,6 \right\} \right\} \right) \\
\end{aligned}\end{split}\]</div>
<p class="last">where we can verify
<span class="math notranslate nohighlight">\(\mathcal{F}_{1}^{\left( 1 \right)} \subset \mathcal{F}_{2} \subset \mathcal{F}_{3}\)</span>
and
<span class="math notranslate nohighlight">\(\mathcal{F}_{1}^{\left( 2 \right)} \subset \mathcal{F}_{2} \subset \mathcal{F}_{3}\)</span>.
On <span class="math notranslate nohighlight">\(\mathcal{F}_{3}\)</span>, an RV can be defined as
<span class="math notranslate nohighlight">\(X\left( \omega \right) = \omega,\omega \in \Omega\)</span>, which has 6
possible realization values, and tells you exactly what happened in the
dicing. On <span class="math notranslate nohighlight">\(\mathcal{F}_{2}\)</span>, the best RV we can define is like
<span class="math notranslate nohighlight">\(X\left( \omega \right) = \left\{ \begin{matrix}   0 &amp; \omega \in \left\{ 1,3 \right\} \\   1 &amp; \omega = 5 \\   2 &amp; \omega = 2 \\   3 &amp; \omega \in \left\{ 4,6 \right\} \\   \end{matrix} \right.\ \)</span>, whose realization tells you if the dicing result
is even or odd as well as small or big. On
<span class="math notranslate nohighlight">\(\mathcal{F}_{1}^{\left( 1 \right)}\)</span>, the best RV one can define is
<span class="math notranslate nohighlight">\(X\left( \omega \right) = \left\{ \begin{matrix}   0 &amp; \omega \in \left\{ 1,3,5 \right\} \\   1 &amp; \omega \in \left\{ 2,4,6 \right\} \\   \end{matrix} \right.\ \)</span>, whose realization only tells you if the dicing
result is even or odd; similarly, on
<span class="math notranslate nohighlight">\(\mathcal{F}_{1}^{\left( 2 \right)}\)</span>, the best RV one can tell whether
the result is small or big; but <span class="math notranslate nohighlight">\(\mathcal{F}_{1}^{\left( 1 \right)}\)</span> and
<span class="math notranslate nohighlight">\(\mathcal{F}_{1}^{\left( 2 \right)}\)</span> is not comparable.
Again, we see the pattern that coarser σ-algebra loses information in terms of the RV with best realization variety we can define.</p>
</div>
</div>
<div class="section" id="marginalization-conditional-expectation-conditional-probablity-and-bayes-rules">
<h3>1.1.2. Marginalization, Conditional Expectation, Conditional Probablity and Bayes’ Rules<a class="headerlink" href="#marginalization-conditional-expectation-conditional-probablity-and-bayes-rules" title="Permalink to this headline">¶</a></h3>
<p><span style="padding-left:20px"></span> A random variable can be a vector, e.g.
<span class="math notranslate nohighlight">\(\left( X_{1},X_{2},X_{3} \right)\)</span>, called a  <span><span class="def"> <span class="target" id="index-64"></span>random vector</span></span>. Since by
definition a random variable can be either a scalar or a vector, <span class="emp">very often</span>
the concept of random variable and random vector are
used exchangeably and both are abbreviated as RV. <span class="emp">Intuitively</span>, a marginal random variable removes some
dimensions of the random vector, e.g. <span class="math notranslate nohighlight">\(X_{1}\)</span>, or
<span class="math notranslate nohighlight">\(\left( X_{1},X_{2} \right)\)</span> or <span class="math notranslate nohighlight">\(\left( X_{2},X_{3} \right)\)</span>, etc. The
removal of some dimensions of a random vector is called  <span><span class="def"> <span class="target" id="index-65"></span>marginalization</span></span>. <span class="emp">Formally</span>, suppose the original RV is
<span class="math notranslate nohighlight">\(\left( X,Y \right):\left( \Omega,\mathcal{F} \right) \rightarrow \left( S_{1} \times S_{2},\mathcal{E} \right)\)</span>,
then define <span class="math notranslate nohighlight">\(\mathcal{E}_{1},\mathcal{E}\)</span> be σ-algebras s.t. if
<span class="math notranslate nohighlight">\(B \in \mathcal{E}\)</span>, then
<span class="math notranslate nohighlight">\(B_{1} = \left\{ x:\left( x,y \right) \in B \right\} \in \mathcal{E}_{1}\)</span>,
and
<span class="math notranslate nohighlight">\(B_{2} = \left\{ y:\left( x,y \right) \in B \right\} \in \mathcal{E}_{2}\)</span>;
in plain words, <span class="math notranslate nohighlight">\(\mathcal{E}_{1},\mathcal{E}_{2}\)</span> are derived by only
keeping the first/second dimension of elements in all sets in
<span class="math notranslate nohighlight">\(\mathcal{E}\)</span>. Then
<span class="math notranslate nohighlight">\(X:\left( \Omega,\mathcal{F} \right) \rightarrow \left( S_{1},\mathcal{E}_{1} \right)\)</span>
is a  <span><span class="def"> <span class="target" id="index-66"></span>marginal random variable</span></span> of <span class="math notranslate nohighlight">\(\left( X,Y \right)\)</span>
if:math:<cite>text{X}left( omega right) = x</cite> iff
<span class="math notranslate nohighlight">\(\left( X,Y \right)\left( \omega \right) = \left( x,y \right)\)</span>, and
<span class="math notranslate nohighlight">\(Y:\left( \Omega,\mathcal{F} \right) \rightarrow \left( S_{2},\mathcal{E}_{2} \right)\)</span>
is the other  <span><span class="def"> <span class="target" id="index-67"></span>marginal random variable</span></span> if
<span class="math notranslate nohighlight">\(Y\left( \omega \right) = y\)</span> iff
<span class="math notranslate nohighlight">\(\left( X,Y \right)\left( \omega \right) = \left( x,y \right)\)</span>, and
recursively the marginal RVs of <span class="math notranslate nohighlight">\(X,Y\)</span> are also marginal RVs of
<span class="math notranslate nohighlight">\(\left( X,Y \right)\)</span>. Let <span class="math notranslate nohighlight">\(μ_{1},μ_{2}\)</span> be chosen measures for
<span class="math notranslate nohighlight">\(S_{1},S_{2}\)</span>, then another measure <span class="math notranslate nohighlight">\(μ\)</span> is said to be the  <span><span class="def"> <span class="target" id="index-68"></span>product measure</span></span> of <span class="math notranslate nohighlight">\(μ_{1},μ_{2}\)</span> if
<span class="math notranslate nohighlight">\(μ\left( B_{1} \times B_{2} \right) = μ_{1}\left( B_{1} \right)μ_{2}\left( B_{2} \right),\forall B_{1} \in \mathcal{E}_{1},B_{2} \in \mathcal{E}_{2}\)</span>. <span class="emp">For example</span>,
<span><span class="fact-highlight"> Lebesgue measure on <span class="math notranslate nohighlight">\(\mathbb{R}^{2}\)</span> is a
product measure of Lebesgue measures on <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> </span></span>.
<span><span class="theorem-highlight"> Denote <span class="math notranslate nohighlight">\(μ_{1} \times μ_{2} := μ\)</span>. Let <span class="math notranslate nohighlight">\(μ_{1},μ_{2}\)</span> be chosen measures
for <span class="math notranslate nohighlight">\(S_{1},S_{2}\)</span> respectively choose product measure
<span class="math notranslate nohighlight">\(μ_{2} \times μ_{2}\)</span> for <span class="math notranslate nohighlight">\(S_{1} \times S_{2}\)</span>, then the density
functions <span class="math notranslate nohighlight">\(f\left( x,y\right),f_{X}\left( x \right),f_{Y}\left( y \right)\)</span>
of <span class="math notranslate nohighlight">\(\left(X,Y\right),X,Y\)</span> satisfy <span class="math notranslate nohighlight">\(f_{X}\left( x \right) = \int_{S_{1} \times S_{2}}^{}{f\left( x,y \right)}dy\)</span>
and <span class="math notranslate nohighlight">\(f_{Y}\left( y \right) = \int_{S_{1} \times S_{2}}^{}{f\left( x,y \right)}\operatorname{d}x\)</span> </span></span>,
and <span class="math notranslate nohighlight">\(f_{X}\left( x \right)\)</span> and <span class="math notranslate nohighlight">\(f_{Y}\left( y \right)\)</span> are both called  <span><span class="def"> <span class="target" id="index-69"></span>marginal density functions</span></span>. This can be proved as the following,
for any <span class="math notranslate nohighlight">\(B \in \mathcal{E}_{1}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\mathbb{P}\left( X \in B \right) &amp;=\mathbb{P}\left( X \in B,Y \in S_{2} \right)=\mathbb{P}\left( \left( X,Y \right) \in B \times S_{2} \right) = \int_{B \times S_{2}}^{}{f\left( x,y \right)}d\left( μ_{1} \times μ_{2} \right) \\
&amp;{\color{conn1}{=}} \int_{B}^{}{\left( \int_{S_{2}}^{}{f\left( x,y \right)\operatorname{d}μ_{2})} \right)\operatorname{d}μ_{1}} = \int_{B}^{}{f_{X}(x)\operatorname{d}μ_{1})}
\end{aligned}\end{split}\]</div>
<p>where the <span class="conn1">key step</span> uses  <span><span class="exdef"> <span class="target" id="index-70"></span>Tonelli’s theorem</span></span>, which states <span><span class="fact-highlight"> the
integration of a non-negative function w.r.t. a product measure can be
broken into iterated integrations in any order</span></span>. The proof is the same for the other identity.</p>
<p><span style="padding-left:20px"></span> Given a real-valued RV
<span class="math notranslate nohighlight">\(X:\left( \Omega\mathcal{,F} \right) \rightarrow \left( \mathbb{R}\mathcal{,B}\left( \mathbb{R} \right) \right)\)</span>,
The  <span><span class="def"> <span class="target" id="index-71"></span>expectation</span></span> of a random variable <span class="math notranslate nohighlight">\(X\)</span> is defined as
<span class="math notranslate nohighlight">\(\mathbb{E}X = \int_{\Omega}^{}{X\operatorname{d}\mathbb{P}}\)</span>, the integration of
function <span class="math notranslate nohighlight">\(X\)</span> over entire sample space under measure <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>. The  <span><span class="def"> <span class="target" id="index-72"></span>conditional expectation by an event</span></span> is defined as
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|A \right\rbrack = \frac{\int_{A}^{}{X\operatorname{d}\mathbb{P}}}{\mathbb{P}\left( A \right)}\)</span>.
The arithmetic rules for expectation,
like linearity, <span class="emp">inherits</span> from those arithmetic
properties in <a class="reference internal" href="__01_probability.html#fact-integration-arithemetics"><span class="std std-ref">Fact 1-2</span></a>. Surprisingly, <span><span class="comment-highlight"> the construction of expectation
conditioned on a RV and the conditional density turn out the trickiest
essential concepts in probability theory</span></span>. A <span class="emp">naive construction</span> one can think of is a
<span class="math notranslate nohighlight">\(\left( \Omega^{2},σ\left( \mathcal{F}^{2} \right) \right) \rightarrow \left( \mathbb{R}\mathcal{,B}\left( \mathbb{R} \right) \right)\)</span>
RV defined on a product measure space; but this is <span class="emp">not adopted</span> by the theory, because <strong>1)</strong> it is hard to tell
what probability measure should be defined on the product space in order
for the theory to be consistent; <strong>2)</strong> the theory prefers to construct
every essential concept within the same probability space
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F,}\mathbb{P} \right)\)</span> which can help avoid
complicacies when the theory is further developed.
<span class="emp">Therefore</span>, the conditional expectation is no doubt a RV,
but we would like to define it in the original space, i.e. a
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right) \rightarrow \left( \mathbb{R}\mathcal{,B}\left( \mathbb{R} \right) \right)\)</span>
RV. Although such definition seems odd at the first glance, it turns out
consistent with the classic theory and other measure-theoretic
definitions.</p>
<p><span style="padding-left:20px"></span> We start with considering the easier case of a partition
<span class="math notranslate nohighlight">\(\mathcal{P \subseteq F}\)</span>. We feel it might be reasonable to design the
expectation of a RV <span class="math notranslate nohighlight">\(X\)</span> conditioned on <span class="math notranslate nohighlight">\(\mathcal{P}\)</span> as another RV <span class="math notranslate nohighlight">\(Y\)</span>
as a piecewise function s.t.
<span class="math notranslate nohighlight">\(Y\left( \omega \right)=\mathbb{E}\left\lbrack X|A \right\rbrack\)</span> if
<span class="math notranslate nohighlight">\(\omega \in A\)</span> for any <span class="math notranslate nohighlight">\(A \in \mathcal{P}\)</span>. <span class="emp">Formally</span>,
suppose <span class="math notranslate nohighlight">\(\mathbb{E}X\)</span> is finite, then the  <span><span class="def"> <span class="target" id="index-73"></span>conditional expectation given partition</span></span> <span class="math notranslate nohighlight">\(\mathcal{P =}\left\{ A_{1},A_{2},\ldots \right\}\)</span> is
defined as a RV <span class="math notranslate nohighlight">\(Y:\Omega\mathbb{\rightarrow R}\)</span> s.t. <strong>1)</strong> measurable
by <span class="math notranslate nohighlight">\(σ\left( \mathcal{P} \right)\)</span>; <strong>2)</strong> <span class="math notranslate nohighlight">\(Y \in \mathcal{L}^{1}\)</span>;
<strong>3)</strong> <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack Y|A \right\rbrack=\mathbb{E}\left\lbrack X|A \right\rbrack,\forall A \in \mathcal{P}\)</span>.
Since any <span class="math notranslate nohighlight">\(A \in σ\left( \mathcal{P} \right)\)</span> is a finite or
countable union of disjoint sets in <span class="math notranslate nohighlight">\(\mathcal{P}\)</span>, then it is easy to
see:math:<cite>colorbox{result}{$mathbb{E}leftlbrack Y|A rightrbrack=mathbb{E}leftlbrack X|A rightrbrack,forall A in σleft( mathcal{P} right)$}</cite>.
<span><span class="result-highlight"> Such RV <span class="math notranslate nohighlight">\(Y\)</span> exists since we can define</span></span></p>
<div class="math notranslate nohighlight" id="equation-eq-conditional-expt-given-partition">
<span class="eqno">(1.2)<a class="headerlink" href="#equation-eq-conditional-expt-given-partition" title="Permalink to this equation">¶</a></span>\[\colorbox{rlt}{$Y\left( \omega \right)=\mathbb{E}\left\lbrack X|A_{i} \right\rbrack,\forall\omega \in A_{i},i = 1,2,\ldots$}\]</div>
<p>which is a piecewise constant function, and it satisfies all above
conditions; we denoted such <span class="math notranslate nohighlight">\(Y\)</span> by
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|\mathcal{P} \right\rbrack\)</span>. It can be shown
such <span class="math notranslate nohighlight">\(Y\)</span> is unique almost surely as in <em>Theorem 1?€?1</em>, then any other conditional expectation by partition <span class="math notranslate nohighlight">\(\mathcal P\)</span>
different from <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|\mathcal{P} \right\rbrack\)</span> only
differs on a set of zero measure. This intermediate definition could
make us feel more comfortable with the following general definition.</p>
<table class="colwidths - given docutils" style="background:none; border:none;" width="90%" align="center" ><colgroup><col width = "100%" /></colgroup><tbody><tr class="row-odd" style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;"><a class="reference internal image-reference" href="conditional_expectation.png"><img alt="conditional_expectation.png" src="conditional_expectation.png" /></a>
</td></tr><tr><td  style="background:none; border:none;"><p id="figure-conditional-expt"><span style="text-align:justify"> <span><span class="ibold"> Figure 1-1</span></span> Illustration of conditional expectation.</span></span> Consider <span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right) = \left( \mathbb{R}\mathcal{,B}\left( \mathbb{R} \right) \right)\)</span>,
then the probability measure on <span class="math notranslate nohighlight">\(Ω\)</span> can be represented by a density function <span class="math notranslate nohighlight">\(𝑓\)</span>. Suppose <span class="math notranslate nohighlight">\(X\left( \omega \right) = \omega^{2},Z\left( \omega \right) = \left\{ \begin{matrix}   3 &amp; \omega \in \left( - \infty, - 1 \right\rbrack \\   1 &amp; \omega \in \left( - 1,2 \right\rbrack \\   2 &amp; \omega \in \left( 2,3 \right\rbrack \\   4 &amp; \omega \in \left( 3, + \infty \right) \\   \end{matrix} \right.\ \)</span>,
then <span class="math notranslate nohighlight">\(σ\left( Z \right)\)</span> is generated by partition
<span class="math notranslate nohighlight">\(\mathcal{P}=\left\{ \left( - \infty, - 1 \right\rbrack,\left( - 1,2 \right\rbrack,\left( 2,3 \right\rbrack,\left( 3, + \infty \right) \right\}\)</span>,
and <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|Z \right\rbrack\)</span> in this example is
transforming <span class="math notranslate nohighlight">\(X\)</span> to a piecewise function <span class="math notranslate nohighlight">\(\mathbb{E}[X|Z](\omega)=\int_{A}X\operatorname{\mathbb{P}}=\int_{A}Xf\)</span> if <span class="math notranslate nohighlight">\(\omega \in A \in \mathcal{P}\)</span>
, i.e. within each <span class="math notranslate nohighlight">\(𝐴∈𝒫\)</span> RV <span class="math notranslate nohighlight">\(𝑋\)</span> becomes a constant that is the average value of <span class="math notranslate nohighlight">\(𝑋\)</span> over <span class="math notranslate nohighlight">\(𝐴\)</span>. The effect of <span class="math notranslate nohighlight">\(𝑍\)</span> in this example
is to determine those regions where <span class="math notranslate nohighlight">\(𝑋\)</span> is to be piecewisely averaged. The concrete form of <span class="math notranslate nohighlight">\(𝑍\)</span> does not matter as long as <span class="math notranslate nohighlight">\(𝜎(𝑍)=𝜎(\mathcal{P})\)</span>.</p>
</td></tr></tbody></table><p><span style="padding-left:20px"></span> <span class="emp">Then, more generally</span>, given a coarser <span class="math notranslate nohighlight">\(σ\)</span>-algebra
<span class="math notranslate nohighlight">\(\mathcal{G \subseteq F}\)</span>, and suppose <span class="math notranslate nohighlight">\(\mathbb{E}X\)</span> is finite, the  <span><span class="def"> <span class="target" id="index-74"></span>conditional expectation by</span></span> <span class="math notranslate nohighlight">\(σ`**-algebra** :math:\)</span>mathcal{G}` is
defined as a RV <span class="math notranslate nohighlight">\(Y:\Omega\mathbb{\rightarrow R}\)</span> s.t. <span><span class="fact-highlight"> <strong>1)</strong> measurable
by <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>; <strong>2)</strong> <span class="math notranslate nohighlight">\(\mathbb{E}Y\)</span> is finite; <strong>3)</strong>
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack Y|A \right\rbrack = \mathbb{E}\left\lbrack X|A \right\rbrack,\forall A \in \mathcal{G}\)</span> </span></span>.
We take it for granted <span><span class="fact-highlight"> that such <span class="math notranslate nohighlight">\(Y\)</span> can be shown to exist</span></span> and its a.s. uniqueness is shown below in
<a class="reference internal" href="__01_probability.html#theorem-conditional-expt-uniqueness"><span class="std std-ref">Theorem 1-1</span></a>, and we denote such <span class="math notranslate nohighlight">\(Y\)</span> as
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|\mathcal{G} \right\rbrack\)</span>. Now we can also
define  <span><span class="def"> <span class="target" id="index-75"></span>conditional expectation by a random variable</span></span>
<span class="math notranslate nohighlight">\(Z:\left( \Omega\mathcal{,F} \right) \rightarrow \left( \mathbb{R}\mathcal{,B}\left( \mathbb{R} \right) \right)\)</span>
s.t. <span class="math notranslate nohighlight">\(σ\left( Z \right)\mathcal{\subset F}\)</span> as
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|Z \right\rbrack:=\mathbb{E}\left\lbrack X|σ\left( Z \right) \right\rbrack\)</span>.
The conditional expectation given a partition <span class="math notranslate nohighlight">\(\mathcal{P}\)</span> is
consistent with this definition in that
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|\mathcal{P} \right\rbrack=\mathbb{E}\left\lbrack X|σ\left( \mathcal{P} \right) \right\rbrack\)</span>.</p>
<ul style="margin-left:20px">
<p><li style="margin-top:10px"></p>
<p id="theorem-conditional-expt-uniqueness"><span class="ititle">Theorem 1-1.</span> <span class="bemp">Uniqueness of conditional expectation.</span> <span><span class="theorem-highlight"> The conditional expectation
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|\mathcal{G} \right\rbrack\)</span> as defined above is
a.s. unique</span></span>. We present this because it is not a hard proof. Suppose <span class="math notranslate nohighlight">\(Y = \mathbb{E}\left\lbrack X|\mathcal{G} \right\rbrack\)</span> for
some <span class="math notranslate nohighlight">\(\mathcal{G \subseteq F}\)</span>. Suppose there is another RV <span class="math notranslate nohighlight">\(Y^{'}\)</span> also satisfying all axioms for
conditional expectation <span class="math notranslate nohighlight">\(Y\)</span>. Let <span class="math notranslate nohighlight">\(A_{\epsilon} = \left\{ Y - Y^{'} \geq \epsilon \right\}\)</span>, then
<span class="math notranslate nohighlight">\(Y - Y^{'}\)</span> is a RV by <a class="reference internal" href="__01_probability.html#lemma-prob-random-variable-arithmetic-rules"><span class="std std-ref">Lemma 1-2</span></a> and thus
<span class="math notranslate nohighlight">\(A_{\epsilon}\in \mathcal{ F}\)</span> for any <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span> by <a class="reference internal" href="__01_probability.html#lemma-prob-measurable-function-basic-measurable-sets"><span class="std std-ref">Lemma 1-1</span></a>. By
the definition of conditional expectation, we have</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left\lbrack Y|A_{\epsilon} \right\rbrack\mathbb{- E}\left\lbrack Y^{'}|A_{\epsilon} \right\rbrack=\mathbb{E}\left\lbrack X|A_{\epsilon} \right\rbrack\mathbb{- E}\left\lbrack X|A_{\epsilon} \right\rbrack = 0\]</div>
<p>Meanwhile,</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left\lbrack Y|A_{\epsilon} \right\rbrack\mathbb{- E}\left\lbrack Y^{'}|A_{\epsilon} \right\rbrack = \int_{A_{\epsilon}}^{}{Y\operatorname{d}\mathbb{P}} - \int_{A_{\epsilon}}^{}{Y^{'}\operatorname{d}\mathbb{P}} = \int_{A_{\epsilon}}^{}{\left( Y - Y^{'} \right)\operatorname{d}\mathbb{P}} \geq \int_{A_{\epsilon}}^{}{\epsilon \operatorname{d}\mathbb{P}} = \epsilon\mathbb{P}\left( A_{\epsilon} \right)\]</div>
<div class="math notranslate nohighlight">
\[0 \leq \mathbb{P}\left( Y - Y^{'} &gt; 0 \right)=\mathbb{P}\left( \bigcup_{n = 1}^{\infty}A_{\frac{1}{n}} \right) \leq \sum_{n = 1}^{\infty}{\mathbb{P}\left( A_{\frac{1}{n}} \right)} = \sum_{n = 1}^{\infty}0 = 0\mathbb{\Rightarrow P}\left( Y - Y^{'} \leq 0 \right) = 1\]</div>
<p>That is, <span class="math notranslate nohighlight">\(Y \leq Y^{'}\)</span> a.s., and switch the role of <span class="math notranslate nohighlight">\(Y,Y^{'}\)</span> in above
proof we have <span class="math notranslate nohighlight">\(Y \geq Y^{'}\)</span> a.s., then <span class="math notranslate nohighlight">\(Y = Y^{'}\)</span> a.s., which
completes the proof.</p>
</li>
<p><li style="margin-top:10px"></p>
<p id="property-expecation-identity-is-probability"><span class="ititle">Property 1-1.</span> Given a subset <span class="math notranslate nohighlight">\(B\)</span> of some underlyingly assumed whole set, we define
an  <span><span class="def"> <span class="target" id="index-76"></span>identity function</span></span> as
<span class="math notranslate nohighlight">\(\mathbb{I}_{B}\left( x \right) = \left\{ \begin{matrix} 1 &amp; x \in B \\ 0 &amp; x \notin B \\ \end{matrix} \right.\ \)</span>, and we call
<span class="math notranslate nohighlight">\(\mathbb{I}_{B}\left( X \right)\)</span> as the identity random variable. We
have
<span class="math notranslate nohighlight">\(\colorbox{rlt}{$\mathbb{E}\left\lbrack \mathbb{I}_{B}\left( X \right) \right\rbrack=\mathbb{P}\left( X \in B \right)$}\)</span>.
This easily follows from</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left\lbrack \mathbb{I}_{B}\left( X \right) \right\rbrack = \int_{\Omega}^{}{\mathbb{I}_{X^{- 1}\left( B \right)}\left( \omega \right)\operatorname{d}\mathbb{P}} = \int_{X^{- 1}\left( B \right)}^{}{\operatorname{d}\mathbb{P}}=\mathbb{P}\left( \omega \in X^{- 1}\left( B \right) \right)=\mathbb{P}\left( X \in B \right)\]</div>
</li>
<p><li style="margin-top:10px"></p>
<p id="property-conditional-expt-absorbing-condition"><span class="ititle">Property 1-2.</span> <span><span class="result-highlight"> If a RV <span class="math notranslate nohighlight">\(X \in \mathcal{L}^{1}\)</span> is <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>-measurable, then
a.s. <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|\mathcal{G} \right\rbrack = X\)</span> </span></span>. By
definition a RV <span class="math notranslate nohighlight">\(Y\)</span> is
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|\mathcal{G} \right\rbrack\)</span> iff
<span class="math notranslate nohighlight">\(Y \in \mathcal{L}^{1}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>-measurable and
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack Y|A \right\rbrack=\mathbb{E}\left\lbrack X|A \right\rbrack,\forall A \in \mathcal{G}\)</span>.
Now <span class="math notranslate nohighlight">\(X \in \mathcal{L}^{1}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>-measurable, and
trivially
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|A \right\rbrack=\mathbb{E}\left\lbrack X|A \right\rbrack,\forall A \in \mathcal{G}\)</span>,
therefore <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|\mathcal{G} \right\rbrack = X\)</span>
a.s. Note here “a.s.” is due to the a.s. uniqueness of conditional
expectation.</p>
<p><span class="emp">Again</span>, if thinking about a general <span class="math notranslate nohighlight">\(σ\)</span>-algebra
is counter-intuitive, we can always instead think of a partition and
its generated <span class="math notranslate nohighlight">\(σ\)</span>-algebra. Given a partition
<span class="math notranslate nohighlight">\(\mathcal{P =}\left\{ A_{i} \right\}\)</span>, a RV <span class="math notranslate nohighlight">\(X\)</span> being
<span class="math notranslate nohighlight">\(σ\left( \mathcal{P} \right)\)</span>-measurable means <span class="math notranslate nohighlight">\(X\)</span> is
piece-wise constant over each disjoint set <span class="math notranslate nohighlight">\(A_{i}\)</span> in <span class="math notranslate nohighlight">\(\mathcal{P}\)</span>,
say <span class="math notranslate nohighlight">\(X\left( \omega \right) = x_{i},\forall\omega \in A_{i}\)</span>. Then
a.s.
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|σ\left( \mathcal{P} \right) \right\rbrack\left( \omega \right) = \frac{\int_{A_{i}}^{}{x_{i}\operatorname{d}\mathbb{P}}}{\mathbb{P}\left( A_{i} \right)} = x_{i},,\forall\omega \in A_{i}\)</span>,
i.e.
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|σ\left( \mathcal{P} \right) \right\rbrack\)</span>
is a.s. the same function as <span class="math notranslate nohighlight">\(X\)</span>.</p>
</li>
<p><li style="margin-top:10px"></p>
<p id="theorem-conditional-expt-tower-property"><span class="ititle">Theorem 1-2.</span> <span class="bemp">Law of Total Expectation (Tower Property).</span> <span><span class="theorem-highlight"> Given two <span class="math notranslate nohighlight">\(σ\)</span>-algebras
<span class="math notranslate nohighlight">\(\mathcal{G}_{1} \subseteq \mathcal{G}_{2}\mathcal{\subset F}\)</span>, if
<span class="math notranslate nohighlight">\(X \in \mathcal{L}^{1}\)</span> then a.s.</span></span></p>
<div class="math notranslate nohighlight">
\[\colorbox{thm}{$\mathbb{E}\left\lbrack \mathbb{E}\left\lbrack X|\mathcal{G}_{2} \right\rbrack|\mathcal{G}_{1} \right\rbrack=\mathbb{E}\left\lbrack \mathbb{E}\left\lbrack X|\mathcal{G}_{1} \right\rbrack|\mathcal{G}_{2} \right\rbrack=\mathbb{E}\left\lbrack X|\mathcal{G}_{1} \right\rbrack$}\]</div>
<p><span class="math notranslate nohighlight">\(\mathbb{E}\left| X \right| &lt; + \infty\)</span> guarantees all conditional
expectations are finite. By definition of conditional expectation,
given <span class="math notranslate nohighlight">\(\mathcal{G}_{1}\mathcal{\subset F}\)</span>, a RV
<span class="math notranslate nohighlight">\(Y = \mathbb{E}\left\lbrack X|\mathcal{G}_{1} \right\rbrack\)</span> iff it is
finite, <span class="math notranslate nohighlight">\(\mathcal{G}_{1}\)</span>-measurable and
<span class="math notranslate nohighlight">\(\int_{A}^{}{Y\operatorname{d}\mathbb{P}} = \int_{A}^{}{X\operatorname{d}\mathbb{P}},\forall A \in \mathcal{G}_{1}\)</span>.
Therefore,</p>
<div class="math notranslate nohighlight">
\[\int_{A}^{}{\mathbb{E}\left\lbrack X|\mathcal{G}_{1} \right\rbrack \operatorname{d}\mathbb{P}} = \int_{A}^{}{X\operatorname{d}\mathbb{P}},\forall A \in \mathcal{G}_{1}\]</div>
<div class="math notranslate nohighlight">
\[\int_{A}^{}{\mathbb{E}\left\lbrack X|\mathcal{G}_{2} \right\rbrack \operatorname{d}\mathbb{P}} = \int_{A}^{}{X\operatorname{d}\mathbb{P}},\forall A \in \mathcal{G}_{2}\]</div>
<p>Then since <span class="math notranslate nohighlight">\(\mathcal{G}_{1} \subseteq \mathcal{G}_{2}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\int_{A}^{}{\mathbb{E}\left\lbrack X|\mathcal{G}_{2} \right\rbrack \operatorname{d}\mathbb{P}} = \int_{A}^{}{X\operatorname{d}\mathbb{P}},\forall A \in \mathcal{G}_{1} \Rightarrow \int_{A}^{}{\mathbb{E}\left\lbrack X|\mathcal{G}_{1} \right\rbrack \operatorname{d}\mathbb{P}} = \int_{A}^{}{\mathbb{E}\left\lbrack X|\mathcal{G}_{2} \right\rbrack \operatorname{d}\mathbb{P}},\forall A \in \mathcal{G}_{1}\]</div>
<p>Note <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|\mathcal{G}_{1} \right\rbrack\)</span> is
finite, <span class="math notranslate nohighlight">\(\mathcal{G}_{1}\)</span>-measurable and together with (1?€?3) we have</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left\lbrack X|\mathcal{G}_{1} \right\rbrack=\mathbb{E}\left\lbrack \mathbb{E}\left\lbrack X|\mathcal{G}_{2} \right\rbrack|\mathcal{G}_{1} \right\rbrack\]</div>
<p>The other identity is simply due to
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|\mathcal{G}_{1} \right\rbrack\)</span> is
<span class="math notranslate nohighlight">\(\mathcal{G}_{1}\)</span> measurable and hence <span class="math notranslate nohighlight">\(\mathcal{G}_{2}\)</span> measurable,
and then by <a class="reference internal" href="__01_probability.html#property-conditional-expt-absorbing-condition"><span class="std std-ref">Property 1-2</span></a>, we have
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack \mathbb{E}\left\lbrack X|\mathcal{G}_{1} \right\rbrack|\mathcal{G}_{2} \right\rbrack=\mathbb{E}\left\lbrack X|\mathcal{G}_{1} \right\rbrack\)</span>.</p>
<p id="corollary-conditional-expt-tower-rvs"><span class="ititle2">Corollary 1-1.</span>. For any RV <span class="math notranslate nohighlight">\(X,Y,Z\)</span>,
<span><span class="result-highlight"> if <span class="math notranslate nohighlight">\(σ\left( Z \right) \subseteq σ\left( Y \right)\)</span>, then
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack \mathbb{E}\left\lbrack X|Y \right\rbrack|Z \right\rbrack=\mathbb{E}\left\lbrack \mathbb{E}\left\lbrack X|Z \right\rbrack|Y \right\rbrack=\mathbb{E}\left\lbrack X|Z \right\rbrack\)</span> </span></span>.</p>
<p id="corollary-conditional-expt-absorbing-rv"><span class="ititle2">Corollary 1-2.</span>. For any RV <span class="math notranslate nohighlight">\(X,Y\)</span>, we
have <span><span class="result-highlight"> a.s. <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack \mathbb{E}\left\lbrack X|Y \right\rbrack \right\rbrack=\mathbb{E}X\)</span> </span></span>,
simply because</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left\lbrack \mathbb{E}\left\lbrack X|Y \right\rbrack \right\rbrack=\mathbb{E}\left\lbrack \mathbb{E}\left\lbrack X|σ\left( Y \right) \right\rbrack|\left\{ \Omega,\varnothing \right\} \right\rbrack=\mathbb{E}\left\lbrack X|\left\{ \Omega,\varnothing \right\} \right\rbrack=\mathbb{E}X\]</div>
<p id="corollary-conditional-expt-intuitive-total-expectation"><span class="ititle2">Corollary 1-3.</span>. Let
<span class="math notranslate nohighlight">\(\mathcal{P =}\left\{ A_{1},A_{2},\ldots \right\}\)</span> be a partition of
<span class="math notranslate nohighlight">\(\Omega\)</span>, then for any RV <span class="math notranslate nohighlight">\(X\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\colorbox{rlt}{$\mathbb{E}X = \sum_{i}^{}{\mathbb{E}\left\lbrack X|A_{i} \right\rbrack\mathbb{P}\left( A_{i} \right)}$}\]</div>
<p>Let RV <span class="math notranslate nohighlight">\(Y\)</span> be any piecewise constant function over <span class="math notranslate nohighlight">\(\mathcal{P}\)</span>, then
<span class="math notranslate nohighlight">\(σ\left( Y \right) = σ\left( P \right)\)</span>. Recall
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|σ\left( P \right) \right\rbrack\)</span> is also
a piecewise constant function defined a.s. by (1?€?1), then</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left\lbrack \mathbb{E}\left\lbrack X|σ\left( P \right) \right\rbrack \right\rbrack = \int_{\Omega}^{}{\mathbb{E}\left\lbrack X|σ\left( P \right) \right\rbrack \operatorname{d}\mathbb{P}} = \sum_{i}^{}{\mathbb{E}\left\lbrack X|A_{i} \right\rbrack\mathbb{P}\left( A_{i} \right)}\]</div>
<p>and therefore</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}X = \mathbb{E}\left\lbrack \mathbb{E}\left\lbrack X|Y \right\rbrack \right\rbrack = \mathbb{E}\left\lbrack \mathbb{E}\left\lbrack X|σ\left( P \right) \right\rbrack \right\rbrack = \sum_{i}^{}{\mathbb{E}\left\lbrack X|A_{i} \right\rbrack\mathbb{P}\left( A_{i} \right)}\]</div>
</li></ul>
<p>Given an event <span class="math notranslate nohighlight">\(A_{1}\in \mathcal{ F}\)</span>, the  <span><span class="def"> <span class="target" id="index-77"></span>conditional probability measure</span></span> given <span class="math notranslate nohighlight">\(A_{1}\)</span> is defined as
<span class="math notranslate nohighlight">\(\mathbb{P}_{A_{1}}\left( A \right) = \frac{\mathbb{P}\left( A\bigcap A_{1} \right)}{\mathbb{P}\left( A_{1} \right)},\forall A\in \mathcal{ F}\)</span>.
We more often denote
<span class="math notranslate nohighlight">\({\mathbb{P}\left( A|A_{1} \right):=\mathbb{P}}_{A_{1}}\left( A \right)\)</span>.
For any two events <span class="math notranslate nohighlight">\(A_{1},A_{2}\in \mathcal{ F}\)</span>, the identity
<span class="math notranslate nohighlight">\(\colorbox{rlt}{$\mathbb{P}\left( A_{1}|A_{2} \right) = \frac{\mathbb{P}\left( A_{1}\bigcap A_{2} \right)}{\mathbb{P}\left( A_{2} \right)}$}\)</span>
is known as the  <span><span class="def"> <span class="target" id="index-78"></span>Bayes’ rule for events</span></span>. Let <span class="math notranslate nohighlight">\(A_{1},A_{2},\ldots\)</span> be
any partition of <span class="math notranslate nohighlight">\(\Omega\)</span> in <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, then for any
<span class="math notranslate nohighlight">\(A\in \mathcal{ F}\)</span> we have</p>
<div class="math notranslate nohighlight" id="equation-eq-total-probability-for-events">
<span class="eqno">(1.3)<a class="headerlink" href="#equation-eq-total-probability-for-events" title="Permalink to this equation">¶</a></span>\[\colorbox{rlt}{$\mathbb{P}\left( A \right) = \sum_{i}^{}{\mathbb{P}\left( A|A_{i} \right)\mathbb{P}\left( A_{i} \right)}$}\]</div>
<p>because <span class="math notranslate nohighlight">\(A = \bigcup_{i}^{}\left( A\bigcap A_{i} \right)\)</span> and
<span class="math notranslate nohighlight">\(\mathbb{P}\left( A \right)=\mathbb{P}\left( \bigcup_{i}^{}\left( A\bigcap A_{i} \right) \right) = \sum_{i}^{}{\mathbb{P}\left( A\bigcap A_{i} \right)} = \sum_{i}^{}{\mathbb{P}\left( A|A_{i} \right)\mathbb{P}\left( A_{i} \right)}\)</span>,
where we use axiom 2) of measure and the Bayes’ rule. We call identity
(1?€?4) as the  <span><span class="def"> <span class="target" id="index-79"></span>law of total probability for events</span></span>.
<span class="emp">underline</span> total probability is Bayes’ rule when the
summation has only one addend, therefor it is Bayes’ rule generalized to
multiple events.</p>
<p><span style="padding-left:20px"></span> We now construct the idea of conditional probability. Similar to
conditional expectation, given an event <span class="math notranslate nohighlight">\(A\in \mathcal{ F}\)</span> and a
<span class="math notranslate nohighlight">\(σ\)</span>-algebra <span class="math notranslate nohighlight">\(\mathcal{G \subseteq F}\)</span>, we define the  <span><span class="def"> <span class="target" id="index-80"></span>conditional probability given σ-algebra</span></span> <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>, denoted by
<span class="math notranslate nohighlight">\(\mathbb{P}\left( A|\mathcal{G} \right)\)</span>, as a non-negative
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,G} \right) \rightarrow \left( \mathbb{R}\mathcal{,B}\left( \mathbb{R} \right) \right)\)</span>
RV s.t.</p>
<div class="math notranslate nohighlight" id="equation-eq-conditional-prob-axiom">
<span class="eqno">(1.4)<a class="headerlink" href="#equation-eq-conditional-prob-axiom" title="Permalink to this equation">¶</a></span>\[\forall G \in \mathcal{G,}\mathbb{P}\left( A \cap G \right) = \int_{G}^{}{\mathbb{P}\left( A \middle| \mathcal{G} \right)\operatorname{d}\mathbb{P}}\]</div>
<p>This can be shown to be a.s. unique in <a class="reference internal" href="__01_probability.html#theorem-conditional-prob-existence-uniqueness"><span class="std std-ref">Theorem 1-3</span></a>.
We then define  <span><span class="def"> <span class="target" id="index-81"></span>conditional probability given partition</span></span> <span class="math notranslate nohighlight">\(\mathcal{P}\)</span> by
<span class="math notranslate nohighlight">\(\mathbb{P}\left( A|\mathcal{P} \right):=\mathbb{P}\left( A|σ\left( \mathcal{P} \right) \right)\)</span>
and  <span><span class="def"> <span class="target" id="index-82"></span>conditional probability given RV</span></span> <span class="math notranslate nohighlight">\(Y\)</span> by
<span class="math notranslate nohighlight">\(\mathbb{P}\left( A|Y \right):=\mathbb{P}\left( A|σ\left( Y \right) \right)\)</span>.
It is not hard to figure out the <span class="emp">intuition</span> behind <a class="reference internal" href="#equation-eq-conditional-prob-axiom">Eq.1.4</a>
if we consider <span class="math notranslate nohighlight">\(\mathbb{P}\left( A|\mathcal{P} \right)\)</span> where partition
<span class="math notranslate nohighlight">\(\mathcal{P =}\left\{ A_{1},A_{2},\ldots \right\}\)</span>. In this case
<span class="math notranslate nohighlight">\(\mathbb{P}\left( A|\mathcal{P} \right)\)</span> is a piecewise constant
function, therefore, given any <span class="math notranslate nohighlight">\(A\in \mathcal{ F}\)</span>, we have</p>
<table class="colwidths - given docutils" style="background:none; border:none;" width="100%" align="center" ><colgroup><col width = "60%" /><col width = "40%" /></colgroup><tbody><tr class="row-odd" style="text-align:center; vertical-align:middle; "><td  rowspan="2" style="background:none; border:none;text-align:justify; vertical-align:middle; "><div class="math notranslate nohighlight">
\[\mathbb{P}\left( A \cap A_{i} \right) = \int_{A_{i}}^{}{\mathbb{P}\left( A \middle| \mathcal{G} \right)\operatorname{d}\mathbb{P}} = \mathbb{P}\left( A \middle| \mathcal{G} \right)\mathbb{P}\left( A_{i} \right) \Rightarrow \mathbb{P}\left( A \middle| \mathcal{G} \right)\left( \omega \right) = \frac{\mathbb{P}\left( A \cap A_{i} \right)}{\mathbb{P}\left( A_{i} \right)}=\mathbb{P}\left( A|A_{i} \right),\forall\omega \in A_{i}\]</div>
</td><td  style="background:none; border:none;"><a class="reference internal image-reference" href="conditional_probability.png"><img alt="conditional_probability.png" src="conditional_probability.png" /></a>
</td></tr><tr><td  style="background:none; border:none;text-align:justify; vertical-align:middle; "><p id="figure-conditional-prob"><span style="text-align:justify"> <span><span class="ibold"> Figure 1-2</span></span> Illustration of conditional probability.</span></span> <span class="math notranslate nohighlight">\(\mathbb{P}\left( A|Y \right)\)</span>
where <span class="math notranslate nohighlight">\(Y\)</span> is a discrete RV and
<span class="math notranslate nohighlight">\(\left| Y\left( \Omega \right) \right| = 4\)</span>.
<span class="math notranslate nohighlight">\(\mathbb{P}\left( A|Y \right)\)</span> is a piecewise constant function over
each <span class="math notranslate nohighlight">\(A_{i},i = 1,2,3,4\)</span>, and with
<span class="math notranslate nohighlight">\(\mathbb{P}\left( \cdot |Y \right)\left( \omega \right)\)</span> can be four
different probability measures depending which <span class="math notranslate nohighlight">\(A_{i}\)</span> contains
<span class="math notranslate nohighlight">\(\omega\)</span>.</p>
</td></tr></tbody></table><p>meaning <span class="math notranslate nohighlight">\(\mathbb{P}\left( A \middle| \mathcal{G} \right)\)</span> over each
<span class="math notranslate nohighlight">\(A_{i}\)</span> is simply a constant
<span class="math notranslate nohighlight">\(\frac{\mathbb{P}\left( A \cap A_{i} \right)}{\mathbb{P}\left( A_{i} \right)}\)</span>,
the conditional probability of event <span class="math notranslate nohighlight">\(A\)</span> given <span class="math notranslate nohighlight">\(A_{i}\)</span>; this is similar
to the conditional expectation of a RV <span class="math notranslate nohighlight">\(X\)</span> given a partition
<span class="math notranslate nohighlight">\(\mathcal{P}\)</span>, which is a piecewise constant function that equals
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X|A_{i} \right\rbrack\)</span> over each set <span class="math notranslate nohighlight">\(A_{i}\)</span> in
the partition.</p>
<ul style="margin-left:20px">
<p><li style="margin-top:10px"></p>
<p id="theorem-conditional-prob-existence-uniqueness"><span class="ititle">Theorem 1-3.</span> <span class="bemp">Existence and uniqueness of conditional probability.</span> <span><span class="theorem-highlight"> For any <span class="math notranslate nohighlight">\(A\in \mathcal{ F}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{G \subseteq F}\)</span>, the
conditional probability exists and is a.s. unique</span></span>. Notice
<span class="math notranslate nohighlight">\(\mathbb{P}_{A}\left( \cdot \right):=\mathbb{P}\left( A\bigcap_{}^{}\left( \cdot \right) \right)\)</span>
is a measure for <span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,G} \right)\)</span> by checking those
axioms, and we can also verify <span class="math notranslate nohighlight">\(\mathbb{P}_{A}\mathbb{\ll P}\)</span>. Also,
<span class="math notranslate nohighlight">\(\mathbb{P}\)</span> is also a valid measure for
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,G} \right)\)</span>, then by <a class="reference internal" href="__01_probability.html#fact-prob-randon-nikodym-theorem"><span class="std std-ref">Fact 1-4</span></a> Radon-Nikodym
Theorem we have a non-negative <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>-measurable function
<span class="math notranslate nohighlight">\(\mathbb{P}\left( A \middle| \mathcal{G} \right) = \frac{\operatorname{d}\mathbb{P}_{A}}{\operatorname{d}\mathbb{P}}\)</span>
exists and is a.s. unique.</p>
</li>
<p><li style="margin-top:10px"></p>
<p id="property-conditional-prob-as-measure"><span class="ititle">Property 1-3.</span> <span><span class="result-highlight"> Given any <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span>, then
<span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{G}}^{\left( \omega \right)}\left( \cdot \right):=\mathbb{P}\left( \cdot \middle| \mathcal{G} \right)\left( \omega \right)\)</span>
is a.s. a valid measure on <span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right)\)</span> </span></span>.
<strong>First</strong>, <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{G}}^{\left( \omega \right)} \geq 0\)</span>
by definition. <strong>Secondly</strong>,
<span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{G}}^{\left( \omega \right)}\left( \varnothing \right) = 0\)</span>
because by (1?€?5)
<span class="math notranslate nohighlight">\(0 = \mathbb{P}\left( \varnothing \cap G \right) = \int_{G}^{}{\mathbb{P}\left( \varnothing \middle| \mathcal{G} \right)\operatorname{d}\mathbb{P,\forall}G \in \mathcal{G \Rightarrow}\mathbb{P}\left( \varnothing \middle| \mathcal{G} \right) \equiv 0}\)</span>
a.s. <strong>Thirdly</strong>, for any
<span class="math notranslate nohighlight">\(A_{1},A_{2}\in \mathcal{ F,}A_{1}\bigcap A_{2} = \varnothing\)</span>, then
<span class="math notranslate nohighlight">\(A_{1}\bigcap G\)</span> is disjoint form <span class="math notranslate nohighlight">\(A_{2}\bigcap G\)</span> and</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\int_{G}^{}{\mathbb{P}\left( A_{1}\bigcup A_{2} \middle| \mathcal{G} \right)\operatorname{d}\mathbb{P}}
&amp;=\mathbb{P}\left( \left( A_{1}\bigcup A_{2} \right) \cap G \right)
=\mathbb{P}\left( \left( A_{1}\bigcap G \right)\bigcup\left( A_{2}\bigcap G \right) \right)
=\mathbb{P}\left( A_{1}\bigcap G \right)\mathbb{+ P}\left( A_{2}\bigcap G \right) \\
&amp;= \int_{G}^{}{\mathbb{P}\left( A_{1} \middle| \mathcal{G} \right)\operatorname{d}\mathbb{P}} + \int_{G}^{}{\mathbb{P}\left( A_{2} \middle| \mathcal{G} \right)\operatorname{d}\mathbb{P}}
= \int_{G}^{}{\left( \mathbb{P}\left( A_{1} \middle| \mathcal{G} \right)\mathbb{+ P}\left( A_{2} \middle| \mathcal{G} \right) \right)\operatorname{d}\mathbb{P}}
\end{aligned}\end{split}\]</div>
<p>Then by uniqueness we have a.s.
<span class="math notranslate nohighlight">\(\mathbb{P}\left( A_{1}\bigcup A_{2} \middle| \mathcal{G} \right) = \mathbb{P}\left( A_{1} \middle| \mathcal{G} \right)\mathbb{+ P}\left( A_{2} \middle| \mathcal{G} \right)\)</span>.</p>
</li></ul>
<p><span style="padding-left:20px"></span> Now given a <span class="math notranslate nohighlight">\((\Omega,\mathcal{F}) \to (S,\mathcal{E})\)</span> RV <span class="math notranslate nohighlight">\(Y\)</span>, define the  <span><span class="def"> <span class="target" id="index-83"></span>regular conditional probability</span></span> conditioned on <span class="math notranslate nohighlight">\(Y\)</span> as a
function <span class="math notranslate nohighlight">\(\upsilon\left( x,A \right):S\mathcal{\times F \rightarrow}\left\lbrack 0,1 \right\rbrack\)</span> s.t. <span><span class="fact-highlight"> <strong>1)</strong> <span class="math notranslate nohighlight">\(\upsilon\left( x, \cdot \right)\)</span> is a valid probability measure
on <span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right)\)</span> for any <span class="math notranslate nohighlight">\(x \in S\)</span>; <strong>2)</strong>
<span class="math notranslate nohighlight">\(\upsilon\left( \cdot ,A \right)\)</span> is a <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>-measurable function
in the first argument s.t.
<span class="math notranslate nohighlight">\(\int_{B}^{}{\upsilon\left( y,A \right)\operatorname{d}Y} = \mathbb{P}\left( A\bigcap Y^{- 1}\left( B \right) \right),\forall A\in \mathcal{ F}\)</span> </span></span>,
where <span class="emp">for simplicity</span> we denote <span class="math notranslate nohighlight">\(\mathbb{P}_Y:=\mathbb{P} \circ Y^{-1}\)</span> and the integration w.r.t. the distribution <span class="math notranslate nohighlight">\(\mathbb{P}_{Y}\)</span>
of RV <span class="math notranslate nohighlight">\(Y\)</span> as
<span class="math notranslate nohighlight">\(\int_{B}^{}{\upsilon dY} := \int_{B}^{}{\upsilon \operatorname{d}\left( \mathbb{P \circ}Y^{- 1} \right)} = \int_{B}^{}{\upsilon \operatorname{d}\mathbb{P}_{Y}}\)</span>.
The existence of <span class="math notranslate nohighlight">\(\upsilon\left( x,A \right)\)</span> satisfying axiom 2) can be
shown in the same way as <a class="reference internal" href="__01_probability.html#theorem-conditional-prob-existence-uniqueness"><span class="std std-ref">Theorem 1-3</span></a> since
<span class="math notranslate nohighlight">\(\mathbb{P}\left( A\bigcap Y^{- 1}\left( \cdot \right) \right)\)</span> and
<span class="math notranslate nohighlight">\(\mathbb{P}_{Y}\)</span> are a valid measures on
<span class="math notranslate nohighlight">\(\left( S,\mathcal{E} \right)\)</span> and <span class="math notranslate nohighlight">\(\mathbb{P}\left( A\bigcap Y^{- 1}\left( \cdot \right) \right) \ll \mathbb{P}_{Y}\)</span>.
As shown in <a class="reference internal" href="__01_probability.html#property-conditional-prob-as-measure"><span class="std std-ref">Property 1-3</span></a>, for any <span class="math notranslate nohighlight">\(x \in S\)</span>, <span class="math notranslate nohighlight">\(\upsilon\left( x, \cdot \right)\)</span> is a.s. a probability
measure, however, to satisfy axiom 1), which does not contain the “a.s.”
modifier, there has to be additional requirement on
<span class="math notranslate nohighlight">\(\left( S,\mathcal{E} \right)\)</span>; it requires <span class="math notranslate nohighlight">\(S\)</span> be a  <span><span class="exdef"> <span class="target" id="index-84"></span>separable metric space</span></span> and <span class="math notranslate nohighlight">\(\mathcal{E = B}\left( S \right)\)</span>, and the proof is beyond our scope; we
state the fact our ordinary space
<span class="math notranslate nohighlight">\(\left( \mathbb{R}^{n}\mathcal{,B}\left( \mathbb{R}^{n} \right) \right)\)</span>
satisfy this requirement and so <span><span class="fact-highlight"> the regular conditional probability is guaranteed to exist on
<span class="math notranslate nohighlight">\(\left( \mathbb{R}^{n}\mathcal{,B}\left( \mathbb{R}^{n} \right) \right)\)</span> </span></span>.
The <a class="reference internal" href="__01_probability.html#fact-integration-change-of-variable-image-measure"><span class="std std-ref">Fact 1-3</span></a> change of variable indicates
<span class="math notranslate nohighlight">\(\int_B \upsilon\operatorname{d}Y=\int_{B} \upsilon \operatorname{d}(\mathbb{P} \circ Y^{-1})=\int_{Y^{-1}(B)}(\upsilon \circ Y)\operatorname{d}\mathbb{P}\)</span>
, and then the axiom 2) becomes</p>
<div class="math notranslate nohighlight">
\[\int_{Y^{- 1}\left( B \right)}^{}{\upsilon\left( Y\left( \omega \right),A \right)\operatorname{d}\mathbb{P}}=\mathbb{P}\left( A\bigcap Y^{- 1}\left( B \right) \right),\forall A\in \mathcal{ F} \Longleftrightarrow \int_{G}^{}{\upsilon\left( Y\left( \omega \right),A \right)\operatorname{d}\mathbb{P}}=\mathbb{P}\left( A\bigcap G \right),\forall A\in \mathcal{ F}Gσ\left( Y \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\upsilon\left( Y\left( \omega \right),A \right)\)</span> is a
<span class="math notranslate nohighlight">\(σ\left( Y \right)\)</span>-measurable function and we can see
<span><span class="result-highlight"> <span class="math notranslate nohighlight">\(\upsilon\left( Y\left( \omega \right),A \right) = \mathbb{P}\left( A \middle| σ\left( Y \right) \right)\left( \omega \right)\)</span> a.s.</span></span>
by uniqueness of <a class="reference internal" href="__01_probability.html#theorem-conditional-prob-existence-uniqueness"><span class="std std-ref">Theorem 1-3</span></a>, and it is guaranteed
<span class="math notranslate nohighlight">\(\mathbb{P}_{σ{(Y)}}^{\left( \omega \right)}(\cdot)=\mathbb{P}(\cdot|\mathcal{G})(\omega)\)</span>
can be chosen to be a probability measure for all <span class="math notranslate nohighlight">\(ω∈Ω\)</span> if the regular conditional probability exists.
Also, for any
<span class="math notranslate nohighlight">\(\omega_{1},\omega_{2} \in \Omega,\omega_{1} \neq \omega_{2}\)</span> but
<span class="math notranslate nohighlight">\(Y\left( \omega_{1} \right) = Y\left( \omega_{2} \right)\)</span>, we have
<span class="math notranslate nohighlight">\(\mathbb{P}_{σ{(Y)}}^{\left( \omega_1 \right)}=\mathbb{P}_{σ{(Y)}}^{\left( \omega_2 \right)}\)</span>
Finally, we are able to define  <span><span class="def"> <span class="target" id="index-85"></span>conditional distribution of a RV</span></span>
<span class="math notranslate nohighlight">\(X:\left( \Omega\mathcal{,F} \right) \rightarrow \left( S,\mathcal{E} \right)\)</span>
given <span class="math notranslate nohighlight">\(Y = y\)</span> as the probability measure</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}_{X|Y = y}\left( \cdot \right) := \upsilon\left( y,X^{- 1}\left( \cdot \right) \right) = \upsilon\left( Y\left( \omega \right),X^{- 1}\left( \cdot \right) \right),\omega{\in Y}^{- 1}\left( y \right)\]</div>
<p>and define the  <span><span class="def"> <span class="target" id="index-86"></span>conditional probability density</span></span>
<span class="math notranslate nohighlight">\(f_{X|Y}\left( x,y \right)\)</span> as the Radon-Nikodym derivative of
<span class="math notranslate nohighlight">\(\mathbb{P}_{X|Y}\)</span>. The axiom 2) is actually the  <span><span class="def"> <span class="target" id="index-87"></span>law of total probability</span></span>, a general version <a class="reference internal" href="#equation-eq-total-probability-for-events">Eq.1.3</a>. <span class="emp">To see the connection</span>,
suppose <span class="math notranslate nohighlight">\(Y^{- 1}\)</span> is a discrete RV, then axiom
2) becomes</p>
<div class="math notranslate nohighlight">
\[\sum_{y \in B}^{}{\mathbb{P}_{X|Y = y}\left( A \right)\mathbb{P}\left( Y = y \right)} = \sum_{y \in B}^{}{\upsilon\left( y,A \right)\mathbb{P}\left( Y = y \right)}\mathbb{= P}\left( A\bigcap Y^{- 1}\left( B \right) \right),\forall A\in \mathcal{ F,}B\in \mathcal{E}\]</div>
<ul style="margin-left:20px">
<p><li style="margin-top:10px"></p>
<p id="theorem-bayes-rule-for-density"><span class="ititle">Theorem 1-4.</span> <span class="bemp">Bayes’ rule for density function.</span> <span><span class="theorem-highlight"> Suppose <span class="math notranslate nohighlight">\(\left( X,Y \right)\)</span> is a
<span class="math notranslate nohighlight">\(\left( \Omega\mathcal{,F} \right) \rightarrow \left( S_{1} \times S_{2},\mathcal{E} \right)\)</span>
RV with density <span class="math notranslate nohighlight">\(f\left( x,y \right) \in \mathcal{L}^{1}\)</span>. Let <span class="math notranslate nohighlight">\(X,Y\)</span> be
the marginal RVs with marginal densities <span class="math notranslate nohighlight">\(f_{X}\left( x \right)\)</span> and
<span class="math notranslate nohighlight">\(f_{Y}\left( y \right)\)</span>, then <span class="math notranslate nohighlight">\(\mathbb{P}_{Y}\)</span>-a.s.
<span class="math notranslate nohighlight">\(f_{X|Y}\left( x,y \right) = \frac{f\left( x,y \right)}{f_{Y}\left( y \right)}\)</span> </span></span>,
i.e.</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}_{X|Y = y}\left( B \right) = \int_{B}^{}\frac{f\left( x,y \right)}{f_{Y}\left( y \right)}\operatorname{d}x,\forall B\in \mathcal{E}\]</div>
<p>where note the integration is w.r.t. the Lebesgue measure. Fix any <span class="math notranslate nohighlight">\(B_{1}\in \mathcal{E}\)</span>, then for any <span class="math notranslate nohighlight">\(B_{2}\in \mathcal{E}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\int_{B_{2}}^{}{\left( \int_{B_{1}}^{}\frac{f\left( x,y \right)}{f_{Y}\left( y \right)}\operatorname{d}x \right)\operatorname{d}Y}
&amp;=\int_{B_{2}}^{}{\left( \int_{B_{1}}^{}\frac{f\left( x,y \right)}{f_{Y}\left( y \right)}\operatorname{d}x \right)f_{Y}\left( y \right)dy}
=\int_{B_{2}}^{}{\left( \int_{B_{1}}^{}{f\left( x,y \right)}\operatorname{d}x \right)dy} \\
&amp;=\int_{B_{1} \times B_{2}}^{}{f\left( x,y \right)\operatorname{d}x\operatorname{d}y}
=\mathbb{P}\left( B_{1} \times B_{2} \right)
=\mathbb{P}\left( \left\{ X \in B_{1} \right\} \cap \left\{ Y \in B_{2} \right\} \right) \\
&amp;=\int_{{Y^{- 1}(B}_{2})}^{}{\mathbb{P}\left( X \in B_{1}|σ\left( Y \right) \right)\operatorname{d}\mathbb{P}}
=\int_{{Y^{- 1}(B}_{2})}^{}{\upsilon\left( Y\left( \omega \right),X^{- 1}\left( B_{1} \right) \right)\operatorname{d}\mathbb{P}} \\
&amp;=\int_{{Y^{- 1}(B}_{2})}^{}{\left( \upsilon\left( \cdot ,X^{- 1}\left( B_{1} \right) \right) \circ Y \right)\operatorname{d}\mathbb{P}}
=\int_{B_{2}}^{}{\left( \upsilon\left( y,X^{- 1}\left( B_{1} \right) \right) \circ Y \right)\operatorname{d}Y} \\
&amp;=\int_{B_{2}}^{}{\mathbb{P}_{X|Y = y}(B_{1})\operatorname{d}Y}
\end{aligned}\end{split}\]</div>
<p>That is, <span class="math notranslate nohighlight">\(\int_{B_{2}}^{}{\left( \int_{B_{1}}^{}\frac{f\left( x,y \right)}{f_{Y}\left( y \right)}\operatorname{d}x \right)\operatorname{d}Y}=\int_{B_{2}}^{}{\mathbb{P}_{X|Y = y}(B_{1})\operatorname{d}Y}\)</span>
indicating <span class="math notranslate nohighlight">\(\mathbb{P}_{X|Y = y}\left( B_{1} \right)=\int_{B_{1}}^{}\frac{f\left( x,y \right)}{f_{Y}\left( y \right)}\operatorname{d}x\)</span>
(<span class="math notranslate nohighlight">\(\mathbb{P}_{Y}\)</span>-a.s.).</p>
</li></ul>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["color.js","autoload-all.js"] }
  });

      MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
   var color = MathJax.Extension["TeX/color"];
   color.colors["theorem"] = color.getColor('RGB','255,229,153');
       color.colors["result"] = color.getColor('RGB','189,214,238');
       color.colors["fact"] = color.getColor('RGB','255,255,204');
       color.colors["emperical"] = color.getColor('RGB','253,240,207');
       color.colors["comment"] = color.getColor('RGB','204,255,204');
   color.colors["thm"] = color.getColor('RGB','255,229,153');
       color.colors["rlt"] = color.getColor('RGB','189,214,238');
       color.colors["emp"] = color.getColor('RGB','253,240,207');
       color.colors["comm"] = color.getColor('RGB','204,255,204');
       color.colors["conn1"] = color.getColor('RGB','255,0,255');
       color.colors["conn2"] = color.getColor('RGB','237,125,49');
       color.colors["conn3"] = color.getColor('RGB','112,48,160');
      });
</script></div>
</div>
<div class="section" id="covariance-matrix">
<h2>1.2. Covariance Matrix<a class="headerlink" href="#covariance-matrix" title="Permalink to this headline">¶</a></h2>
<p><span style="padding-left:20px"></span> Given two scalar RVs <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, then the covariance
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\operatorname{cov} \left( {X,Y} \right) = \mathbb{E}\left[ {\left( {X - \mathbb{E}X} \right)\left( {Y - \mathbb{E}Y} \right)} \right]$}\)</span>.
Given a RV vector <span class="math notranslate nohighlight">\(X = \left( {\begin{array}{*{20}{c}}{{X_1}} \\   \vdots  \\  {{X_M}}\end{array}} \right)\)</span>,
define <span class="math notranslate nohighlight">\(\mathbb{E}X = \left( {\begin{array}{*{20}{c}}{\mathbb{E}{X_1}} \\\vdots  \\{\mathbb{E}{X_M}}\end{array}} \right)\)</span>
(and similarly the expectation of a RV matrix is to take expectation on each entry of that matrix), then the  <span><span class="def"> <span class="target" id="index-88"></span>covariance matrix</span></span> is defined as the following,</p>
<div class="math notranslate nohighlight" id="equation-eq-rv-cov">
<span class="eqno">(1.5)<a class="headerlink" href="#equation-eq-rv-cov" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
\Sigma \left( X \right) &amp; = \left( {\begin{array}{*{20}{c}}
{\operatorname{cov} \left( {{X_1},{X_1}} \right)}&amp; \cdots &amp;{\operatorname{cov} \left( {{X_1},{X_M}} \right)} \\
\vdots &amp; \ddots &amp; \vdots  \\
{\operatorname{cov} \left( {{X_M},{X_1}} \right)}&amp; \cdots &amp;{\operatorname{cov} \left( {{X_M},{X_M}} \right)}
\end{array}} \right) \hfill \\
&amp; = \left( {\begin{array}{*{20}{c}}
{\mathbb{E}\left[ {\left( {{X_1} - \mathbb{E}{X_1}} \right)\left( {{X_1} - \mathbb{E}{X_1}} \right)} \right]}&amp; \cdots &amp;{\mathbb{E}\left[ {\left( {{X_1} - \mathbb{E}{X_1}} \right)\left( {{X_M} - \mathbb{E}{X_M}} \right)} \right]} \\
\vdots &amp; \ddots &amp; \vdots  \\
{\mathbb{E}\left[ {\left( {{X_M} - \mathbb{E}{X_M}} \right)\left( {{X_1} - \mathbb{E}{X_1}} \right)} \right]}&amp; \cdots &amp;{\mathbb{E}\left[ {\left( {{X_M} - \mathbb{E}{X_M}} \right)\left( {{X_M} - \mathbb{E}{X_M}} \right)} \right]}
\end{array}} \right) \hfill \\
&amp;=\colorbox{fact}{$\mathbb{E}\left[ {\left( {X - \mathbb{E}X} \right){{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}} \right]$} \hfill \\
\end{align}\end{split}\]</div>
<p>where the diagonal elements are  <span><span class="exdef"> <span class="target" id="index-89"></span>variances</span></span> that can be denoted by <span class="math notranslate nohighlight">\(\operatorname{var}\left( X_{i} \right) := \operatorname{cov}\left( X_{i},X_{i} \right),i = 1,\ldots,M\)</span>
. We <span class="emp">note</span> there is difference that for two scalar RVs <span class="math notranslate nohighlight">\(X,Y\)</span>, <span class="math notranslate nohighlight">\(\operatorname{cov}⁡(X,Y)\)</span> is a value, but <span class="math notranslate nohighlight">\(\operatorname{\Sigma}\begin{pmatrix} X \\ Y \\ \end{pmatrix}\)</span> is a <span class="math notranslate nohighlight">\(2×2\)</span> matrix.</p>
<p><span style="padding-left:20px"></span> On the other hand, given two RVs <span class="math notranslate nohighlight">\(X,Y\)</span> and draw samples <span class="math notranslate nohighlight">\({\mathbf{x}} = \left( {{x_1}, \ldots ,{x_N}} \right)\sim X\)</span>
and <span class="math notranslate nohighlight">\({\mathbf{y}} = \left( {{y_1}, \ldots ,{y_N}} \right)\sim Y\)</span>, then we define the  <span><span class="def"> <span class="target" id="index-90"></span>sample covariance</span></span> of them as
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\operatorname{cov} \left( {{\mathbf{x}},{\mathbf{y}}} \right) = \frac{1}{{N - 1}}{\left( {{\mathbf{x}} - {\mathbf{\bar x}}} \right)^{\text{T}}}\left( {{\mathbf{y}} - {\mathbf{\bar y}}} \right)$}\)</span>.
Given a RV vector <span class="math notranslate nohighlight">\(X = \left( {\begin{array}{*{20}{c}}{{X_1}} \\\vdots  \\{{X_M}}\end{array}} \right)\)</span>,
we can draw  <span><span class="def"> <span class="target" id="index-91"></span>samples</span></span> <span class="math notranslate nohighlight">\({\mathbf{x}}_1^{\text{T}} = \left( {{x_{1,1}}, \ldots ,{x_{1,N}}} \right)\sim {X_1}, \ldots , {\mathbf{x}}_M^{\text{T}} = \left( {{x_{M,1}}, \ldots ,{x_{M,M}}} \right)\sim {X_M}\)</span>,
and form a  <span><span class="exdef"> <span class="target" id="index-92"></span>sample matrix</span></span> <span class="math notranslate nohighlight">\({\mathbf{X}} = \left( {\begin{array}{*{20}{c}}{{\mathbf{x}}_1^{\text{T}}} \\\vdots  \\{{\mathbf{x}}_M^{\text{T}}}\end{array}} \right)\)</span>.
In the machine learning context, the rows of <span class="math notranslate nohighlight">\(𝐗\)</span> are also referred to as <span class="tooltip">  <span><span class="def"> <span class="target" id="index-93"></span>feature vectors</span></span> <span class="tooltiptext"> Feature vectors are column vectors even though they are rows in the data matrix.</span></span> because it treats the RVs <span class="math notranslate nohighlight">\(X_1,…,X_M\)</span> as representing <span class="math notranslate nohighlight">\(M\)</span> random features of a data point;
and the columns of <span class="math notranslate nohighlight">\(𝐗\)</span> are called  <span><span class="def"> <span class="target" id="index-94"></span>data entries</span></span>, because they are actually observed data.
Then the  <span><span class="def"> <span class="target" id="index-95"></span>sample covariance matrix</span></span> is defined <span class="red">w.r.t. the feature vectors</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-sample-cov">
<span class="eqno">(1.6)<a class="headerlink" href="#equation-eq-sample-cov" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
{\Sigma }\left( {\mathbf{X}} \right) &amp;= \left( {\begin{array}{*{20}{c}}
{\operatorname{cov} \left( {{{\mathbf{x}}_1},{{\mathbf{x}}_1}} \right)}&amp; \cdots &amp;{\operatorname{cov} \left( {{{\mathbf{x}}_1},{{\mathbf{x}}_M}} \right)} \\
\vdots &amp; \ddots &amp; \vdots  \\
{\operatorname{cov} \left( {{{\mathbf{x}}_M},{{\mathbf{x}}_1}} \right)}&amp; \cdots &amp;{\operatorname{cov} \left( {{{\mathbf{x}}_M},{{\mathbf{x}}_M}} \right)}
\end{array}} \right) \hfill \\
&amp;= \frac{1}{{N - 1}}\left( {\begin{array}{*{20}{c}}
{{{\left( {{{\mathbf{x}}_1} - \overline {{{\mathbf{x}}_1}} } \right)}^{\text{T}}}\left( {{{\mathbf{x}}_1} - \overline {{{\mathbf{x}}_1}} } \right)}&amp; \cdots &amp;{{{\left( {{{\mathbf{x}}_1} - \overline {{{\mathbf{x}}_1}} } \right)}^{\text{T}}}\left( {{{\mathbf{x}}_M} - \overline {{{\mathbf{x}}_M}} } \right)} \\
\vdots &amp; \ddots &amp; \vdots  \\
{{{\left( {{{\mathbf{x}}_M} - \overline {{{\mathbf{x}}_M}} } \right)}^{\text{T}}}\left( {{{\mathbf{x}}_1} - \overline {{{\mathbf{x}}_1}} } \right)}&amp; \cdots &amp;{{{\left( {{{\mathbf{x}}_M} - \overline {{{\mathbf{x}}_M}} } \right)}^{\text{T}}}\left( {{{\mathbf{x}}_M} - \overline {{{\mathbf{x}}_M}} } \right)}
\end{array}} \right) \hfill \\
&amp;= \frac{1}{{N - 1}}\left( {{\mathbf{X}} - {\mathbf{\bar X}}} \right){\left( {{\mathbf{X}} - {\mathbf{\bar X}}} \right)^{\text{T}}} \hfill \\
\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\({\overline {{{\mathbf{x}}_i}} ^{\text{T}}} = \frac{1}{N}\mathop \sum \limits_{j = 1}^N {x_{i,j}}{1^{\text{T}}} = \left( {\frac{1}{N}\mathop \sum \limits_{j = 1}^N {x_{i,j}}, \ldots ,\frac{1}{N}\mathop \sum \limits_{j = 1}^N {x_{i,j}}} \right)\)</span>
(the same mean value repeating itself for <span class="math notranslate nohighlight">\(N\)</span> times)
and <span class="math notranslate nohighlight">\({\mathbf{\bar X}} = \left( {\begin{array}{*{20}{c}}{\overline {{{\mathbf{x}}_1}} } \\\vdots  \\{\overline {{{\mathbf{x}}_M}} }\end{array}} \right)\)</span>,
and the diagonal elements are  <span><span class="exdef"> <span class="target" id="index-96"></span>sample variances</span></span> that can be denoted as <span class="math notranslate nohighlight">\(\operatorname{var}\left( \mathbf{x}_{i} \right) := \operatorname{cov}\left( \mathbf{x}_{i},\mathbf{x}_{i} \right), i=1,...,M\)</span>.
The sum of variances, or the trace of the covariance matrix, is called the  <span><span class="def"> <span class="target" id="index-97"></span>total variance</span></span> of <span class="math notranslate nohighlight">\(X\)</span>.
In addition, <span class="emp">note</span> <span class="math notranslate nohighlight">\(\operatorname{cov} \left( {{\mathbf{x}},{\mathbf{y}}} \right)\)</span> is a value, while <span class="math notranslate nohighlight">\(Σ(x,y)\)</span> is a <span class="math notranslate nohighlight">\(2×2\)</span> matrix.</p>
<p><span style="padding-left:20px"></span>  <span><span class="exdef"> <span class="target" id="index-98"></span>Pearson’s correlation</span></span> is the normalized version of covariance, defined as
<span class="math notranslate nohighlight">\(\operatorname{corr}\left( X,Y \right) = \frac{\operatorname{cov}\left( X,Y \right)}{\sqrt{\operatorname{var}\left( X \right)}\sqrt{\operatorname{var}\left( Y \right)}}\)</span>
for two scalar RVs <span class="math notranslate nohighlight">\(X,Y\)</span>, and
<span class="math notranslate nohighlight">\(\operatorname{corr}\left( \mathbf{x},\mathbf{y} \right) = \frac{\operatorname{cov}\left( \mathbf{x},\mathbf{y} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x} \right)}\sqrt{\operatorname{var}\left( \mathbf{y} \right)}}\)</span>
for two samples <span class="math notranslate nohighlight">\(\mathbf{x},\mathbf{y}\)</span>. A  <span><span class="def"> <span class="target" id="index-99"></span>correlation matrix</span></span> for a
RV vector <span class="math notranslate nohighlight">\(X = \begin{pmatrix} X_{1} \\  \vdots \\ X_{M} \\ \end{pmatrix}\)</span> or a sample matrix <span class="math notranslate nohighlight">\(\mathbf{X} = \begin{pmatrix} \mathbf{x}_{1}^{\rm{T}} \\  \vdots \\ \mathbf{x}_{M}^{\rm{T}} \\ \end{pmatrix}\)</span> are just replacing
<span class="math notranslate nohighlight">\(\operatorname{cov}\left( \cdot , \cdot \right)\)</span> elements in (1?€?1) and
(1?€?2) by corresponding
<span class="math notranslate nohighlight">\(\operatorname{corr}\left( \cdot , \cdot \right)\)</span>. Since
<span class="math notranslate nohighlight">\(\operatorname{corr}\left( X,X \right) = 1\)</span> and
<span class="math notranslate nohighlight">\(\operatorname{corr}\left( \mathbf{x},\mathbf{x} \right) = 1\)</span>, then <span><span class="result-highlight"> the diagonal elements of a correlation matrix will all be 1 </span></span>. Let
<span class="math notranslate nohighlight">\(\Sigma_{\text{corr}}\)</span> denote a correlation matrix, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\operatorname{\Sigma_{\text{corr}}} \left( X \right) = \begin{pmatrix}
1 &amp; \cdots &amp; \frac{\operatorname{cov}\left( X_{1},X_{M} \right)}{\sqrt{\operatorname{var}\left( X_{1} \right)}\sqrt{\operatorname{var}\left( X_{M} \right)}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\operatorname{cov}\left( X_{M},X_{1} \right)}{\sqrt{\operatorname{var}\left( X_{M} \right)}\sqrt{\operatorname{var}\left( X_{1} \right)}} &amp; \cdots &amp; 1 \\
\end{pmatrix} = \begin{pmatrix}
1 &amp; \cdots &amp; \operatorname{E}{\lbrack\frac{\left( X_{1}\mathbb{- E}X_{1} \right)}{\sqrt{\operatorname{var}\left( X_{1} \right)}}\frac{\left( X_{M}\mathbb{- E}X_{M} \right)}{\sqrt{\operatorname{var}\left( X_{M} \right)}}\rbrack} \\
\vdots &amp; \ddots &amp; \vdots \\
\operatorname{E}{\lbrack\frac{\left( X_{M}\mathbb{- E}X_{M} \right)}{\sqrt{\operatorname{var}\left( X_{M} \right)}}\frac{\left( X_{1}\mathbb{- E}X_{1} \right)}{\sqrt{\operatorname{var}\left( X_{1} \right)}}\rbrack} &amp; \cdots &amp; 1 \\
\end{pmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\Sigma_{\text{corr}}\left( \mathbf{X} \right) = \begin{pmatrix}
1 &amp; \cdots &amp; \frac{\operatorname{cov}\left( \mathbf{x}_{1},\mathbf{x}_{M} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{1} \right)}\sqrt{\operatorname{var}\left( \mathbf{x}_{M} \right)}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\operatorname{cov}\left( \mathbf{x}_{M},\mathbf{x}_{1} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{1} \right)}\sqrt{\operatorname{var}\left( \mathbf{x}_{M} \right)}} &amp; \cdots &amp; 1 \\
\end{pmatrix} = \frac{1}{N - 1}\begin{pmatrix}
1 &amp; \cdots &amp; \frac{\left( \mathbf{x}_{1} - \overline{\mathbf{x}_{1}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{1} \right)}}^{\rm{T}}\frac{\left( \mathbf{x}_{M} - \overline{\mathbf{x}_{M}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{M} \right)}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\left( \mathbf{x}_{M} - \overline{\mathbf{x}_{M}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{M} \right)}}^{\rm{T}}\frac{\left( \mathbf{x}_{1} - \overline{\mathbf{x}_{1}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{1} \right)}} &amp; \cdots &amp; 1 \\
\end{pmatrix}\end{split}\]</div>
<p>Given a RV <span class="math notranslate nohighlight">\(X\)</span>, we can define
<span class="math notranslate nohighlight">\(\widetilde{X} = \frac{\left( X - \mathbb{E}X \right)}{\sqrt{\operatorname{var}\left( X \right)}}\)</span>
as the  <span><span class="exdef"> <span class="target" id="index-100"></span>standardized RV</span></span> (or  <span><span class="exdef"> <span class="target" id="index-101"></span>normalized RV</span></span>) of <span class="math notranslate nohighlight">\(X\)</span> of zero
expectation and unit variance; given a sample <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, we can
define
<span class="math notranslate nohighlight">\(\widetilde{\mathbf{x}} = \frac{\left( \mathbf{x} - \overline{\mathbf{x}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x} \right)}}\)</span>
as the  <span><span class="exdef"> <span class="target" id="index-102"></span>standardized sample</span></span> (or  <span><span class="exdef"> <span class="target" id="index-103"></span>normalized sample</span></span>) of
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with zero mean and unit variance. We thus can define  <span><span class="exdef"> <span class="target" id="index-104"></span>standardize RV vector</span></span> <span class="math notranslate nohighlight">\(\widetilde{X} = \begin{pmatrix} {\widetilde{X}}_{1} \\  \vdots \\ {\widetilde{X}}_{M} \\ \end{pmatrix}\)</span> and  <span><span class="exdef"> <span class="target" id="index-105"></span>standardized sample matrix</span></span>
<span class="math notranslate nohighlight">\(\widetilde{\mathbf{X}} = \begin{pmatrix} {\widetilde{\mathbf{x}}}_{1}^{\rm{T}} \\  \vdots \\ {\widetilde{\mathbf{x}}}_{M}^{\rm{T}} \\ \end{pmatrix}\)</span>, and therefore
<span><span class="result-highlight"> <span class="math notranslate nohighlight">\(\operatorname{}\left( X \right) = \operatorname{E}{\lbrack{\widetilde{X}\widetilde{X}}^{\rm{T}}\rbrack},\Sigma_{\text{corr}}\left( \mathbf{X} \right) = \frac{1}{N - 1}\widetilde{\mathbf{X}}{\widetilde{\mathbf{X}}}^{\rm{T}}\)</span>,
where we see correlation matrix is just the concept of covariance matrix applied on standardize (normalized) RV or sample.</span></span></p>
<p><span style="padding-left:20px"></span>  <span><span class="def"> <span class="target" id="index-106"></span>Cross-covariance matrix</span></span> and  <span><span class="def"> <span class="target" id="index-107"></span>cross-correlation matrix</span></span> is a generalized concept of covariance matrix and correlation matrix that consider two RV vectors <span class="math notranslate nohighlight">\(X,Y\)</span> or two sample matrices <span class="math notranslate nohighlight">\(\mathbf{X},\mathbf{Y}\)</span>. For example, suppose <span class="math notranslate nohighlight">\(X,Y\)</span> are of lengths <span class="math notranslate nohighlight">\(M_{1},M_{2}\)</span>, following <a class="reference internal" href="#equation-eq-rv-cov">Eq.1.5</a>, the cross-covariance matrix for <span class="math notranslate nohighlight">\(X,Y\)</span> is defined as a <span class="math notranslate nohighlight">\(M_{1} \times M_{2}\)</span> matrix (<span class="emp">note</span> need not be a square matrix),</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned} \operatorname{\Sigma}\left( X,Y \right)
&amp;= \begin{pmatrix}
\operatorname{cov}{(X_{1},Y_{1})} &amp; \cdots &amp; \operatorname{cov}{(X_{1},Y_{M_{2}})} \\
\vdots &amp; \ddots &amp; \vdots \\
\operatorname{cov}{(X_{M_{1}},Y_{1})} &amp; \cdots &amp; \operatorname{cov}{(X_{M_{1}},Y_{M_{2}})} \\
\end{pmatrix} \\
&amp;= \begin{pmatrix}
\operatorname{E}{\lbrack\left( X_{1}\mathbb{- E}X_{1} \right)\left( Y_{1}\mathbb{- E}Y_{1} \right)\rbrack} &amp; \cdots &amp; \operatorname{E}{\lbrack\left( X_{1}\mathbb{- E}X_{1} \right)\left( Y_{M_{2}}\mathbb{- E}Y_{M_{2}} \right)\rbrack} \\
\vdots &amp; \ddots &amp; \vdots \\
\operatorname{E}{\lbrack\left( X_{M_{1}}\mathbb{- E}X_{M_{1}} \right)\left( Y_{1}\mathbb{- E}Y_{1} \right)\rbrack} &amp; \cdots &amp; \operatorname{E}{\lbrack\left( X_{M_{1}}\mathbb{- E}X_{M_{1}} \right)\left( Y_{M_{2}}\mathbb{- E}Y_{M_{2}} \right)\rbrack} \\
\end{pmatrix} \\
&amp;= \colorbox{fact}{$\operatorname{E}{\lbrack{\left( X - \mathbb{E}X \right)\left( Y - \mathbb{E}Y \right)}^{\rm{T}}\rbrack}$}
\end{aligned}\end{split}\]</div>
<p>And all others can be defined in the same way, summarized as <span class="math notranslate nohighlight">\(\colorbox{fact}{$\Sigma\left( \mathbf{X,Y} \right) = \frac{1}{N - 1}\left( \mathbf{X} - \overline{\mathbf{X}} \right)\left( \mathbf{Y} - \overline{\mathbf{Y}} \right)^{\rm{T}}$}\)</span>,
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\operatorname{}\left( X,Y \right) = \operatorname{E}{\lbrack{\widetilde{X}\widetilde{Y}}^{\rm{T}}\rbrack}$}\)</span>, and <span class="math notranslate nohighlight">\(\colorbox{fact}{$\Sigma_{\text{corr}}\left( \mathbf{X,Y} \right) = \frac{1}{N - 1}\widetilde{\mathbf{X}}{\widetilde{\mathbf{Y}}}^{\rm{T}}$}\)</span></p>
<div class="admonition caution">
<p class="first admonition-title">Caution</p>
<p>In machine learning problems, we are often given a data matrix <span class="math notranslate nohighlight">\({\mathbf{X}} = \left( {{{\mathbf{x}}_1}, \ldots ,{{\mathbf{x}}_N}} \right)\)</span>
with the columns <span class="math notranslate nohighlight">\({{\mathbf{x}}_1}, \ldots ,{{\mathbf{x}}_N}\)</span> as data entries.
The bold small-letter symbol “<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>” very often represents a data entry in the content of machine learning,
but in statistics it often instead represents a feature vector (as in above discussion), and sometimes this causes confusion.
Therefore, we note it is necessary to understand what “<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>” represents in the context.</p>
<p class="last">The other possible confusion is about the “samples”. It is possible both the data entries and feature vectors are referred to as samples in different contexts.
We again <span class="emp">note</span> <span class="red">sample covariance is always w.r.t. the feature vectors, not data entries</span>,
because it studies how the quantities of different features vary with each other.
Therefore, the “samples” in <a class="reference internal" href="#equation-eq-sample-cov">Eq.1.6</a> refers to feature vectors.</p>
</div>
<ul style="margin-left:20px">
<p><li style="margin-top:10px"></p>
<div class="section" id="property-1-4-classic-representation-of-covariance-by-expectation-or-mean">
<span id="property-cov-to-expectation"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Property 1-4.</span> <span class="bemp">Classic representation of covariance by expectation (or mean).</span><a class="headerlink" href="#property-1-4-classic-representation-of-covariance-by-expectation-or-mean" title="Permalink to this headline">¶</a></h3>Using the fact that <span class="math notranslate nohighlight">\(\colorbox{fact}{$\operatorname{cov}\left( X,Y \right)\mathbb{= E}XY - \mathbb{E}X\mathbb{E}Y$}\)</span> for scalar RVs <span class="math notranslate nohighlight">\(X,Y\)</span>, the covariance matrix has another form
<div class="math notranslate nohighlight">
\[\begin{split}\operatorname{\Sigma}\left( X \right) = \begin{pmatrix}
\mathbb{E}X_{1}^{2} - \mathbb{E}^{2}X_{1} &amp; \cdots &amp; \mathbb{E}{X_{1}X_{M}}\mathbb{- E}X_{1}\mathbb{E}X_{M} \\
\vdots &amp; \ddots &amp; \vdots \\
\mathbb{E}{X_{M}X_{1}}\mathbb{- E}X_{M}\mathbb{E}X_{1} &amp; \cdots &amp; \mathbb{E}X_{M}^{2} - \mathbb{E}^{2}X_{M} \\
\end{pmatrix} = \colorbox{rlt}{$\mathbb{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}X\mathbb{E}^{\mathrm{T}}X$}\end{split}\]</div>
<p>For two samples <span class="math notranslate nohighlight">\(\mathbf{x}\sim X,\mathbf{y}\sim Y\)</span> where
<span class="math notranslate nohighlight">\(\mathbf{x =}\left( x_{1}\mathbf{,\ldots,}x_{N} \right)\mathbf{,}\mathbf{y = (}y_{1}\mathbf{,\ldots,}y_{N}\mathbf{)}\)</span>,
we have</p>
<div class="math notranslate nohighlight">
\[\left( \mathbf{x} - \overline{\mathbf{x}} \right)^{\mathrm{T}}\left( \mathbf{y} - \overline{\mathbf{y}} \right) = \mathbf{x}^{\mathrm{T}}\mathbf{y} - \mathbf{x}^{\mathrm{T}}\overline{\mathbf{y}} - {\overline{\mathbf{x}}}^{\mathrm{T}}\mathbf{y} + {\overline{\mathbf{x}}}^{\mathrm{T}}\overline{\mathbf{y}}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\overline{x} = \frac{\sum_{i = 1}^{N}{\mathbf{x(}i\mathbf{)}}}{N}\)</span>
and <span class="math notranslate nohighlight">\(\overline{y} = \frac{\sum_{i = 1}^{N}{\mathbf{y(}i\mathbf{)}}}{N}\)</span>,
then</p>
<div class="math notranslate nohighlight">
\[{\mathbf{x}^{\mathrm{T}}\overline{\mathbf{y}} = \sum_{i = 1}^{N}{\overline{y}\mathbf{x(}i\mathbf{)}} = \overline{y}\sum_{i = 1}^{N}{\mathbf{x(}i\mathbf{)}} = N\overline{x}\overline{y}}\]</div>
<div class="math notranslate nohighlight">
\[{{\overline{\mathbf{x}}}^{\mathrm{T}}\mathbf{y} = \sum_{i = 1}^{N}{\overline{x}\mathbf{y(}i\mathbf{)}} = \overline{x}\sum_{i = 1}^{N}{\mathbf{y(}i\mathbf{)}} = N\overline{x}\overline{y}}\]</div>
<div class="math notranslate nohighlight">
\[{{\overline{\mathbf{x}}}^{\mathrm{T}}\overline{\mathbf{y}} = \sum_{i = 1}^{N}{\overline{x}\overline{y}} = N\overline{x}\overline{y}}\]</div>
<p>Thus</p>
<div class="math notranslate nohighlight">
\[\left( \mathbf{x} - \overline{\mathbf{x}} \right)^{\mathrm{T}}\left( \mathbf{y} - \overline{\mathbf{y}} \right) = \mathbf{x}^{\mathrm{T}}\mathbf{x +}N\overline{x}\overline{y} - 2N\overline{x}\overline{y} = \mathbf{x}^{\mathrm{T}}\mathbf{x -}N\overline{x}\overline{y} = \mathbf{x}^{\mathrm{T}}\mathbf{y -}{\overline{\mathbf{x}}}^{\mathrm{T}}\overline{\mathbf{y}}\]</div>
<p>which implies</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Sigma(\mathbf{X}) = \frac{1}{N - 1}\begin{pmatrix}
\mathbf{x}_{1}^{\mathrm{T}}\mathbf{x}_{1}{\bf -}{\overline{\mathbf{x}_{1}}}^{\mathrm{T}}\overline{\mathbf{x}_{1}} &amp; \cdots &amp; \mathbf{x}_{1}^{\mathrm{T}}\mathbf{x}_{M}{\bf -}{\overline{\mathbf{x}_{1}}}^{\mathrm{T}}\overline{\mathbf{x}_{M}} \\
\vdots &amp; \ddots &amp; \vdots \\
\mathbf{x}_{M}^{\mathrm{T}}\mathbf{x}_{1}{\bf -}{\overline{\mathbf{x}_{M}}}^{\mathrm{T}}\overline{\mathbf{x}_{1}} &amp; \cdots &amp; \mathbf{x}_{M}^{\mathrm{T}}\mathbf{x}_{M}{\bf -}{\overline{\mathbf{x}_{M}}}^{\mathrm{T}}\overline{\mathbf{x}_{M}} \\
\end{pmatrix} = \colorbox{rlt}{$\frac{1}{N - 1}\left( \mathbf{X}\mathbf{X}^{\mathrm{T}}{\bf -}\overline{\mathbf{X}}{\overline{\mathbf{X}}}^{\mathrm{T}} \right)$}\end{split}\]</div>
<p>We can verify above inference directly works for cross-covariance, and therefore</p>
<div class="math notranslate nohighlight">
\[\colorbox{result}{$\operatorname{\Sigma}\left( X,Y \right) = \mathbb{E}\mathrm{\lbrack}XY^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}X\mathbb{E}^{\rm{T}}Y,\Sigma\left( \mathbf{X,Y} \right) = \frac{1}{N - 1}\left( \mathbf{X}\mathbf{Y}^{\rm{T}}\mathbf{-}\overline{\mathbf{X}}{\overline{\mathbf{Y}}}^{\rm{T}} \right)$}\]</div>
</li>
<p><li style="margin-top:10px"></p>
</div>
<div class="section" id="property-1-5-invariance-to-centralization">
<span id="property-cov-invariant-to-centralization"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Property 1-5.</span> <span class="bemp">Invariance to centralization.</span><a class="headerlink" href="#property-1-5-invariance-to-centralization" title="Permalink to this headline">¶</a></h3>For any random vector <span class="math notranslate nohighlight">\(X\)</span>, we have <span class="math notranslate nohighlight">\(\colorbox{result}{$\Sigma\left( X - \mathbb{E}X \right) = \Sigma(X)$}\)</span>,
since <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack = \mathbf{0}\)</span> and
<div class="math notranslate nohighlight">
\[\Sigma\left( X - \mathbb{E}X \right)
= \mathbb{E}{\lbrack{\left( X - \mathbb{E}X - \mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack \right)\left( X - \mathbb{E}X - \mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack \right)}^{\mathrm{T}}\rbrack}
= \mathbb{E}{\lbrack{\left( X - \mathbb{E}X \right)\left( X - \mathbb{E}X \right)}^{\mathrm{T}}\rbrack} = \Sigma(X)\]</div>
<p>Similarly <span class="math notranslate nohighlight">\(\colorbox{result}{$\Sigma\left( \mathbf{X} - \overline{\mathbf{X}} \right) = \Sigma\left( \mathbf{X} \right)$}\)</span>,
because <span class="math notranslate nohighlight">\(\mathbf{x} - \overline{\mathbf{x}} - \overline{\mathbf{x} - \overline{\mathbf{x}}} = \mathbf{x} - \overline{\mathbf{x}}\)</span>
for any sample <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, and the result following by applying this on <a class="reference internal" href="#equation-eq-sample-cov">Eq.1.6</a>. For cross-covariance matrix, we have
<span class="math notranslate nohighlight">\(\colorbox{result}{$\Sigma\left( X - \mathbb{E}X,Y - \mathbb{E}Y \right) = \Sigma\left( X,Y \right)$}\)</span>
and <span class="math notranslate nohighlight">\(\colorbox{result}{$\Sigma\left( \mathbf{X} - \overline{\mathbf{X}},\mathbf{Y} - \overline{\mathbf{Y}} \right) = \Sigma\left( \mathbf{X,}\mathbf{Y} \right)$}\)</span>
for exactly the same reason.</p>
</li>
<p><li style="margin-top:10px"></p>
</div>
<div class="section" id="theorem-1-5-matrix-arithmetics-of-covariance-matrix">
<span id="theorem-cov-matrix-arithmetic-rules"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-5.</span> <span class="bemp">Matrix arithmetics of covariance matrix.</span><a class="headerlink" href="#theorem-1-5-matrix-arithmetics-of-covariance-matrix" title="Permalink to this headline">¶</a></h3><span><span class="theorem-highlight"> Given <span class="math notranslate nohighlight">\(X = \left( X_{1},\ldots,X_{n} \right)^{\mathrm{T}}\)</span>,
<span class="math notranslate nohighlight">\(\operatorname{var}\left( \mathbf{α}^{\rm{T}}X \right) = \operatorname{var}\left( X^{\rm{T}}\mathbf{α} \right) = \mathbf{α}^{\rm{T}}\operatorname{\Sigma}\left( X \right)\mathbf{α}\)</span> </span></span>.
<span class="emp">Note</span> <span class="math notranslate nohighlight">\(α^{\rm{T}}\Sigma{X}\)</span> is a scalar random variable, and
<div class="math notranslate nohighlight">
\[\mathbb{E}\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)\mathbb{= E}\left( X^{\mathrm{T}}\mathrm{\mathbf{α}} \right) = \mathrm{\mathbf{α}}^{\mathrm{T}}\mathbb{E}X = \left( \mathbb{E}^{\mathrm{T}}X \right)\mathrm{\mathbf{α}}\]</div>
<p>Also <span class="math notranslate nohighlight">\(\mathrm{\mathbf{α}}^{\mathrm{T}}X\)</span> is a scalar RV, and so
<span class="math notranslate nohighlight">\(\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)^{2}={\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)}^{\mathrm{T}} = \mathrm{\mathbf{α}}^{\mathrm{T}}XX^{\mathrm{T}}\mathrm{\mathbf{α}}\)</span>.
Recall <span class="math notranslate nohighlight">\(\operatorname{var} \left( X \right)=\mathbb{E}X^{2} - \mathbb{E}^{2}X\)</span>, then using <a class="reference internal" href="#property-cov-to-expectation"><span class="std std-ref">Property 1-4</span></a>, we have</p>
<div class="math notranslate nohighlight">
\[\operatorname{var} \left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right) = \mathbb{E}\mathrm{\lbrack}\mathrm{\mathbf{α}}^{\mathrm{T}}X\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}\left\lbrack \mathrm{\mathbf{α}}^{\mathrm{T}}X \right\rbrack\mathbb{E}^{\mathrm{T}}\left\lbrack \mathrm{\mathbf{α}}^{\mathrm{T}}X \right\rbrack = \mathrm{\mathbf{α}}^{\mathrm{T}}\mathbb{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack}\mathrm{\mathbf{α}} - \mathrm{\mathbf{α}}^{\mathrm{T}}\mathbb{E}X\mathbb{E}^{\mathrm{T}}X\mathrm{\mathbf{α}}=\mathrm{\mathbf{α}}^{\mathrm{T}}\mathbf{(}\mathbb{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}X\mathbb{E}^{\mathrm{T}}X\mathrm{)}\mathrm{\mathbf{α}}=\mathrm{\mathbf{α}}^{\mathrm{T}}\operatorname{\Sigma}\left( X \right)\mathrm{\mathbf{α}}\]</div>
<p>Similarly, using
<span class="math notranslate nohighlight">\(\operatorname{cov} \left( X,Y \right)\mathbb{= E}XY - \mathbb{E}X\mathbb{E}Y\)</span>, we have
<span class="math notranslate nohighlight">\(\colorbox{theorem}{$\operatorname{cov} \left( \mathrm{\mathbf{α}}^{\mathrm{T}}X,\mathbf{β}^\mathrm{T}Y \right) = \mathrm{\mathbf{α}}^\mathrm{T}\operatorname{\Sigma}\left( X \right)\mathbf{β}$}\)</span>.
Further, if we let
<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{= (}\mathbf{a}_{1}\mathbf{,\ldots,}\mathbf{a}_{n}\mathbf{)}\)</span>,
then
<span class="math notranslate nohighlight">\(\colorbox{theorem}{$\Sigma\left( \mathbf{A}^\mathrm{T}X \right) = \mathbf{A}^\mathrm{T}\operatorname{\Sigma}\left( X \right)\mathbf{A}$}\)</span>,
since
<span class="math notranslate nohighlight">\(\Sigma\left( \mathrm{\mathbf{α}}_{i}X\mathbf{,}\mathrm{\mathbf{α}}_{j}X \right) = \mathrm{\mathbf{α}}_{i}^\mathrm{T}\operatorname{\Sigma}\left( X \right)\mathrm{\mathbf{α}}_{j}\)</span>; for the same reason, we have
<span class="math notranslate nohighlight">\(\Sigma\left( \mathbf{A}^{\rm{T}}X,\mathbf{B}^{\rm{T}}Y \right) = \mathbf{A}^{\rm{T}}\operatorname{\Sigma}\left( X \right)\mathbf{B}\)</span>
for cross-covariance.</p>
<p>On the other hand, given
<span class="math notranslate nohighlight">\(\mathbf{X}=(\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\mathbf{)}\)</span>, we have that
<span class="math notranslate nohighlight">\(\colorbox{theorem}{$\operatorname{var}\left( \mathbf{α}^{\rm{T}}\mathbf{X} \right) = \operatorname{var}\left( \mathbf{X}\mathbf{α} \right) = \mathbf{α}^{\rm{T}}\operatorname{\Sigma}\left( \mathbf{X} \right)\mathbf{α}$}\)</span>.
First check</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\{ \begin{matrix}
\overline{\mathbf{\text{Xα}}} = \overline{\sum_{i = 1}^{n}{\mathrm{\mathbf{α}}\left( i \right)\mathbf{x}_{i}}} = \frac{1}{m}\sum_{j = 1}^{m}{\sum_{i = 1}^{n}{\mathrm{\mathbf{α}}\left( i \right)\mathbf{x}_{i}(j)}} \\
\overline{\mathbf{X}}\mathrm{\mathbf{α}}=\sum_{i = 1}^{n}{\mathrm{\mathbf{α}}\left( i \right)\overline{\mathbf{x}_{i}}} = \sum_{i = 1}^{n}\left( \mathrm{\mathbf{α}}\left( i \right) \times \frac{1}{m}\sum_{j = 1}^{m}{\mathbf{x}_{i}\left( j \right)} \right) = \frac{1}{m}\sum_{i = 1}^{n}\left( \sum_{j = 1}^{m}{\mathrm{\mathbf{α}}\left( i \right)\mathbf{x}_{i}\left( j \right)} \right) \\
\end{matrix} \Rightarrow \colorbox{rlt}{$\overline{\mathbf{\text{Xα}}} = \overline{\mathbf{X}}\mathrm{\mathbf{α}}$} \right.\end{split}\]</div>
<p>Then we have</p>
<div class="math notranslate nohighlight">
\[\operatorname{var} \left( \mathbf{\text{Xα}} \right) = \frac{1}{n + 1}\left( \left( \mathbf{\text{Xα}} \right)\mathrm{T}\left( \mathbf{\text{Xα}} \right)\mathbf{-}{\overline{\mathbf{\text{Xα}}}}^\mathrm{T}\overline{\mathbf{\text{Xα}}} \right) = \frac{1}{n + 1}\left( \mathrm{\mathbf{α}}^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{Xα -}\mathrm{\mathbf{α}}^\mathrm{T}{\overline{\mathbf{X}}}^\mathrm{T}\overline{\mathbf{X}}\mathrm{\mathbf{α}} \right) = \mathrm{\mathbf{α}}^\mathrm{T}\Sigma\left( \mathbf{X} \right)\mathrm{\mathbf{α}}\]</div>
<p>By similar calculation,
<span class="math notranslate nohighlight">\(\operatorname{cov} \left( \mathbf{Xα,Xβ} \right) = \mathrm{\mathbf{α}}^\mathrm{T}\Sigma\left( \mathbf{X} \right)\mathbf{β}\)</span>.
Let <span class="math notranslate nohighlight">\(\mathbf{Y} = \mathbf{\text{XA}}\)</span> for any matrix
<span class="math notranslate nohighlight">\(\mathbf{A} = (\mathbf{a}_{1}\mathbf{,\ldots,}\mathbf{a}_{n}\mathbf{)}\)</span>,
then
<span class="math notranslate nohighlight">\(\colorbox{theorem}{$\Sigma(\mathbf{\text{A}}^{\rm{T}}\mathbf{\mathrm{X}}) = \mathbf{A}^\mathrm{T}\Sigma(\mathbf{X})\mathbf{A}$}\)</span>,
since <span class="math notranslate nohighlight">\(\ \Sigma\left( \mathbf{a}_{i}^{\rm{T}}\mathbf{X}\mathbf{,}\mathbf{a}_{j}^{\rm{T}}\mathbf{X} \right) = \mathbf{a}_{i}^{\rm{T}}\operatorname{\Sigma}\left( \mathbf{X} \right)\mathbf{a}_{j}\)</span>;
of course we also have <span><span class="theorem-highlight"> <span class="math notranslate nohighlight">\(\Sigma(\mathbf{A}^{\text{T}}\mathbf{X,}\mathbf{B}^{\rm{T}}\mathbf{Y}) = \mathbf{A}^{\rm{T}}\Sigma(\mathbf{X,Y})\mathbf{B}\)</span> for cross-covariance </span></span>.</p>
<p>For the same reason, <span><span class="theorem-highlight"> a square cross-covariance matrix is positive-semidefinite</span></span>.</p>
</li>
<p><li style="margin-top:10px"></p>
</div>
<div class="section" id="theorem-1-6-positive-definiteness-of-covariance-matrix">
<span id="theorem-cov-semipositiveness"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-6.</span> <span class="bemp">Positive definiteness of covariance matrix.</span><a class="headerlink" href="#theorem-1-6-positive-definiteness-of-covariance-matrix" title="Permalink to this headline">¶</a></h3><span><span class="theorem-highlight"> Covariance matrix is clearly symmetric, and moreover they are semi-positive definite</span></span>, since for any constant vector <span class="math notranslate nohighlight">\(\mathbf{α}\)</span>, using <a class="reference internal" href="#theorem-cov-matrix-arithmetic-rules"><span class="std std-ref">Theorem 1-5</span></a>, we have
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
{{\mathbf{α }}^{\text{T}}}\left( {\Sigma \left( X \right)} \right){\mathbf{α }} &amp;= \Sigma \left( {{{\mathbf{α }}^{\text{T}}}X} \right) \hfill \\
&amp;= \mathbb{E}\left[ {\left( {{{\mathbf{α }}^{\text{T}}}X - \mathbb{E}{{\mathbf{α }}^{\text{T}}}X} \right){{\left( {{{\mathbf{α }}^{\text{T}}}X - \mathbb{E}{{\mathbf{α }}^{\text{T}}}X} \right)}^{\text{T}}}} \right] = \mathbb{E}\left[ {\left( {{{\mathbf{α }}^{\text{T}}}X - {{\mathbf{α }}^{\text{T}}}\mathbb{E}X} \right){{\left( {{{\mathbf{α }}^{\text{T}}}X - {{\mathbf{α }}^{\text{T}}}\mathbb{E}X} \right)}^{\text{T}}}} \right] \hfill \\
&amp;= \mathbb{E}\left[ {{{\mathbf{α }}^{\text{T}}}\left( {X - \mathbb{E}X} \right){{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}{\mathbf{α }}} \right] = \mathbb{E}\left[ {{{\left( {{{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}{\mathbf{α }}} \right)}^{\text{T}}}\left( {{{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}{\mathbf{α }}} \right)} \right] \geqslant 0 \hfill \\
\end{aligned}\end{split}\]</div>
<p>For sample covariance matrix, check that (omiting the coefficient)</p>
<div class="math notranslate nohighlight">
\[\mathbf{α}^{\rm{T}}\left( \Sigma\left( \mathbf{X} \right) \right)\mathbf{α}
=\Sigma\left( \mathbf{\text{Xα}} \right) \propto \left( \mathbf{\text{Xα}} - \overline{\mathbf{\text{Xα}}} \right)^{\rm{T}}\left( \mathbf{\text{Xα}} - \overline{\mathbf{\text{Xα}}} \right) \geq 0\]</div>
</li>
<p><li style="margin-top:10px"></p>
</div>
<div class="section" id="property-1-6-sample-covariance-represneted-by-rank-1-sum">
<span id="property-cov-as-rank1-sum"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Property 1-6.</span> <span class="bemp">Sample covariance represneted by rank-1 sum.</span><a class="headerlink" href="#property-1-6-sample-covariance-represneted-by-rank-1-sum" title="Permalink to this headline">¶</a></h3>Recall <span class="math notranslate nohighlight">\(\mathbf{X} = \begin{pmatrix} \mathbf{x}_{1}^{\rm{T}} \\  \vdots \\ \mathbf{x}_{M}^{\rm{T}} \\ \end{pmatrix}\)</span> are feature vectors, and the covariance matrix in <a class="reference internal" href="#equation-eq-sample-cov">Eq.1.6</a> is defined w.r.t. the feature vectors. Suppose
<span class="math notranslate nohighlight">\(\mathbf{X} = \left( \mathbf{𝓍}_{1},\ldots,\mathbf{𝓍}_{N} \right)\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{𝓍}_{1},\ldots,\mathbf{𝓍}_{N}\)</span> are columns of
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> as data entries, and similarly <span class="math notranslate nohighlight">\(\mathbf{Y} = \left( \mathbf{𝓎}_{1},\ldots,\mathbf{𝓎}_{N} \right)\)</span>
and let <span class="math notranslate nohighlight">\(\overline{\mathbf{𝓍}} = \frac{1}{N}\sum_{i = 1}^{N}\mathbf{𝓍}_{i}\)</span> and <span class="math notranslate nohighlight">\(\overline{\mathbf{𝓎}} = \frac{1}{N}\sum_{i = 1}^{N}\mathbf{𝓎}_{j}\)</span> be the mean vector of all data entries. Then we can show
<span class="math notranslate nohighlight">\(\Sigma\left( \mathbf{X} \right)\)</span> or <span class="math notranslate nohighlight">\(\Sigma\left( \mathbf{X,Y} \right)\)</span> can also be represented by
sum of rank-1 addends dependent on the data entries (rather than the feature vectors) as
<div class="math notranslate nohighlight" id="equation-eq-cov-rank1-data-entry-representation">
<span class="eqno">(1.7)<a class="headerlink" href="#equation-eq-cov-rank1-data-entry-representation" title="Permalink to this equation">¶</a></span>\[\Sigma\left( \mathbf{X,Y} \right) = \frac{1}{N - 1}\sum_{k = 1}^{N}{\left( \mathbf{𝓍}_{k} - \overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓎}_{k} - \overline{\mathbf{𝓎}} \right)^{\rm{T}}}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{\Sigma} := \Sigma\left( \mathbf{X},\mathbf{Y} \right)\)</span> for convenience.
By <a class="reference internal" href="#equation-eq-sample-cov">Eq.1.6</a>, we have (omitting the coefficient)</p>
<div class="math notranslate nohighlight">
\[\mathbf{\Sigma}\left( i,j \right) \propto \left( \mathbf{x}_{i} - \overline{\mathbf{x}_{i}} \right)^{\rm{T}}\left( \mathbf{y}_{j} - \overline{\mathbf{y}_{j}} \right)\]</div>
<p>Note <span class="math notranslate nohighlight">\(\mathbf{𝓍}_{j}\left( i \right) = x_{i,j} = \mathbf{x}_{i}\left( j \right)\)</span>
and <span class="math notranslate nohighlight">\(\overline{\mathbf{𝓍}}\left( i \right)\mathbf{=}\overline{x_{i}} = \frac{1}{N}\sum_{k = 1}^{N}x_{i,k}\)</span>, <span class="math notranslate nohighlight">\(\overline{\mathbf{𝓎}}\left( j \right)\mathbf{=}\overline{y_{j}} = \frac{1}{N}\sum_{j = 1}^{N}y_{j,k}\)</span>,
we have</p>
<div class="math notranslate nohighlight" id="equation-eq-cov-elementwise-representation-by-data-entries">
<span class="eqno">(1.8)<a class="headerlink" href="#equation-eq-cov-elementwise-representation-by-data-entries" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
&amp; \mathbf{\Sigma}\left( i,j \right) \propto \left( \mathbf{x}_{i} - \overline{\mathbf{x}_{i}} \right)^{\rm{T}}\left( \mathbf{y}_{j} - \overline{\mathbf{y}_{j}} \right)
= \sum_{k = 1}^{N}{\left( \mathbf{x}_{i}\left( k \right) - \overline{x_{i}} \right)\left( \mathbf{y}_{j}\left( k \right) - \overline{y_{j}} \right)} \\
&amp;= \sum_{k = 1}^{N}{\left( \mathbf{𝓍}_{k}\left( i \right) - \overline{x_{i}} \right)\left( \mathbf{𝓎}_{k}\left( j \right) - \overline{y_{j}} \right)}
= \sum_{k = 1}^{N}{\left( \mathbf{𝓍}_{k}\left( i \right) - \overline{\mathbf{𝓍}}\left( i \right) \right)\left( \mathbf{𝓎}_{k}\left( j \right) - \overline{\mathbf{𝓎}}\left( j \right) \right)} \\
&amp;= \sum_{k = 1}^{N}{\left( \left( \mathbf{𝓍}_{k} - \overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓎}_{k} - \overline{\mathbf{𝓎}} \right)^{\rm{T}} \right)\left( i,j \right)} \end{aligned}\end{split}\]</div>
<p>The above identity immediately implies <a class="reference internal" href="#equation-eq-cov-rank1-data-entry-representation">Eq.1.7</a>.</p>
<p id="corollary-cov-as-mixed"><span class="ititle2">Corollary 1-4.</span> <span class="math notranslate nohighlight">\(\colorbox{result}{$\mathbf{X}\mathbf{X}^{\rm{T}} = \left( N - 1 \right)\Sigma\left( \mathbf{X} \right) + N\overline{\mathbf{𝓍}}{\overline{\mathbf{𝓍}}}^{\rm{T}}$}\)</span>.
Check that <span class="math notranslate nohighlight">\(\mathbf{\text{XX}}^{\rm{T}} = \begin{pmatrix} \mathbf{x}_{1}^{\rm{T}}\mathbf{}_{1} &amp; \cdots &amp; \mathbf{x}_{1}^{\rm{T}}\mathbf{x}_{M} \\  \vdots &amp; \ddots &amp; \vdots \\ \mathbf{x}_{M}^{\rm{T}}\mathbf{x}_{1} &amp; \cdots &amp; \mathbf{x}_{M}^{  T}\mathbf{x}_{M} \\ \end{pmatrix}\)</span>, and by the same inference as <a class="reference internal" href="#equation-eq-cov-elementwise-representation-by-data-entries">Eq.1.8</a>
we have <span class="tooltip"> <span class="math notranslate nohighlight">\(\mathbf{\text{XX}}^{\rm{T}} = \sum_{k = 1}^{N}{\mathbf{𝓍}_{k}{\mathbf{𝓍}_{k}}^{\rm{T}}}\)</span> <span class="tooltiptext"> This is the rank-1 decomposition of any symmetric matrix in linear algebra.</span></span>,
and then use <a class="reference internal" href="#equation-eq-cov-rank1-data-entry-representation">Eq.1.7</a>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned} \mathbf{\text{XX}}^{\rm{T}} &amp;= \sum_{k = 1}^{N}{\mathbf{𝓍}_{k}{\mathbf{𝓍}_{k}}^{\rm{T}}} \\
&amp;= \sum_{k = 1}^{N}{\left( \mathbf{𝓍}_{k} - \overline{\mathbf{𝓍}} \right)({\mathbf{𝓍}_{k} - \overline{\mathbf{𝓍}})}^{\rm{T}}} + \sum_{k = 1}^{N}\left( \overline{\mathbf{𝓍}}\mathbf{𝓍}_{k}^{\rm{T}} + \mathbf{𝓍}_{k}{\overline{\mathbf{𝓍}}}^{\rm{T}} - \overline{\mathbf{𝓍}}{\overline{\mathbf{𝓍}}}^{\rm{T}} \right) \\
&amp;= \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)\mathbf{+}\overline{\mathbf{𝓍}}\left( \sum_{k = 1}^{N}\mathbf{𝓍}_{k}^{\rm{T}} \right) + \left( \sum_{k = 1}^{N}\mathbf{𝓍}_{k} \right){\overline{\mathbf{𝓍}}}^{\rm{T}} - N\overline{\mathbf{𝓍}}{\overline{\mathbf{𝓍}}}^{\rm{T}} \\
&amp;= \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)\mathbf{+}N\overline{\mathbf{𝓍}}{\overline{\mathbf{𝓍}}}^{\rm{T}} + N\overline{\mathbf{𝓍}}{\overline{\mathbf{𝓍}}}^{\rm{T}} - N\overline{\mathbf{𝓍}}{\overline{\mathbf{𝓍}}}^{\rm{T}} \\
&amp;= \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)\mathbf{+}N\overline{\mathbf{𝓍}} {\overline{\mathbf{𝓍}}}^{\rm{T}} \end{aligned}\end{split}\]</div>
</li>
<p><li style="margin-top:10px"></p>
</div>
<div class="section" id="theorem-1-7-block-decomposition-of-covariance-matrix">
<span id="theorem-cov-as-rank1-sum"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-7.</span> <span class="bemp">Block decomposition of covariance matrix.</span><a class="headerlink" href="#theorem-1-7-block-decomposition-of-covariance-matrix" title="Permalink to this headline">¶</a></h3>Again consider <span class="math notranslate nohighlight">\(\mathbf{X} = \left( \mathbf{𝓍}_{1},\ldots,\mathbf{𝓍}_{N} \right)\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{𝓍}_{1},\ldots,\mathbf{𝓍}_{N}\)</span> are data entries, and
<span class="math notranslate nohighlight">\(\overline{\mathbf{𝓍}} = \frac{1}{N}\sum_{i = 1}^{N}\mathbf{𝓍}_{i}\)</span>.
Suppose <span class="math notranslate nohighlight">\(\mathbf{𝓍}_{1},\ldots,\mathbf{𝓍}_{N}\)</span> are categorized into
<span class="math notranslate nohighlight">\(K\)</span> non-overlapping groups <span class="math notranslate nohighlight">\(G_{1},\ldots,G_{k}\)</span>. Let
<span class="math notranslate nohighlight">\(N_{1},\ldots,N_{k}\)</span> be the size of the groups, and
<span class="math notranslate nohighlight">\({\overline{\mathbf{𝓍}}}^{k} = \frac{1}{N_{k}}\sum_{\mathbf{𝓍} \in G_{k}}^{}\mathbf{𝓍}\)</span>, then
<div class="math notranslate nohighlight" id="equation-eq-cov-block-decomposition">
<span class="eqno">(1.9)<a class="headerlink" href="#equation-eq-cov-block-decomposition" title="Permalink to this equation">¶</a></span>\[\colorbox{theorem}{$\left( N - 1 \right)\Sigma\left( \mathbf{X} \right)
= \color{conn1}{\sum_{k = 1}^{K}{\left( N_{k} - 1 \right)\Sigma\left( \mathbf{X}_{k} \right)}}
+ \color{conn2}{\sum_{k = 1}^{K}{N_{k}\left( {\overline{\mathbf{𝓍}}}^{k} - \overline{\mathbf{𝓍}} \right)\left( {\overline{\mathbf{𝓍}}}^{k} - \overline{\mathbf{𝓍}} \right)^{\rm{T}}}}$}\]</div>
<p>This is because</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\left( N - 1 \right)\Sigma\left( \mathbf{X} \right)
&amp;= \sum_{j = 1}^{N}{\left( \mathbf{𝓍}_{j} - \overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓍}_{j} - \overline{\mathbf{𝓍}} \right)^{\rm{T}}} \\
&amp;= \sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( \mathbf{𝓍} - \overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓍} - \overline{\mathbf{𝓍}} \right)^{\rm{T}}}} \\
&amp;= \sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k}\mathbf{+}{\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k}\mathbf{+}{\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}}}} \\
&amp;= \sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)^{\rm{T}}}} + \sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}}}} + \sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}}}} + \sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)^{\rm{T}}}}
\end{aligned}\end{split}\]</div>
<p>where we have</p>
<div class="math notranslate nohighlight">
\[\sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)^{\rm{T}}}}
= \color{conn1}{\sum_{k = 1}^{K}{\left( N_{k} - 1 \right)\Sigma\left( \mathbf{X}_{k} \right)}}\]</div>
<div class="math notranslate nohighlight">
\[\sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}}}}
= \color{conn2}{\sum_{k = 1}^{K}{N_{k}\left( {\overline{\mathbf{𝓍}}}^{k} - \overline{\mathbf{𝓍}} \right)\left( {\overline{\mathbf{𝓍}}}^{k} - \overline{\mathbf{𝓍}} \right)^{\rm{T}}}}\]</div>
<p>The other two summations are zero matrices. For example,</p>
<div class="math notranslate nohighlight">
\[\sum_{k = 1}^{K}{\sum_{\mathbf{𝓍} \in G_{k}}^{}{\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right)\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}}}}
= \sum_{k = 1}^{K}\left( \left( \sum_{\mathbf{𝓍} \in G_{k}}^{}\left( \mathbf{𝓍} - {\overline{\mathbf{𝓍}}}^{k} \right) \right)\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}} \right)
= \sum_{k = 1}^{K}{\left( N_{k}{\overline{\mathbf{𝓍}}}^{k}\mathbf{-}N_{k}{\overline{\mathbf{𝓍}}}^{k} \right)\left( {\overline{\mathbf{𝓍}}}^{k}\mathbf{-}\overline{\mathbf{𝓍}} \right)^{\rm{T}}}
= \mathbf{O}\]</div>
<p>Now the identity of <a class="reference internal" href="#equation-eq-cov-block-decomposition">Eq.1.9</a> is obvious.</p>
</li></ul>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["color.js","autoload-all.js"] }
  });

      MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
   var color = MathJax.Extension["TeX/color"];
   color.colors["theorem"] = color.getColor('RGB','255,229,153');
       color.colors["result"] = color.getColor('RGB','189,214,238');
       color.colors["fact"] = color.getColor('RGB','255,255,204');
       color.colors["emperical"] = color.getColor('RGB','253,240,207');
       color.colors["comment"] = color.getColor('RGB','204,255,204');
   color.colors["thm"] = color.getColor('RGB','255,229,153');
       color.colors["rlt"] = color.getColor('RGB','189,214,238');
       color.colors["emp"] = color.getColor('RGB','253,240,207');
       color.colors["comm"] = color.getColor('RGB','204,255,204');
       color.colors["conn1"] = color.getColor('RGB','255,0,255');
       color.colors["conn2"] = color.getColor('RGB','237,125,49');
       color.colors["conn3"] = color.getColor('RGB','112,48,160');
      });
</script></div>
</div>
<div class="section" id="multivariate-gaussian-distribution">
<h2>1.3. Multivariate Gaussian Distribution<a class="headerlink" href="#multivariate-gaussian-distribution" title="Permalink to this headline">¶</a></h2>
<p>An <span class="math notranslate nohighlight">\(m\)</span>-dimensional Multivariate Gaussian distribution is defined by an
<span class="math notranslate nohighlight">\(m\)</span>-dimensional mean <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span> and a <span class="math notranslate nohighlight">\(m \times m\)</span> non-singular
covariance <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>, denoted by
<span class="math notranslate nohighlight">\(\operatorname{Gaussian}\left( \mathbf{μ},\mathbf{\Sigma} \right)\)</span>,
whose density function can be written as</p>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-density">
<span class="eqno">(1.10)<a class="headerlink" href="#equation-eq-gaussian-density" title="Permalink to this equation">¶</a></span>\[p\left( \mathbf{x} \right)
= \frac{1}{\left( 2\pi \right)^{\frac{m}{2}}}\frac{1}{\left| \mathbf{\Sigma} \right|^{\frac{1}{2}}}\exp\left\{ - \frac{1}{2}\left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{\Sigma}^{- 1}\left( \mathbf{x} - \mathbf{μ} \right) \right\}
= \frac{1}{\left( 2\pi \right)^{\frac{m}{2}}}\frac{1}{\left| \mathbf{\Sigma} \right|^{\frac{1}{2}}}\kappa\left( \mathbf{x},\mathbf{μ} \right)
= \frac{1}{\left( 2\pi \right)^{\frac{m}{2}}}\frac{1}{\left| \mathbf{\Sigma} \right|^{\frac{1}{2}}}\exp\left\{ - \frac{1}{2}𝒹_{M}\left( \mathbf{x};\mathbf{μ,}\mathbf{\Sigma} \right) \right\}\]</div>
<p>where <span class="math notranslate nohighlight">\(\left| \mathbf{\Sigma} \right|\)</span> is the determinant of
<span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>, <span class="math notranslate nohighlight">\(\kappa\)</span> is the  <span><span class="exdef"> <span class="target" id="index-108"></span>Gaussian kernel</span></span>, and
<span class="math notranslate nohighlight">\(𝒹_{M}\left( \mathbf{x};\mathbf{μ,\Sigma} \right) = \left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{\Sigma}^{- 1}\left( \mathbf{x} - \mathbf{μ} \right)\)</span>
is called the  <span><span class="exdef"> <span class="target" id="index-109"></span>Mahalanobis distance</span></span>. <span class="math notranslate nohighlight">\(\kappa\)</span> and <span class="math notranslate nohighlight">\(𝒹_{M}\)</span> measure the similarity/distance
between an observation <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and a distribution with
mean <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span> and covariance <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span> (not necessarily
Gaussian). <span class="emp">Note</span> <span class="math notranslate nohighlight">\(𝒹_{M}\)</span> is reduced to
Euclidean distance from observation <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to mean <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span>
when the covariance is identity. Recall the basic fact that co-variance
matrix is symmetric and positive semidefinite, and in the case of
Gaussian <span class="tooltip"> it must be positive definite <span class="tooltiptext"> This is because <span class="math notranslate nohighlight">\(|Σ|≠0\)</span>; otherwise the Gaussian density of <a class="reference internal" href="#equation-eq-gaussian-density">Eq.1.10</a> would be invalid. Therefore, all eigenvalues of Σ must be positive, because the determinant equals to the product of all eigenvalues. </span></span>, thus
<span class="math notranslate nohighlight">\(𝒹_{M}\left( \mathbf{x};\mathbf{μ,\Sigma} \right) \geq 0\)</span>
and <span class="math notranslate nohighlight">\(𝒹_{M}\left( \mathbf{x};\mathbf{μ,\Sigma} \right) = 0\)</span>
iff <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{μ}\)</span>. The inverse of covariance matrix
<span class="math notranslate nohighlight">\(\mathbf{\Sigma}^{- 1}\)</span> is also named the  <span><span class="def"> <span class="target" id="index-110"></span>precision matrix</span></span>, denoted
by :math:<a href="#id3"><span class="problematic" id="id4">`</span></a>mathbf{Ⲗ}:=mathbf{Sigma}^{- 1}`, and the density can be written in terms of precision
matrix as</p>
<div class="math notranslate nohighlight">
\[p\left( \mathbf{x} \right) = \frac{\left| \mathbf{Ⲗ} \right|^{\frac{1}{2}}}{\left( 2\pi \right)^{\frac{m}{2}}}\exp\left\{ - \frac{1}{2}\left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{Ⲗ}\left( \mathbf{x} - \mathbf{μ} \right) \right\}\]</div>
<p>The <span class="emp">limitation</span> of Gaussian is <span><span class="comment-highlight"> its quadratic number of parameters which could be a problem for high-dimensional computation,
and its very limited unimodal shape which could not represent complicated real-world distributions.</span></span></p>
<ul style="margin-left:20px">
<p><li style="margin-top:10px"> We state useful gradient results from matrix derivatives, which are frequently used in finding analytical solution for optimization
problems. They are soon applied to prove the maximum likelihood of multivariate Gaussian.</p>
<p id="fact-gradient-affine-transform"><span class="ititle2">Fact 1-5.</span> Affine transformation
<span class="math notranslate nohighlight">\(\mathbf{a}^{\rm{T}}\mathbf{x} + b\mathbf{:}\mathbb{R}^{n}\mathbb{\rightarrow R}\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{a} \in \mathbb{R}^{n},b\mathbb{\in R}\)</span> has gradient
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\nabla\left( \mathbf{a}^{\rm{T}}\mathbf{x} + b \right) = \nabla\left( \mathbf{x}^{\rm{T}}\mathbf{a} + b \right) = \mathbf{a}$}\)</span>.</p>
<p id="fact-gradient-determinant"><span class="ititle2">Fact 1-6.</span> Determinant
<span class="math notranslate nohighlight">\(\left| \mathbf{X} \right|:\mathbb{R}^{n \times n}\mathbb{\rightarrow R}\)</span>
is a scalar-valued matrix function, and we have
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\nabla\log\left| \mathbf{X} \right| = \mathbf{X}^{-T}$}\)</span>.</p>
<p id="fact-gradient-matrix-of-matrix-product"><span class="ititle2">Fact 1-7.</span> The gradient of
<span class="math notranslate nohighlight">\(f\left( \mathbf{X} \right)=\mathbf{u}^{\rm{T}}\mathbf{\text{Xv}}:\mathbb{R}^{m \times n}\mathbb{\rightarrow R}\)</span>,
where <span class="math notranslate nohighlight">\(\mathbf{u} \in \mathbb{R}^{m},\mathbf{v} \in \mathbb{R}^{n}\)</span> are
constant vectors, is <span class="math notranslate nohighlight">\(\colorbox{fact}{$\nabla\mathbf{u}^{\rm{T}}\mathbf{\text{Xv}} \equiv \mathbf{u}\mathbf{v}^{\rm{T}}$}\)</span>.</p>
<p id="fact-gradient-vector-of-matrix-product"><span class="ititle2">Fact 1-8.</span> The gradient of <span class="math notranslate nohighlight">\(f\left( \mathbf{x} \right) = \mathbf{x}^{\rm{T}}\mathbf{\text{Ux}}:\mathbf{x} \in \mathbb{R}^{n}\mathbb{\rightarrow R}\)</span>,
where <span class="math notranslate nohighlight">\(\mathbf{U} \in \mathbb{R}^{n \times n}\)</span> is a constant matrix, is <span class="math notranslate nohighlight">\(\colorbox{fact}{$\nabla\mathbf{x}^{\rm{T}}\mathbf{\text{Ux}} = (\mathbf{U} + \mathbf{U}^{\rm{T}})\mathbf{x}$}\)</span>.</p>
<div class="section" id="theorem-1-8-gaussian-maximum-likelihood-estimators">
<span id="theorem-guassian-mle"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-8.</span> <span class="bemp">Gaussian maximum likelihood estimators.</span><a class="headerlink" href="#theorem-1-8-gaussian-maximum-likelihood-estimators" title="Permalink to this headline">¶</a></h3>Given a data matrix <span class="math notranslate nohighlight">\(\mathbf{X}=\left( \mathbf{x}_{1},\ldots,\mathbf{x}_{N} \right)\)</span>,
we can view its columns <span class="math notranslate nohighlight">\(\mathbf{x}_{1},\ldots,\mathbf{x}_{N}\)</span> as being
drawn i.i.d. drawn from a multivariate Gaussian distribution, then the
log-likelihood is
<div class="math notranslate nohighlight">
\[L \propto \sum_{i = 1}^{N}{- \frac{1}{2}\log\left| \mathbf{\Sigma} \right| - \frac{1}{2}\left( \mathbf{x}_{i} - \mathbf{μ} \right)^{\rm{T}}\mathbf{\Sigma}^{- 1}\left( \mathbf{x}_{i} - \mathbf{μ} \right)}
= \frac{N}{2}\log\left| \mathbf{\Sigma}^{- 1} \right| - \frac{1}{2}\sum_{i = 1}^{N}{\left( \mathbf{x}_{i} - \mathbf{μ} \right)^{\rm{T}}\mathbf{\Sigma}^{- 1}\left( \mathbf{x}_{i} - \mathbf{μ} \right)}\]</div>
<p>Using <a class="reference internal" href="#fact-gradient-determinant"><span class="std std-ref">Fact 1-6</span></a> and <a class="reference internal" href="#fact-gradient-matrix-of-matrix-product"><span class="std std-ref">Fact 1-7</span></a>, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-mle-inf">
<span class="eqno">(1.11)<a class="headerlink" href="#equation-eq-gaussian-mle-inf" title="Permalink to this equation">¶</a></span>\[\frac{\partial L}{\partial\mathbf{\Sigma}^{- 1}}
= \frac{N}{2}\mathbf{\Sigma}^{\rm{T}} - \frac{1}{2}\sum_{i = 1}^{N}{\left( \mathbf{x}_{i} - \mathbf{μ} \right)\left( \mathbf{x}_{i} - \mathbf{μ} \right)^{\rm{T}}}
= 0 \Rightarrow \mathbf{\Sigma}=\frac{1}{N}\sum_{i = 1}^{N}{\left( \mathbf{x}_{i} - \mathbf{μ} \right)\left( \mathbf{x}_{i} - \mathbf{μ} \right)^{\rm{T}}}\]</div>
<p>Then for <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span>, using <a class="reference internal" href="#fact-gradient-vector-of-matrix-product"><span class="std std-ref">Fact 1-8</span></a>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp; L \propto \sum_{i = 1}^{N}{\mathbf{x}_{i}^{\rm{T}}\mathbf{\Sigma}^{- 1}\mathbf{μ} - \frac{1}{2}\mathbf{μ}^{\rm{T}}\mathbf{\Sigma}^{- 1}\mathbf{μ}} \\
&amp; \Rightarrow \frac{\partial L}{\partial\mathbf{μ}} = \sum_{i = 1}^{N}{\mathbf{x}_{i}^{\rm{T}}\mathbf{\Sigma}^{- 1} - \mathbf{\Sigma}^{- 1}\mathbf{μ}} = \mathbf{0} \\
&amp; \Rightarrow \mathbf{\Sigma}^{- 1}\left( \sum_{i = 1}^{N}\mathbf{x}_{i} \right) = N\mathbf{\Sigma}^{- 1}\mathbf{μ} \Rightarrow \colorbox{thm}{$\mathbf{μ}_{\text{ML}}=\frac{\sum_{i = 1}^{N}\mathbf{x}_{i}}{N} = \overline{\mathbf{x}}$} \end{aligned}\end{split}\]</div>
<p>Plug back to <span class="math notranslate nohighlight">\(\mathbf{\Sigma}^{- 1}\)</span> in (1?€?7) and we have</p>
<div class="math notranslate nohighlight">
\[\mathbf{\Sigma}_{\text{ML}}=\frac{1}{N}\sum_{i = 1}^{N}{\left( \mathbf{x}_{i} - \overline{\mathbf{x}} \right)\left( \mathbf{x}_{i} - \overline{\mathbf{x}} \right)^{\rm{T}}}\]</div>
<p>By <a class="reference internal" href="#property-cov-as-rank1-sum"><span class="std std-ref">Property 1-6</span></a>, <span><span class="result-highlight"> <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{\text{ML}}\)</span> equals the biased sample covariance</span></span>, or <span><span class="result-highlight"> <span class="math notranslate nohighlight">\(\frac{N}{N - 1}\mathbf{\Sigma}_{\text{ML}}\)</span> equals the sample covariance matrix</span></span></p>
</li>
<p><li style="margin-top:10px"> Treating samples <span class="math notranslate nohighlight">\(\mathbf{x,y}\)</span> drawn from
<span class="math notranslate nohighlight">\(\operatorname{Gaussian}\left( \mathbf{μ},\mathbf{\Sigma} \right)\)</span> as
two RV vector where
<span class="math notranslate nohighlight">\(\mathbb{E}\mathbf{x} = \mathbb{E}\mathbf{y} = \mathbf{μ}\)</span>, recall we have <span class="math notranslate nohighlight">\(\mathbf{\Sigma =}\mathbb{E}\left\lbrack \mathbf{x}\mathbf{y}^{\rm{T}} \right\rbrack-\mathbf{μ}\mathbf{μ}^{\rm{T}}\)</span>
or <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack \mathbf{x}\mathbf{y}^{\rm{T}} \right\rbrack=\mathbf{μ}\mathbf{μ}^{\rm{T}}\mathbf{+ \Sigma}\)</span> by <a class="reference internal" href="#property-cov-to-expectation"><span class="std std-ref">Property 1-4</span></a>. When <span class="math notranslate nohighlight">\(\mathbf{x},\mathbf{y}\)</span> are independent, we have
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack \mathbf{x}\mathbf{y}^{\rm{T}} \right\rbrack=\mathbf{μ}\mathbf{μ}^{\rm{T}}\)</span>.</p>
</div>
<div class="section" id="theorem-1-9-bias-of-gaussian-mle">
<span id="theorem-guassian-mle-bias"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-9.</span> <span class="bemp">Bias of Gaussian MLE.</span><a class="headerlink" href="#theorem-1-9-bias-of-gaussian-mle" title="Permalink to this headline">¶</a></h3>Now given i.i.d. RVs <span class="math notranslate nohighlight">\(\mathbf{x}_{1},\ldots,\mathbf{x}_{N}\)</span> with mean <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span> and covariance <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>, note
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}\left\lbrack \mathbf{x}_{i}\mathbf{x}_{j}^{\rm{T}} \right\rbrack = \left\{ \begin{matrix}
\mathbf{μ}\mathbf{μ}^{\rm{T}} &amp; i \neq j \\
\mathbf{μ}\mathbf{μ}^{\rm{T}}\mathbf{+}\mathbf{\Sigma} &amp; i = j \\
\end{matrix} \right.\end{split}\]</div>
<p>Then we have</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left\lbrack \mathbf{x}_{i}{\overline{\mathbf{x}}}^{\rm{T}} \right\rbrack = \frac{1}{N}\sum_{i = 1}^{N}{\mathbb{E}\left\lbrack \mathbf{x}_{i}\mathbf{x}_{j}^{\rm{T}} \right\rbrack} = \frac{1}{N}\left( N\mathbf{μ}\mathbf{μ}^{\rm{T}} + \mathbf{\Sigma} \right) = \mathbf{μ}\mathbf{μ}^{\rm{T}} + \frac{\mathbf{\Sigma}}{N}\mathbb{= E}\left\lbrack \overline{\mathbf{x}}\mathbf{x}_{i}^{\rm{T}} \right\rbrack\]</div>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left\lbrack \overline{\mathbf{x}}{\overline{\mathbf{x}}}^{\rm{T}} \right\rbrack
= \frac{1}{N^{2}}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\mathbb{E}\left\lbrack \mathbf{x}_{i}\mathbf{x}_{j}^{\rm{T}} \right\rbrack}}
= \frac{1}{N^{2}}\left(N^{2}\mathbf{μ}\mathbf{μ}^{\rm{T}} + N\mathbf{\Sigma} \right)
= \mathbf{μ}\mathbf{μ}^{\rm{T}} + \frac{\mathbf{\Sigma}}{N}\]</div>
<p>For Gaussian i.i.d. RVs <span class="math notranslate nohighlight">\(\mathbf{x}_{1},\ldots,\mathbf{x}_{N}\)</span> as in</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\mathbf{E}\left\lbrack \mathbf{\Sigma}_{\text{ML}} \right\rbrack
&amp;=\frac{1}{N}\sum_{i = 1}^{N}{\mathbb{E}\left( \mathbf{x}_{i} - \overline{\mathbf{x}} \right)\left( \mathbf{x}_{i} - \overline{\mathbf{x}} \right)^{\rm{T}}}
= \frac{1}{N}\sum_{i = 1}^{N}{\mathbb{E}\left\lbrack \mathbf{x}_{i}\mathbf{x}_{i}^{\rm{T}} - \mathbf{x}_{i}{\overline{\mathbf{x}}}^{\rm{T}} - \overline{\mathbf{x}}\mathbf{x}_{i}^{\rm{T}} + \overline{\mathbf{x}}{\overline{\mathbf{x}}}^{\rm{T}} \right\rbrack} \\
&amp;= \frac{1}{N}\sum_{i = 1}^{N}\left( \cancel{\mathbf{μ}\mathbf{μ}^{\rm{T}}}\mathbf{+ \Sigma} - 2\left( \cancel{\mathbf{μ}\mathbf{μ}^{\rm{T}}} + \frac{\mathbf{\Sigma}}{N} \right) + \cancel{\mathbf{μ}\mathbf{μ}^{\rm{T}}} + \frac{\mathbf{\Sigma}}{N} \right)
= \frac{N - 1}{N}\mathbf{\Sigma}\end{aligned}\end{split}\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{\text{ML}} = \frac{N - 1}{N}\mathbf{\Sigma}\)</span> is a biased estimator. <span class="math notranslate nohighlight">\(\mathbf{μ}_{\text{ML}}\)</span> is trivially an unbiased
estimator, since
<span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack \mathbf{μ}_{\text{ML}} \right\rbrack=\mathbb{E}\left\lbrack \overline{\mathbf{x}} \right\rbrack = \mathbf{μ}\)</span>.</p>
</li>
<p><li style="margin-top:10px"></p>
<p id="fact-matrix-as-rank1-sum"><span class="ititle2">Fact 1-9.</span> Every matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> s.t.
<span class="math notranslate nohighlight">\(\operatorname{rank}\mathbf{A} = k\)</span> can be written as the sum of <span class="math notranslate nohighlight">\(k\)</span> rank-1 matrices, i.e.
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\mathbf{A} = \sum_{i = 1}^{k}{σ_{i}^{2}\mathbf{u}_{i}\mathbf{v}_{i}^{\rm{T}}}$}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{u}_{i}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}_{i}\)</span> are columns from two
matrices <span class="math notranslate nohighlight">\(\mathbf{U},\mathbf{V}\)</span> that come from the  <span><span class="exdef"> <span class="target" id="index-111"></span>reduced SVD</span></span> <span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{\text{U}\Lambda}\mathbf{V}^{\rm{T}}\)</span>; in particular, if
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is symmetric and positive semidefinite, then the singular
values are eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, and the reduced SVD becomes
reduced eigen-decomposition
<span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{\text{Q}\Lambda}\mathbf{Q}^{\rm{T}}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>
consists of <span class="math notranslate nohighlight">\(k\)</span> orthonormal eigenvectors
<span class="math notranslate nohighlight">\(\mathbf{q}_{1},\ldots,\mathbf{q}_{k}\)</span>, and thus
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\mathbf{A =}\sum_{i = 1}^{k}{𝜆_{i}\mathbf{q}_{i}\mathbf{q}_{i}^{\rm{T}}}$}\)</span>;
moreover, <span class="math notranslate nohighlight">\(\colorbox{fact}{$\mathbf{A}^{- 1}=\sum_{i = 1}^{k}{𝜆_{i}^{- 1}\mathbf{q}_{i}\mathbf{q}_{i}^{\rm{T}}}$}\)</span>
because <span class="math notranslate nohighlight">\(\mathbf{A}^{- 1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> share the same eigenspace for each eigenvalue.</p>
</div>
<div class="section" id="property-1-7-shape-of-contours-of-gaussian-density">
<span id="property-guassian-ellipsoid-contour"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Property 1-7.</span> <span class="bemp">Shape of contours of Gaussian density.</span><a class="headerlink" href="#property-1-7-shape-of-contours-of-gaussian-density" title="Permalink to this headline">¶</a></h3>Recall the  <span><span class="exdef"> <span class="target" id="index-112"></span>Mahalanobis distance</span></span>
<span class="math notranslate nohighlight">\(𝒹_{M}\left( \mathbf{x};\mathbf{μ,\Sigma} \right) = \left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{\Sigma}^{- 1}\left( \mathbf{x} - \mathbf{μ} \right)\)</span>.
Let <span class="math notranslate nohighlight">\(𝜆_{1},\ldots,𝜆_{m}\)</span> be the eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>, then
<span class="math notranslate nohighlight">\(\mathbf{\Sigma}^{- 1} = \sum_{i = 1}^{k}{𝜆_{i}^{- 1}\mathbf{q}_{i}\mathbf{q}_{i}^{\rm{T}}}\)</span> as a result of <a class="reference internal" href="#fact-matrix-as-rank1-sum"><span class="std std-ref">Fact 1-9</span></a>, and
<div class="math notranslate nohighlight">
\[𝒹_{M}\left( \mathbf{x};\mathbf{μ,\Sigma} \right) = \sum_{i = 1}^{k}{𝜆_{i}^{- 1}\left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{q}_{i}\mathbf{q}_{i}^{\rm{T}}}\left( \mathbf{x} - \mathbf{μ} \right) = \sum_{i = 1}^{k}{\left( \frac{1}{\sqrt{𝜆_{i}}}\mathbf{q}_{i}^{\rm{T}}\left( \mathbf{x} - \mathbf{μ} \right) \right)^{\rm{T}}\left( \frac{1}{\sqrt{𝜆_{i}}}\mathbf{q}_{i}^{\rm{T}}\left( \mathbf{x} - \mathbf{μ} \right) \right)}\]</div>
<p>Therefore,
<span><span class="result-highlight"> <span class="math notranslate nohighlight">\(𝒹_{M}\left( \mathbf{x};\mathbf{μ,\Sigma} \right) = \sum_{i = 1}^{k}z_{i}^{2} = \mathbf{z}^{\rm{T}}\mathbf{z}\)</span>
where <span class="math notranslate nohighlight">\(z_{i} = \frac{1}{\sqrt{𝜆_{i}}}\mathbf{q}_{i}^{\rm{T}}\left( \mathbf{x} - \mathbf{μ} \right)\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{z} = \mathbf{\Lambda}^{- \frac{1}{2}}\mathbf{Q}\left( \mathbf{x} - \mathbf{μ} \right) = \left( z_{1},\ldots,z_{m} \right)^{\rm{T}}\)</span>,
and <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{\Lambda}^{\frac{1}{2}}\mathbf{Q}^{\rm{T}}\mathbf{z}\mathbf{+}\mathbf{μ}\)</span>. </span></span></p>
<p>Recall an orthonormal matrix represents a rotation (oriented rotation, to be exact), then
<span class="math notranslate nohighlight">\(\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{Q}^{\rm{T}}\left( \cdot \right)\mathbf{+}\mathbf{μ}\)</span>
geometrically transforms the standard frame on a unit circle to a frame on an ellipsoid centered at <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is the
coordinate of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> w.r.t. to the ellipsoid frame. Conversely,
given a contour level <span class="math notranslate nohighlight">\(p\left( \mathbf{x} \right) = p_{0}\)</span>, we have
<span class="math notranslate nohighlight">\(\mathbf{z}^{\rm{T}}\mathbf{z =}c \Rightarrow \left\| \mathbf{z} \right\|_{2} = \sqrt{c}\)</span>
for some constant <span class="math notranslate nohighlight">\(c\)</span> (i.e. the contour is a circle w.r.t. the ellipsoid frame), and then any <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> s.t.
<span class="math notranslate nohighlight">\(\left\| \mathbf{z} \right\|_{2} = \sqrt{c}\)</span> will be on an ellipsoid centered at <span class="math notranslate nohighlight">\(\mathbf{μ}\)</span>.
Therefore, <span><span class="result-highlight"> the contours of multivariate Gaussian density are ellipsoids </span></span>.</p>
</li>
<p><li style="margin-top:10px"></p>
<p id="lemma-guassian-special-exponential-term-format"><span class="ititle">Lemma 1-3.</span> <span><span class="result-highlight"> If a density function
<span class="math notranslate nohighlight">\(p\left( \mathbf{x} \right) \propto \exp\left\{ - \frac{1}{2}\mathbf{x}^{\rm{T}}\mathbf{Ax +}\mathbf{x}^{\rm{T}}\mathbf{\text{Ay}} \right\}\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is symmetric positive semidefinite, then <span class="math notranslate nohighlight">\(p\)</span> must be Gaussian with precision <span class="math notranslate nohighlight">\(\mathbf{Ⲗ}=\mathbf{A}\)</span> and mean
<span class="math notranslate nohighlight">\(\mathbf{μ} = \mathbf{y}\)</span> </span></span>. This is simply we can rearrange it as</p>
<div class="math notranslate nohighlight">
\[p\left( \mathbf{x} \right) \propto \exp\left\{ - \frac{1}{2}\left( \mathbf{x - y} \right)^{\rm{T}}\mathbf{A}\left( \mathbf{x}-\mathbf{y} \right)-\mathbf{y}^{\rm{T}}\mathbf{\text{Ay}} \right\} \propto \exp\left\{ - \frac{1}{2}\left( \mathbf{x - y} \right)^{\rm{T}}\mathbf{A}\left( \mathbf{x}-\mathbf{y} \right) \right\}\]</div>
<p>There is no need to worry about normalization, since we have assumed <span class="math notranslate nohighlight">\(p\)</span> is a density, where its normalization is guaranteed.</p>
<p id="fact-matrix-block-inverse"><span class="ititle2">Fact 1-10.</span> If a block matrix <span class="math notranslate nohighlight">\(\begin{pmatrix} \mathbf{A} &amp; \mathbf{B} \\ \mathbf{C} &amp; \mathbf{D} \\ \end{pmatrix}\)</span> is inversible, then if <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is non-singular, we
have the following with all inverses valid</p>
<div class="math notranslate nohighlight">
\[\begin{split}\colorbox{fact}{$\begin{pmatrix}
\mathbf{A} &amp; \mathbf{B} \\
\mathbf{C} &amp; \mathbf{D} \\
\end{pmatrix}^{- 1} = \begin{pmatrix}
\mathbf{A}^{- 1}\left( \mathbf{I}\mathbf{+}\mathbf{\text{BMC}}\mathbf{A}^{- 1} \right) &amp; - \mathbf{A}^{- 1}\mathbf{\text{BM}} \\
- \mathbf{\text{MC}}\mathbf{A}^{- 1} &amp; \mathbf{M} \\
\end{pmatrix},\mathbf{M} = \left( \mathbf{D} - \mathbf{C}\mathbf{A}^{- 1}\mathbf{B} \right)^{- 1}$}\end{split}\]</div>
<p>and if <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> is non-singular, we have the following with all
inverses valid,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\colorbox{fact}{$\begin{pmatrix}
\mathbf{A} &amp; \mathbf{B} \\
\mathbf{C} &amp; \mathbf{D} \\
\end{pmatrix}^{- 1} = \begin{pmatrix}
\mathbf{M} &amp; - \mathbf{\text{MB}}\mathbf{D}^{- 1} \\
- \mathbf{D}^{- 1}\mathbf{\text{CM}} &amp; \mathbf{D}^{- 1}\left( \mathbf{I} + \mathbf{\text{CMB}}\mathbf{D}^{- 1} \right) \\
\end{pmatrix},\mathbf{M} = \left( \mathbf{A} - \mathbf{B}\mathbf{D}^{- 1}\mathbf{C} \right)^{- 1}$}\end{split}\]</div>
</div>
<div class="section" id="theorem-1-10-conditional-density-of-multivariate-guassian">
<span id="theorem-gaussian-conditional-density"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-10.</span> <span class="bemp">Conditional density of multivariate Guassian.</span><a class="headerlink" href="#theorem-1-10-conditional-density-of-multivariate-guassian" title="Permalink to this headline">¶</a></h3>Given
<span class="math notranslate nohighlight">\(\mathbf{x}\sim\operatorname{Gaussian}\left( \mathbf{μ},\mathbf{\Sigma} \right)\)</span>,
WLOG, partition <span class="math notranslate nohighlight">\(\mathbf{x} = \begin{pmatrix} \mathbf{x}_{a} \\ \mathbf{x}_{b} \\ \end{pmatrix}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{x}_{a} \in \mathbb{R}^{d}\)</span> is the
unknown, and <span class="math notranslate nohighlight">\(\mathbf{x}_{b} \in \mathbb{R}^{m - d}\)</span> is the condition, and we want to find the conditional density
<span class="math notranslate nohighlight">\(p\left( \mathbf{x}_{a}\mathbf{|}\mathbf{x}_{b} \right)\)</span>. Partition the mean and covariance accordingly as <span class="math notranslate nohighlight">\(\mathbf{μ} = \begin{pmatrix} \mathbf{μ}_{a} \\ \mathbf{μ}_{b} \\ \end{pmatrix}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{\Sigma} = \begin{pmatrix} \mathbf{\Sigma}_{{aa}} &amp; \mathbf{\Sigma}_{{ab}} \\ \mathbf{\Sigma}_{{ba}} &amp; \mathbf{\Sigma}_{{bb}} \\ \end{pmatrix}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Ⲗ} = \begin{pmatrix} \mathbf{Ⲗ}_{{aa}} &amp; \mathbf{Ⲗ}_{{ab}} \\ \mathbf{Ⲗ}_{{ba}} &amp; \mathbf{Ⲗ}_{{bb}} \\ \end{pmatrix}\)</span>.
Assume <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{{aa}}\)</span> is non-singular, then we have
<div class="math notranslate nohighlight" id="equation-eq-gaussian-conditional-mean-key-inference">
<span class="eqno">(1.12)<a class="headerlink" href="#equation-eq-gaussian-conditional-mean-key-inference" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned} - \frac{1}{2}\left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{Ⲗ}\left( \mathbf{x} - \mathbf{μ} \right) &amp;= \begin{pmatrix} \mathbf{x}_{a}-\mathbf{μ}_{a} \\ \mathbf{x}_{b}-\mathbf{μ}_{b} \\ \end{pmatrix}^{\rm{T}}\begin{pmatrix} \mathbf{Ⲗ}_{{aa}} &amp; \mathbf{Ⲗ}_{{ab}} \\
\mathbf{Ⲗ}_{{ba}} &amp; \mathbf{Ⲗ}_{{bb}} \\ \end{pmatrix}\begin{pmatrix} \mathbf{x}_{a}-\mathbf{μ}_{a} \\ \mathbf{x}_{b}-\mathbf{μ}_{b} \\ \end{pmatrix} \\
&amp;= - \frac{1}{2}\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right)^{\rm{T}}\mathbf{Ⲗ}_{{aa}}\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right) - \left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right)^{\rm{T}}\mathbf{Ⲗ}_{{ab}}\left( \mathbf{x}_{b}-\mathbf{μ}_{b} \right)-\frac{1}{2}\left( \mathbf{x}_{b}-\mathbf{μ}_{b} \right)^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\left( \mathbf{x}_{b}-\mathbf{μ}_{b} \right) \\
&amp;= - \frac{1}{2}\mathbf{x}_{a}^{\rm{T}}\mathbf{Ⲗ}_{{aa}}\mathbf{x}_{a} + \mathbf{x}_{a}^{\rm{T}}\mathbf{Ⲗ}_{{aa}}\mathbf{μ}_{a}-\mathbf{x}_{a}^{\rm{T}}\mathbf{Ⲗ}_{{ab}}\left( \mathbf{x}_{b}-\mathbf{μ}_{b} \right)\mathbf{+}\text{constant} \\
&amp;= - \frac{1}{2}\mathbf{x}_{a}^{\rm{T}}\mathbf{Ⲗ}_{{aa}}\mathbf{x}_{a} + \mathbf{x}_{a}^{\rm{T}}\mathbf{Ⲗ}_{{aa}}\left( \mathbf{μ}_{a}-\mathbf{Ⲗ}_{{aa}}^{- 1}\mathbf{Ⲗ}_{{ab}}\left( \mathbf{x}_{b}-\mathbf{μ}_{b} \right) \right)\mathbf{+}\text{constant} \end{aligned}\end{split}\]</div>
<p>By <a class="reference internal" href="#lemma-guassian-special-exponential-term-format"><span class="std std-ref">Lemma 1-3</span></a>, it follows that <span><span class="theorem-highlight"> if <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{{aa}}\)</span> is
non-singular, then <span class="math notranslate nohighlight">\(p\left( \mathbf{x}_{a}\mathbf{|}\mathbf{x}_{b} \right)\)</span> is Gaussian </span></span>.
Denote <span class="math notranslate nohighlight">\(\mathbf{x}_{a}\mathbf{|}\mathbf{x}_{b}\mathbf{\sim}\operatorname{Gaussian}\left( \mathbf{μ}_{a|b}\mathbf{,}\mathbf{\Sigma}_{a|b} \right)\)</span>, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-conditional-mean-and-cov-a">
<span class="eqno">(1.13)<a class="headerlink" href="#equation-eq-gaussian-conditional-mean-and-cov-a" title="Permalink to this equation">¶</a></span>\[\colorbox{theorem}{$\mathbf{μ}_{a|b}=\mathbf{μ}_{a}-\mathbf{Ⲗ}_{{aa}}^{- 1}\mathbf{Ⲗ}_{{ab}}\left( \mathbf{x}_{b}-\mathbf{μ}_{b} \right)\mathbf{,}\mathbf{\Sigma}_{a|b}=\mathbf{Ⲗ}_{{aa}}^{- 1}$}\]</div>
<p>If in addition <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{{bb}}\)</span> is non-singular, using
<a class="reference internal" href="#fact-matrix-block-inverse"><span class="std std-ref">Fact 1-10</span></a> and <span class="math notranslate nohighlight">\(\begin{pmatrix} \mathbf{\Sigma}_{{aa}} &amp; \mathbf{\Sigma}_{{ab}} \\ \mathbf{\Sigma}_{{ba}} &amp; \mathbf{\Sigma}_{{bb}} \\ \end{pmatrix}^{- 1} = \begin{pmatrix} \mathbf{Ⲗ}_{{aa}} &amp; \mathbf{Ⲗ}_{{ab}} \\ \mathbf{Ⲗ}_{{ba}} &amp; \mathbf{Ⲗ}_{{bb}} \\ \end{pmatrix}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\{ {\begin{array}{*{20}{l}}
{{Ⲗ _{aa}} = {{\left( {{{\mathbf{\Sigma }}_{aa}} - {{\mathbf{\Sigma }}_{ab}}{\mathbf{\Sigma }}_{bb}^{ - 1}{{\mathbf{\Sigma }}_{ba}}} \right)}^{ - 1}}} \\
{{Ⲗ _{ab}} =  - {Ⲗ _{aa}}{{\mathbf{\Sigma }}_{ab}}{\mathbf{\Sigma }}_{bb}^{ - 1}}
\end{array}} \right. \Rightarrow \left\{ {\begin{array}{*{20}{l}}
{Ⲗ _{aa}^{ - 1}{Ⲗ _{ab}} =  - {{\mathbf{\Sigma }}_{ab}}{\mathbf{\Sigma }}_{bb}^{ - 1}} \\
{Ⲗ _{aa}^{ - 1} = {{\mathbf{\Sigma }}_{aa}} - {{\mathbf{\Sigma }}_{ab}}{\mathbf{\Sigma }}_{bb}^{ - 1}{{\mathbf{\Sigma }}_{ba}}}
\end{array}} \right.\end{split}\]</div>
<p>Plug into <a class="reference internal" href="#equation-eq-gaussian-conditional-mean-and-cov-a">Eq.1.13</a> we have</p>
<div class="math notranslate nohighlight">
\[\colorbox{theorem}{${{\mathbf{μ }}_{a|b}}
= {{\mathbf{μ }}_a} + {{\mathbf{\Sigma }}_{ab}}{\mathbf{\Sigma }}_{bb}^{ - 1}\left( {{{\mathbf{x}}_b} - {{\mathbf{μ }}_b}} \right),{{\mathbf{\Sigma }}_{a|b}}
= {{\mathbf{\Sigma }}_{aa}} - {{\mathbf{\Sigma }}_{ab}}{\mathbf{\Sigma }}_{bb}^{ - 1}{{\mathbf{\Sigma }}_{ba}}$}\]</div>
<p>Similarly, if <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{{bb}}\)</span> is non-singular, then <span class="math notranslate nohighlight">\(p\left( \mathbf{x}_{b}|\mathbf{x}_{a} \right)\)</span> is Gaussian. Following
<a class="reference internal" href="#equation-eq-gaussian-conditional-mean-key-inference">Eq.1.12</a> we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned} - \frac{1}{2}\left( \mathbf{x} - \mathbf{μ} \right)^{\rm{T}}\mathbf{Ⲗ}\left( \mathbf{x} - \mathbf{μ} \right) &amp;= - \frac{1}{2}\mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\mathbf{x}_{b} + \mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\mathbf{μ}_{b}-\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right)^{\rm{T}}\mathbf{Ⲗ}_{{ab}}\mathbf{x}_{b}\mathbf{+}\text{constant} \\
&amp;= - \frac{1}{2}\mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\mathbf{x}_{b} + \mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\mathbf{μ}_{b}-\mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{ba}}\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right)\mathbf{+}\text{constant} \\
&amp;= - \frac{1}{2}\mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\mathbf{x}_{b} + \mathbf{x}_{b}^{\rm{T}}\mathbf{Ⲗ}_{{bb}}\left( \mathbf{μ}_{b}-\mathbf{Ⲗ}_{{bb}}^{- 1}\mathbf{Ⲗ}_{{ba}}\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right) \right)\mathbf{+}\text{constant}\end{aligned}\end{split}\]</div>
<p>If in addition <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{{aa}}\)</span> is non-singular, using
<a class="reference internal" href="#fact-matrix-block-inverse"><span class="std std-ref">Fact 1-10</span></a> we have</p>
<div class="math notranslate nohighlight">
\[\mathbf{Ⲗ}_{{bb}}^{- 1}\mathbf{Ⲗ}_{{ba}}\mathbf{= -}\mathbf{\Sigma}_{{ba}}\mathbf{\Sigma}_{{aa}}^{- 1}\mathbf{,}\mathbf{Ⲗ}_{{bb}}^{- 1}=\mathbf{\Sigma}_{{bb}}-\mathbf{\Sigma}_{{ba}}\mathbf{\Sigma}_{{aa}}^{- 1}\mathbf{\Sigma}_{{ab}}\]</div>
<p>Finally,</p>
<div class="math notranslate nohighlight">
\[\colorbox{theorem}{$\mathbf{μ}_{b|a}=\mathbf{μ}_{b}-\mathbf{Ⲗ}_{{bb}}^{- 1}\mathbf{Ⲗ}_{{ba}}\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right)=\mathbf{μ}_{b}\mathbf{+}\mathbf{\Sigma}_{{ba}}\mathbf{\Sigma}_{{aa}}^{- 1}\left( \mathbf{x}_{a}-\mathbf{μ}_{a} \right)$}\]</div>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-conditional-covariance-b">
<span class="eqno">(1.14)<a class="headerlink" href="#equation-eq-gaussian-conditional-covariance-b" title="Permalink to this equation">¶</a></span>\[\colorbox{theorem}{$\mathbf{\Sigma}_{b|a}=\mathbf{Ⲗ}_{{bb}}^{- 1}=\mathbf{\Sigma}_{{bb}}-\mathbf{\Sigma}_{{ba}}\mathbf{\Sigma}_{{aa}}^{- 1}\mathbf{\Sigma}_{{ab}}$}\]</div>
<p>We <span class="emp">note</span> 1) <span><span class="comment-highlight"> the conditional mean and variance can be simpler in terms of precision, as seen in <a class="reference internal" href="#equation-eq-gaussian-conditional-mean-and-cov-a">Eq.1.13</a> and <a class="reference internal" href="#equation-eq-gaussian-conditional-covariance-b">Eq.1.14</a> </span></span>;
2) <span><span class="comment-highlight"> the conditional mean <span class="math notranslate nohighlight">\(\mathbf{μ}_{a|b}\)</span> is independent of <span class="math notranslate nohighlight">\(\mathbf{x}_{a}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{μ}_{b|a}\)</span> is independent of
<span class="math notranslate nohighlight">\(\mathbf{x}_{b}\)</span>, and both <span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{a|b}\mathbf{,}\mathbf{\Sigma}_{b|a}\)</span> are independent of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> </span></span>, which is expected because the conditional mean and covariance should be determined by the distribution itself and the conditions,
but should not be related to the “unknown” RVs.</p>
</li></ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../02_neural_networks/00_index.html" class="btn btn-neutral float-right" title="2. Basic Neural Networks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../../index.html" class="btn btn-neutral" title="Study Notes in Machine Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Tony Chen, Drexel University.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'0.0.1',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>