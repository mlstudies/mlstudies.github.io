.. role:: red
.. role:: def
.. role:: exdef
.. role:: emp
.. role:: bemp
.. role:: ititle
.. role:: ititle2
.. role:: conn1
.. role:: conn2
.. role:: conn3
.. role:: uline
.. role:: ibold
.. role:: strike
.. role:: python(code)
   :language: python
.. role:: latex(code)
   :language: latex

.. |list-div| raw:: html

   <ul style="margin-left:20px">

.. |end-list-div| raw:: html

   </ul>

.. |enum-div| raw:: html

   <ol style="margin-left:20px">

.. |end-enum-div| raw:: html

   </ol>

.. |item-div| raw:: html

   <li>

.. |item| raw:: html

   <li>

.. |end-item-div| raw:: html

   </li>

.. |end-item| raw:: html

   </li>

.. |tip| raw:: html

   <span class="tooltip">

.. |tiptxt| raw:: html

   <span class="tooltiptext">

.. |theorem| raw:: html

   <span><span class="theorem-highlight">	

.. |result| raw:: html

   <span><span class="result-highlight">

.. |fact| raw:: html

   <span><span class="fact-highlight">

.. |comment| raw:: html

   <span><span class="comment-highlight">

.. |emperical| raw:: html

   <span><span class="emperical-highlight">

.. |strike| raw:: html

   <span><span class="strike">
   
.. |uline| raw:: html

   <span><span class="uline">
   
.. |ibold| raw:: html

   <span><span class="ibold">
   
.. |bold| raw:: html

   <span><span style="font-weight: bold;">
   
.. |italic| raw:: html

   <span><span style="font-style: italic;">
   
.. |para| raw:: html

   <span style="padding-left:20px"></span>

.. |def| raw:: html

   <span><span class="def">

.. |exdef| raw:: html

   <span><span class="exdef">

.. |hidden-cell| raw:: html

   <div class="hidden_cell" style="display:none">01

.. |indent-div| raw:: html

   <div><div style="text-indent: 20px;">

.. |end-div| raw:: html

   </div></div>

.. |span-center| raw:: html

   <span style="text-align:center;display:block">

.. |span-right| raw:: html

   <span style="text-align:right;display:block">

.. |span-justify| raw:: html

   <span style="text-align:justify">

.. |end-span| raw:: html

   </span></span>
   
.. raw:: html

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { extensions: ["color.js","autoload-all.js"] }
    });

	MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
     var color = MathJax.Extension["TeX/color"];
     color.colors["theorem"] = color.getColor('RGB','255,229,153');
	 color.colors["result"] = color.getColor('RGB','189,214,238');
	 color.colors["fact"] = color.getColor('RGB','255,255,204');
	 color.colors["emperical"] = color.getColor('RGB','253,240,207');
	 color.colors["comment"] = color.getColor('RGB','204,255,204');
     color.colors["thm"] = color.getColor('RGB','255,229,153');
	 color.colors["rlt"] = color.getColor('RGB','189,214,238');
	 color.colors["emp"] = color.getColor('RGB','253,240,207');
	 color.colors["comm"] = color.getColor('RGB','204,255,204');
	 color.colors["conn1"] = color.getColor('RGB','255,0,255');
	 color.colors["conn2"] = color.getColor('RGB','237,125,49');
	 color.colors["conn3"] = color.getColor('RGB','112,48,160');
	});
  </script>

Recurrent Networks
-----------------

|para| This section introduces  |exdef| :index:`recurrent networks` |end-span|, another major category
of neural networks that primarily deal with  |exdef| :index:`sequential data` |end-span| like
time series and natural language texts. :emp:`Again`, our data
:math:`\left( \mathbf{X},\mathbf{Y} \right)` consist of the data vectors
:math:`\mathbf{X} = \left( \mathbf{x}_{1},\mathbf{x}_{2},\ldots\mathbf{x}_{N} \right)`
and corresponding target vectors
:math:`\mathbf{Y} = \left( \mathbf{y}_{1},\mathbf{y}_{2},\ldots,\mathbf{y}_{N} \right)`,
and the objective is to find a neural network :math:`\mathcal{N}` that
approximately solves the regression problem
:math:`\mathbf{y}_{1},\ldots,\mathbf{y}_{N}\mathcal{\approx N}\left( \mathbf{x}_{1},\mathbf{x}_{2},\ldots\mathbf{x}_{N} \right)`.
Specially, when :math:`\mathbf{y}_{1},\ldots,\mathbf{y}_{N}` are categorical
scalars, we have a classification problem. As mentioned before, a  |exdef| :index:`neural network` |end-span| :math:`\mathcal{N}`
is a function derived from a complicated composition of  |exdef| :index:`elementary functions` |end-span|.

|para| We say :math:`\left( \mathbf{X},\mathbf{Y} \right)` is  |def| :index:`sequential data` |end-span| if
we assume :math:`\mathbf{y}_{t}` is dependent only on the present and the
past, i.e. assuming there exists underlying functions :math:`𝒻_{t}`
s.t.
:math:`\mathbf{y}_{t} = 𝒻_{t}\left( \mathbf{x}_{1},\ldots,\mathbf{x}_{t} \right),t = 1,\ldots,N`.
A  |def| :index:`recurrent network` |end-span|, denoted by :math:`\mathcal{R}`, aims at approximating
:math:`\mathcal{R \approx}\left( 𝒻_{1},\ldots,𝒻_{N} \right)`
by the following recursive compositions

.. math::
 \begin{aligned}
 \mathbf{h}_{1} &= f\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1} \right),{\widehat{\mathbf{y}}}_{1} = g\left( \mathbf{h}_{1};𝛌_{1} \right) \\
 \mathbf{h}_{2} &= f\left( \mathbf{h}_{1},\mathbf{x}_{2};𝛉_{2} \right) = f\left( f\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1} \right),\mathbf{x}_{2};𝛉_{2} \right),{\widehat{\mathbf{y}}}_{2} = g\left( \mathbf{h}_{2};𝛌_{2} \right) \\
 \mathbf{h}_{3} &= f\left( \mathbf{h}_{2},\mathbf{x}_{3};𝛉_{3} \right) = f\left( f\left( f\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1} \right),\mathbf{x}_{2};𝛉_{2} \right),\mathbf{x}_{3};𝛉_{3} \right),{\widehat{\mathbf{y}}}_{3} = g\left( \mathbf{h}_{3};𝛌_{3} \right) \\
 & \ldots \\
 \mathbf{h}_{N} &= f\left( \mathbf{h}_{N - 1},\mathbf{x}_{N};𝛉_{N} \right),{\widehat{\mathbf{y}}}_{N} = g\left( \mathbf{h}_{N};𝛌_{N} \right)
 \end{aligned}
 :label: eq_rnn_general

where 1) the intermediate variables
:math:`\mathbf{h}_{1},\mathbf{h}_{2},\ldots,\mathbf{h}_{N}` are called  |def| :index:`hidden states` |end-span|
or collectively called the  |def| :index:`hidden layer` |end-span|, and the
derivation of :math:`\mathbf{h}_{t + 1}` from :math:`\mathbf{h}_{t}` is called  |def| :index:`hidden state transition` |end-span|;
since :math:`\mathbf{h}_{t}` is recursively dependent on :math:`\mathbf{h}_{\mathbf{t - 1}}` and :math:`\mathbf{x}_{t}`, then
clearly |result| :math:`\mathbf{h}_{t}` is only dependent on
:math:`\mathbf{x}_{1},\ldots,\mathbf{x}_{t}`, the past and the present |end-span|; 2)
:math:`{\widehat{\mathbf{y}}}_{1},\ldots,{\widehat{\mathbf{y}}}_{N}` are
called the  |exdef| :index:`regressed target values` |end-span| or collective called the  |def| :index:`output layer` |end-span|,
which we hope to be good approximation of
:math:`\mathbf{y}_{1},\ldots,\mathbf{y}_{N}`; 3)
:math:`f\left( \mathbf{h},\mathbf{x};𝛉 \right)` and
:math:`g\left( \mathbf{h};𝛌 \right)` are two function families
and we can see :math:`\colorbox{result}{$g\left( \mathbf{h}_{t};𝛌_{t} \right)$ is only dependent on the past and the present data $\mathbf{x}_{1},\ldots,\mathbf{x}_{t}$}` since :math:`\mathbf{h}_{t}` is only
dependent on them, so |result| function
:math:`g \left( \mathbf{h};𝛌_{t} \right)`
is our approximation of the underlying true function :math:`𝒻_{t}`  |end-span|;
4) :math:`𝛉_{t},𝛌_{t},t = 1,2,\ldots` are
parameters to be inferred through optimization together with estimation
of :math:`\mathbf{h}_{t},{\widehat{\mathbf{y}}}_{t},t = 1,2,\ldots`
The model scheme in :eq:`eq_rnn_general` specifies the general architecture of RNN, and hence we also refer to it specifically as the general recurrent network
The plate diagram of model represented by :eq:`eq_rnn_general` is shown in .


.. raw:: html

   <div class="hidden_cell_cell" style="display: none">$$cell_001$$

.. image:: ./basic_rnn_unfolded.png
  :scale: 100%


.. raw:: html

   $$end_cell_001$$</div>


.. raw:: html

   <div class="hidden_cell_cell" style="display: none">$$cell_002$$

:math:`\Leftrightarrow`


.. raw:: html

   $$end_cell_002$$</div>


.. raw:: html

   <div class="hidden_cell_cell" style="display: none">$$cell_003$$

.. image:: ./basic_rnn_folded.png
  :scale: 150%


.. raw:: html

   $$end_cell_003$$</div>


.. raw:: html

   <table class="colwidths - given docutils" style="background:none; border:none;" ><colgroup><col width = "80%" /><col width = "2%" /><col width = "17%" /></colgroup><tbody><tr class="row-odd" style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;">$$cell_001$$</td><td  style="background:none; border:none;">$$cell_002$$</td><td  style="background:none; border:none;">$$cell_003$$</td></tr><tr style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;">(a)</td><td  style="background:none; border:none;"></td><td  style="background:none; border:none;">(b)</td></tr></tbody></table>


.. _figure_basic_rnn:

|span-justify| |ibold| Figure 3-1 |end-span| The plate diagram of the recurrent network.
(a) the unfolded diagram, with back propagation direction illustrated;
(b) the folded diagram. |end-span|

|para| For :eq:`eq_rnn_general`, :emp:`note` very often we may use  |exdef| :index:`time-homogenous parameters` |end-span|,
i.e. letting :math:`𝛉_{t} \equiv 𝛉_{1}` and/or
:math:`𝛌_{t} \equiv 𝛌_{1}` to reduce model
complexity. :emp:`Also note` the dimensions of different
variables in the network: a data vector :math:`\mathbf{x}`, a hidden state
vector :math:`\mathbf{h}` and a target vector :math:`\mathbf{y}` may have different
dimensions; :emp:`even` a regressed vector :math:`{\widehat{\mathbf{y}}}_{t}`
and its corresponding target vector :math:`\mathbf{y}_{t}` can have the different dimensions,
as long as a proper differentiable loss function can be chosen for them, see the later
example of :eq:`eq_lstm_classifier_nllloss`.

|para| To optimize our neural network, we must choose a  |exdef| :index:`loss function` |end-span|
:math:`ℓ`, which is a differentiable function dependent on the
regressed values :math:`\widehat{\mathbf{y}}` s and true target vectors
:math:`\mathbf{y}` s. For convenience, denote
:math:`f_{𝛉}\left( \mathbf{h},\mathbf{x} \right) := f\left( \mathbf{h,x};𝛉 \right)`
and
:math:`g_{𝛌}\left( \mathbf{h} \right) := \left( \mathbf{h};𝛌 \right)`.
After :math:`ℓ` is chosen, note again we need to adjust values for
:math:`{\widehat{\mathbf{y}}}_{t}`, :math:`𝛌_{t}`, :math:`\mathbf{h}_{t}`, :math:`𝛉_{t}`
through gradient-based optimization, the  |exdef| :index:`back propagation` |end-span| for :eq:`eq_rnn_general` is the
following process of gradient calculation: 1) the gradients of
output-layer unknowns :math:`\widehat{\mathbf{y}}` s and parameters :math:`𝛌` s,

.. math::
 \begin{aligned}
 \color{conn1}{\frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}} &\ \text{calculated as itself},t = 1,\ldots,N \\
 \frac{\partialℓ}{\partial𝛌_{t}}
 &= \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}\frac{\partial{\widehat{\mathbf{y}}}_{t}}{\partial𝛌_{t}}
 = {\color{conn1}{\frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}}} \frac{\partial g_{𝛌_{t}}\left( \mathbf{h}_{t} \right)}{\partial𝛌_{t}},t = 1,\ldots,N
 \end{aligned}
 :label: eq_rnn_back_prop_output_layer

and 2) the gradients of the hidden states :math:`\mathbf{h}_{t}` and parameters :math:`𝛉_{t}` for :math:`t = N,N - 1` are

.. math::
 \begin{aligned}
 {\color{conn2}\frac{\partialℓ}{\partial\mathbf{h}_{N}}}
 &= \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{N}}\frac{\partial{\widehat{\mathbf{y}}}_{N}}{\partial\mathbf{h}_{N}}
 = {\color{conn1} \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{N}}}\frac{\partial g_{𝛌_{N}}\left( \mathbf{h}_{N} \right)}{\partial\mathbf{h}_{N}} \\
 \frac{\partialℓ}{\partial𝛉_{N}}
 &= {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{N}}}\frac{\partial\mathbf{h}_{N}}{\partial𝛉_{N}}
 = \frac{\partialℓ}{\partial\mathbf{h}_{N}}\frac{\partial f_{𝛉_{N}}\left( \mathbf{h}_{N - 1},\mathbf{x}_{N} \right)}{\partial𝛉_{N}} \\
 {\color{conn2}{\frac{\partialℓ}{\partial\mathbf{h}_{N - 1}}}}
 &= \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{N - 1}}\frac{\partial{\widehat{\mathbf{y}}}_{N - 1}}{\partial\mathbf{h}_{N - 1}}
 + \frac{\partialℓ}{\partial\mathbf{h}_{N}}\frac{\partial\mathbf{h}_{N}}{\partial\mathbf{h}_{N - 1}}
 = {\color{conn1} \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{N - 1}}}\frac{\partial g_{𝛌_{N-1}}\left(\mathbf{h}_{N-1}\right)}{\partial \mathbf{h}_{N-1}}
 + {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{N}}}\frac{\partial f_{𝛉_{N}}\left( \mathbf{h}_{N - 1},\mathbf{x}_{N} \right)}{\partial\mathbf{h}_{N - 1}} \\
 \frac{\partialℓ}{\partial𝛉_{N - 1}}
 &= \frac{\partialℓ}{\partial\mathbf{h}_{N - 1}}\frac{\partial\mathbf{h}_{N - 1}}{\partial𝛉_{N - 1}}
 = {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{N - 1}}}\frac{\partial f_{𝛉_{N - 1}}\left( \mathbf{h}_{N - 2},\mathbf{x}_{N - 1} \right)}{\partial𝛉_{N - 1}}
 \end{aligned}

and then it is easy to see generally

.. math::
 \begin{aligned}
 {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{t}}}
 &= {\color{conn1} \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}}\frac{\partial g_{𝛌_{t}}\left( \mathbf{h}_{t} \right)\ }{\partial\mathbf{h}_{t}}
 + {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{t + 1}}}\frac{\partial f_{𝛉_{t + 1}}\left( \mathbf{h}_{t},\mathbf{x}_{t + 1} \right)}{\partial\mathbf{h}_{t}}, t=1,\ldots,N-1 \\
 \frac{\partialℓ}{\partial𝛉_{t}}
 &= {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{t}}}\frac{\partial f_{𝛉_{t}}\left( \mathbf{h}_{t - 1},\mathbf{x}_{t} \right)}{\partial𝛉_{t}},t = 1,\ldots,N - 1
 \end{aligned}
 :label: eq_rnn_back_prop_hidden_layer

If the parameters are time-homogeneous, i.e.
:math:`𝛌_{t} \equiv 𝛌` and
:math:`𝛉_{t} \equiv 𝛉`, then :eq:`eq_rnn_back_prop_output_layer` and :eq:`eq_rnn_back_prop_hidden_layer` are
replaced by

.. math::
 \begin{aligned}
 {\color{conn1} \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}} &\ \text{calculated as itself},t = 1,\ldots,N \\
 \frac{\partialℓ}{\partial𝛌}
 &= \sum_{t = 1}^{N}{{\color{conn1} \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}}\frac{\partial g_{𝛌}\left( \mathbf{h}_{t} \right)}{\partial𝛌}} \\
 {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{t}}}
 &= {\color{conn1} \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}}\frac{\partial g_{𝛌}}{\partial\mathbf{h}_{t}}
 + {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{t + 1}}}\frac{\partial f_{𝛉}\left( \mathbf{h}_{t},\mathbf{x}_{t + 1} \right)}{\partial\mathbf{h}_{t}},t = 1,\ldots,N - 1 \\
 \frac{\partialℓ}{\partial𝛉}
 &= \sum_{t = 1}^{N}{{\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{t}}}\frac{\partial f_{𝛉}\left( \mathbf{h}_{t - 1},\mathbf{x}_{t} \right)}{\partial𝛉}}
 \end{aligned}
 :label: eq_rnn_back_prop_homogeneous



|list-div|

|item| 


.. _model_lstm_single_gate:

:ititle:`Model 3-1.` :bemp:`LSTM with Single Forget Gate.`
^^^^^^^^^^^^^^^^^^^^
A well-known variant of RNN :eq:`eq_rnn_general` is the  |def| :index:`long short-term memory` |end-span|
network (LSTM), whose :emp:`essential idea` is to introduce the
vector-valued  |def| :index:`forget rate function` |end-span| or  |def| :index:`forget gate function` |end-span| or  |def| :index:`damping factor function` |end-span|
:math:`σ_{𝛉^{\left( σ \right)}} = \left( σ_{1},\ldots,σ_{m}\  \right)_{𝛉^{\left( σ \right)}} \in \left\lbrack 0,1 \right\rbrack^{m}`
to the hidden states such as the following, where ":math:`\circ`" denotes element-wise product,

.. math::
 \begin{aligned}
 \mathbf{h}_{1}
 &= σ\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1}^{\left( σ \right)} \right) \circ f\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1} \right),{\widehat{\mathbf{y}}}_{1} = g\left( \mathbf{h}_{1};𝛌_{1} \right) \\
 \mathbf{h}_{2}
 &= σ\left( \mathbf{h}_{1},\mathbf{x}_{2};𝛉_{2}^{\left( σ \right)} \right) \circ f\left( \mathbf{h}_{1},\mathbf{x}_{2};𝛉_{2} \right),{\widehat{\mathbf{y}}}_{2} = g\left( \mathbf{h}_{2};𝛌_{2} \right) \\
 \mathbf{h}_{3}
 &= σ\left( \mathbf{h}_{2},\mathbf{x}_{3};𝛉_{3}^{\left( σ \right)} \right) \circ f\left( \mathbf{h}_{2},\mathbf{x}_{3};𝛉_{3} \right),{\widehat{\mathbf{y}}}_{3} = g\left( \mathbf{h}_{3};𝛌_{3} \right) \\
 \ldots \\
 \mathbf{h}_{N}
 &= σ\left( \mathbf{h}_{N - 1},\mathbf{x}_{N};𝛉_{N}^{\left( σ \right)} \right) \circ f\left( \mathbf{h}_{N - 1},\mathbf{x}_{N};𝛉_{N} \right),{\widehat{\mathbf{y}}}_{N} = g\left( \mathbf{h}_{N};𝛌_{N} \right)
 \end{aligned}
 :label: eq_lstm_sg_general

We often call the forget gate function as simply a  |def| :index:`forget gate` |end-span| for
simplicity and refer to the model represented by :eq:`eq_lstm_sg_general` as the  |def| :index:`LSTM with single forget gate` |end-span|.
It be viewed as a special design of the general form of RNN in :eq:`eq_rnn_general` by letting
:math:`f\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t} \right) := σ\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t}^{\left( σ \right)} \right)f\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t} \right)`;
or conversely you may also view RNN as a special case of the LSTM using constant
:math:`σ_{𝛉^{\left( σ \right)}} \equiv 1` as the
forgetting rate. The :emp:`philosophy` behind LSTM is so that
|comment| the effects of a hidden state :math:`\mathbf{h}_{t}` should have a limited
effect on a far-away future hidden states :math:`\mathbf{h}_{t + T},T \gg 0`
when more recent data are provided, or the network's current state
should gradually "forget" the far-away past and hence the "short-term"
memory |end-span|. This philosophy intuitively makes sense for many applications,
e.g. today's stock price is likely to be more dependent on this
months' historical prices, rather than last month's price history. A
concrete example of such "history forgetting" is in :eq:`eq_lstm_sg_example_model` and :eq:`eq_lstm_sg_example_model_forgetting_illustration`.
The plate diagram of this general LSTM is given in Figure .


.. raw:: html

   <div class="hidden_cell_cell" style="display: none">$$cell_011$$

.. image:: ./lstm_single_gate_unfolded.png
  :scale: 100%


.. raw:: html

   $$end_cell_011$$</div>


.. raw:: html

   <div class="hidden_cell_cell" style="display: none">$$cell_012$$

:math:`\Leftrightarrow`


.. raw:: html

   $$end_cell_012$$</div>


.. raw:: html

   <div class="hidden_cell_cell" style="display: none">$$cell_013$$

.. image:: ./lstm_single_gate_folded.png
  :scale: 150%


.. raw:: html

   $$end_cell_013$$</div>


.. raw:: html

   <table class="colwidths - given docutils" style="background:none; border:none;" ><colgroup><col width = "80%" /><col width = "2%" /><col width = "17%" /></colgroup><tbody><tr class="row-odd" style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;">$$cell_011$$</td><td  style="background:none; border:none;">$$cell_012$$</td><td  style="background:none; border:none;">$$cell_013$$</td></tr><tr style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;">(a)</td><td  style="background:none; border:none;"></td><td  style="background:none; border:none;">(b)</td></tr></tbody></table>


.. _figure_lstm_single_gate:

|span-justify| |ibold| Figure 3-2 |end-span| The unfolded and folded plate diagram of the LSTM with single forget gate.
LSTM can be viewed as a special case of general RNN such that it defines a forgetting rate function
to dampen the effects of far-away past on the current state; or conversely the general RNN can be viewed
as a special case of LSTM by setting forgetting rate as the constant 1. |end-span|

:emp:`For example`, if the all labels in
:math:`\mathbf{y}_{1},\ldots,\mathbf{y}_{N}` are in or can be normalized to
range :math:`\left\lbrack - 1,1 \right\rbrack` (the range of the :math:`\tanh`
function), then a widely used canonical design based on :eq:`eq_lstm_sg_general` with
time-homogeneous parameters is as the following :eq:`eq_lstm_sg_example_model`. We
:emp:`note` a forget gate usually depends on surrounding units
in the network, but there is no recognized guideline for choice. In
:eq:`eq_lstm_sg_example_model` the forget gate is a sigmoid function dependent on the previous
hidden state and the current input.

.. math::
 \begin{aligned}
 σ\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t}^{\left( σ \right)} \right)
 &:= \text{sigmoid}\left( \mathbf{b}^{\left( σ \right)} + \mathbf{U}^{\left( σ \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( σ \right)}\mathbf{x}_{t} \right)\
 \text{where } 𝛉_{t}^{\left( σ \right)} := \left( \mathbf{b}^{\left( σ \right)},\mathbf{U}^{\left( σ \right)},\mathbf{V}^{\left( σ \right)} \right)\\
 f\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t} \right)
 &:= \mathbf{h}_{t - 1} + \text{sigmoid}\left( \mathbf{b}^{\left( f \right)} + \mathbf{U}^{\left( f \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( f \right)}\mathbf{x}_{t} \right)\
 \text{where } 𝛉_{t} := \left( \mathbf{b}^{\left( f \right)},\mathbf{U}^{\left( f \right)},\mathbf{V}^{\left( f \right)} \right)\\
 g\left( \mathbf{h}_{t};𝛌_{t} \right)
 &:= \text{sigmoid}\left( \mathbf{b}^{\left( g \right)} + \mathbf{U}^{\left( g \right)}\mathbf{h}_{t} + \mathbf{V}^{\left( g \right)}\mathbf{x}_{t} \right) \circ \tanh\left( \mathbf{h}_{t} \right)\
 \text{where }𝛌_{t} := \left( \mathbf{b}^{\left( g \right)},\mathbf{U}^{\left( g \right)},\mathbf{V}^{\left( g \right)} \right)\\
 \end{aligned}
 :label: eq_lstm_sg_example_model

where :math:`\text{sigmoid}\left( \cdot \right)` applies sigmoid function to
each element of a vector. The hidden state transition is then

.. math::
 \mathbf{h}_{t} = \text{sigmoid}\left( \mathbf{b}^{\left( σ \right)} + \mathbf{U}^{\left( σ \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( σ \right)}\mathbf{x}_{t} \right) \circ \mathbf{h}_{t - 1} + \text{sigmoid}\left( \mathbf{b}^{\left( σ \right)} + \mathbf{U}^{\left( σ \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( σ \right)}\mathbf{x}_{t} \right) \circ \text{sigmoid}\left( \mathbf{b}^{\left( f \right)} + \mathbf{U}^{\left( f \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( f \right)}\mathbf{x}_{t} \right)
 :label: eq_lstm_sg_example_model_hidden_state_transition

Let
:math:`\mathbf{σ}_{t} := σ\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t}^{\left( σ \right)} \right)`,
based on above design, we have

.. math::
 \mathbf{h}_{t + 1} = \mathbf{σ}_{t + 1} \circ \mathbf{h}_{t} + \text{others}

.. math::
 \mathbf{h}_{t + 2} = \mathbf{σ}_{t + 2} \circ \mathbf{σ}_{t + 1} \circ \mathbf{h}_{t} + \text{others}

and so on, we find

.. math::
 \mathbf{h}_{t + T} = \mathbf{σ}_{t + T} \circ \mathbf{σ}_{t + T - 1} \circ \ldots \circ \mathbf{σ}_{t + 1} \circ \mathbf{h}_{t} + \text{others}
 :label: eq_lstm_sg_example_model_forgetting_illustration

will be a vector of very small quantities when :math:`T` is large, effectively
limiting the effect of :math:`\mathbf{h}_{t}` on :math:`\mathbf{h}_{t + T}` and
hence :math:`{\widehat{\mathbf{y}}}_{t + T}`.


|end-item|

|item| 


.. _model_lstm_multiple_gates:

:ititle:`Model 3-2.` :bemp:`LSTM with Multiple Forget Gate.`
^^^^^^^^^^^^^^^^^^^^
A further canonical development of :eq:`eq_lstm_sg_general` is to introduce multiple forget
gates to achieve more flexibility, as in :eq:`eq_lstm_mg_general`, where
:math:`σ^{\left( j \right)},j = 1,\ldots,K` are forget gate functions,



.. raw:: html

   <div class="hidden_cell_cell" style="display: none">$$cell_021$$

.. image:: ./lstm_multiple_gate_folded.png
  :scale: 100%


.. raw:: html

   $$end_cell_021$$</div>


.. raw:: html

   <div class="hidden_cell_cell" style="display: none">$$cell_022$$

.. math::
 \begin{aligned}
 \mathbf{h}_{1}
 &= \sum_{j = 1}^{K}{σ^{\left( j \right)}\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1}^{\left( σ,j \right)} \right) \circ f^{\left( j \right)}\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1}^{\left( j \right)} \right)},{\widehat{\mathbf{y}}}_{1} = g\left( \mathbf{h}_{1};𝛌_{1} \right) \\
 \mathbf{h}_{2}
 &= \sum_{j = 1}^{K}{σ^{\left( j \right)}\left( \mathbf{h}_{1},\mathbf{x}_{2};𝛉_{2}^{\left( σ,j \right)} \right) \circ f^{\left( j \right)}\left( \mathbf{h}_{1},\mathbf{x}_{2};𝛉_{2}^{\left( j \right)} \right)},{\widehat{\mathbf{y}}}_{2} = g\left( \mathbf{h}_{2};𝛌_{2} \right) \\
 \mathbf{h}_{3}
 &= \sum_{j = 1}^{K}{σ^{\left( j \right)}\left( \mathbf{h}_{2},\mathbf{x}_{3};𝛉_{3}^{\left( σ,j \right)} \right) \circ f^{\left( j \right)}\left( \mathbf{h}_{2},\mathbf{x}_{3};𝛉_{3}^{\left( j \right)} \right)},{\widehat{\mathbf{y}}}_{3} = g\left( \mathbf{h}_{3};𝛌_{3} \right) \\
 & \ldots \\
 \mathbf{h}_{N}
 &= \sum_{j = 1}^{K}{σ^{\left( j \right)}\left( \mathbf{h}_{N - 1},\mathbf{x}_{N};𝛉_{N}^{\left( σ,j \right)} \right) \circ f^{\left( j \right)}\left( \mathbf{h}_{N - 1},\mathbf{x}_{N};𝛉_{N}^{\left( j \right)} \right)},{\widehat{\mathbf{y}}}_{N} = g\left( \mathbf{h}_{N};𝛌_{N} \right)
 \end{aligned}
 :label: eq_lstm_mg_general


.. raw:: html

   $$end_cell_022$$</div>


.. raw:: html

   <div class="hidden_cell_cell" style="display: none">$$cell_023$$


.. _figure_lstm_multiple_gates:

|span-justify| |ibold| Figure 3-3 |end-span| The folded diagram of a 2-forget-gate LSTM. |end-span|


.. raw:: html

   $$end_cell_023$$</div>


.. raw:: html

   <table class="colwidths - given docutils" style="background:none; border:none;" ><colgroup><col width = "25%" /><col width = "75%" /></colgroup><tbody><tr class="row-odd" style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;">$$cell_021$$</td><td  rowspan="2" style="background:none; border:none;">$$cell_022$$</td></tr><tr style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;">$$cell_023$$</td><td  style="background:none; border:none;"></td></tr></tbody></table>

To see why :eq:`eq_lstm_mg_general` makes sense, we can have a network with design similar to :eq:`eq_lstm_sg_example_model` as

.. math::
 \begin{aligned}
 σ^{\left( j \right)}\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t}^{\left( σ,j \right)} \right)
 &:= \text{sigmoid}\left( \mathbf{b}^{\left( σ,j \right)} + \mathbf{U}^{\left( σ,j \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( σ,j \right)}\mathbf{x}_{t} \right),j = 1,2 \\
 f^{\left( 1 \right)}\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t} \right) &:= \mathbf{h}_{t - 1} \\
 f^{\left( 2 \right)}\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t} \right) &:= \text{sigmoid}\left( \mathbf{b}^{\left( f \right)} + \mathbf{U}^{\left( f \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( f \right)}\mathbf{x}_{t} \right) \\
 g\left( \mathbf{h}_{t};𝛌_{t} \right) &:= \text{sigmoid}\left( \mathbf{b}^{\left( g \right)} + \mathbf{U}^{\left( g \right)}\mathbf{h}_{t} + \mathbf{V}^{\left( g \right)}\mathbf{x}_{t} \right) \circ \tanh\left( \mathbf{h}_{t} \right)
 \end{aligned}
 :label: eq_lstm_mg_example_model

The we have a hidden state transition as in :eq:`eq_lstm_mg_example_model_hidden_state_transition`,
similar to :eq:`eq_lstm_sg_example_model_hidden_state_transition` but
with independent sigmoid functions applied to the two addends.

.. math::
 \mathbf{h}_{t} = \text{sigmoid}\left( \mathbf{b}^{\left( σ,1 \right)} + \mathbf{U}^{\left( σ,1 \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( σ,1 \right)}\mathbf{x}_{t} \right) \circ \mathbf{h}_{t - 1} + \text{sisigmoid}\left( \mathbf{b}^{\left( σ,2 \right)} + \mathbf{U}^{\left( σ,2 \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( σ,2 \right)}\mathbf{x}_{t} \right) \circ \text{sigmoid}\left( \mathbf{b}^{\left( f \right)} + \mathbf{U}^{\left( f \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( f \right)}\mathbf{x}_{t} \right)
 :label: eq_lstm_mg_example_model_hidden_state_transition

The model represented by :eq:`eq_lstm_mg_example_model` is implemented by :python:`torch.nn.LSTM` in PyTorch.

|end-item| |end-list-div|



PyTorch implementation of basic LSTM classifier
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

|para| We now present a basic implementation of LSTM for classification of
sequence elements using PyTorch. The setup is that we have a label set
:math:`L`, multiple training data sets :math:`X_{1},X_{2},\ldots` where each
training data set consists of a sequence of data vectors and a sequence
of corresponding labels

.. math::
 X_{𝒾} = \left( \left( \mathbf{x}_{1}^{\left( 𝒾 \right)},\ldots,\mathbf{x}_{N^{\left( 𝒾 \right)}}^{\left( 𝒾 \right)} \right),\left( y_{1}^{\left( 𝒾 \right)},\ldots y_{N^{\left( 𝒾 \right)}}^{\left( 𝒾 \right)} \right) \right),
 \mathbf{x}_{N^{\left( 𝒾 \right)}}^{\left( 𝒾 \right)} \in \mathbb{R}^{m},
 y_{N^{\left( 𝒾 \right)}}^{\left( 𝒾 \right)} \in \left\{ 1,2,\ldots,\left| L \right| \right\},𝒾 = 1,2,\ldots,

|para| A concrete example of such classification is Part-of-Speech (PoS)
tagging for natural language. In this case, :math:`L` is the set of all
possible PoS tags representing nouns, verbs, adjectives, etc., such like
the `Penn Treebank PoS tags <https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html>`_;
:math:`X_{1},X_{2},\ldots` are sentences, and
:math:`\mathbf{x}_{1},\mathbf{x}_{2},\ldots` are embedding vectors of words in
a sentence, and :math:`y_{1},y_{2},\ldots` are labeling the PoS tags for each word.
We use :python:`torch.nn.LSTM` that implements :eq:`eq_lstm_mg_example_model`. :emp:`However`, the
output of :eq:`eq_lstm_mg_example_model` is a vector, the target values for classification are
categorical scalar labels; :emp:`therefore`, the model is tuned
as the following to suit our situation,



|enum-div|


.. raw:: html

   <li value="1" style="margin-top:10px">
The dimension of hidden states :math:`\mathbf{h}_{t}` is free to choose, but the dimension
of outputs :math:`{\widehat{\mathbf{y}}}_{t}` is designed to be equal to :math:`\left| L \right|`.
Moreover, we can design :math:`{\widehat{\mathbf{y}}}_{t}` as a discrete probability distribution.
All these designs are for the loss function \-- the well-known
negative-log-likelihood now can be chosen as the loss function \--
taking negative logarithm on the one element of
:math:`{\widehat{\mathbf{y}}}_{t}` indexed by true label :math:`y_{t}`, i.e.

.. math::
 \mathcal{l = -}\sum_{t = 1}^{N}{\log\left( {\widehat{\mathbf{y}}}_{t}\left( y_{t} \right) \right)}
 :label: eq_lstm_classifier_nllloss


|end-item|


.. raw:: html

   <li value="2" style="margin-top:10px">
Now in order for :math:`{\widehat{\mathbf{y}}}_{t} = g\left( \mathbf{h}_{t};𝛌_{t} \right)`
to be a probability distribution vector of dimension equal to the size
of label set, we can let

.. math::
 g\left( \mathbf{h}_{t};𝛌_{t} \right) := \operatorname{softmax}\left( \mathbf{c + W}\left( \text{sigmoid}\left( \mathbf{b}^{\left( g \right)} + \mathbf{U}^{\left( g \right)}\mathbf{h}_{t} + \mathbf{V}^{\left( g \right)}\mathbf{x}_{t} \right) \circ \tanh\left( \mathbf{h}_{t} \right) \right) \right)
where :math:`\mathbf{c \in}\mathbf{R}^{\left| L \right|}` and
:math:`\mathbf{W} \in \mathbb{R}^{\left| L \right| \times \dim\mathbf{h}_{t}}`,
and :math:`\log\left( \cdot \right),\operatorname{softmax}\left( \cdot \right)`
are applied element-wise if the input is a vector.

|end-item| |end-enum-div|



The following code is based on PyTorch `official document <https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html>`_.
It is a (nearly) complete code for a working basic LSTM classifier. The
adaptation of this code to a particular application, like PoS tagging,
is trivial. First of all, import necessary components of PyTorch

.. code-block:: python

 import torch
 import torch.nn as nn
 import torch.optim as optim
 import torch.nn.functional as nnF

Then we declare a class called :python:`LSTMClassfier` to construct the model.

.. code-block:: python

 # Every PyTorch neural network must inherit nn.Module
 class LSTMClassifier(nn.Module):

     # data_dim is the dimension of data vectors
     # state_dim is the dimension of hidden state vectors
     # label_set_size is the size of the label set, equal to the output vector dimension
     def |uline| init |end-span|(self, data_dim, hidden_state_dim, label_set_size):
         super(LSTMClassifier, self).|uline| init |end-span|()
         self.state_dim = hidden_state_dim

         # Creates a PyTorch LSTM model object.
         # The PyTorch LSTM takes data vectors as inputs,
         # and outputs a vector with dimensionality hidden_state_dim.
         self.lstm = nn.LSTM(data_dim, hidden_state_dim)

         # Creates a linear transformation for dimension conversion.
         self.linear_dim_conversion = nn.Linear(hidden_state_dim, label_set_size)

         # Initializes the hidden states.
         self.states = self.init_states()

     def init_states(self):
         #   Initializes states used by the build-in LSTM model. The first one stores
         # the output vectors, and the second one stores the hidden-state vectors. The
         # state tensors have to be of the shape specified below, required by PyTorch.
         return (torch.zeros(1, 1, self.state_dim),
                 torch.zeros(1, 1, self.state_dim))

     #   In PyTorch, the nn.Module class defines an abstract function named "forward".
     #   Any customized network in PyTorch inheriting nn.Module must implement this
     # "forward" function to define the network.
     #   Our network accepts a sequence of data vectors as the input, represented by
     # a matrix.
     def forward(self, sequence):
         # The network is constructed based on the build-in LSTM model.
         # It yields the output vectors and updates the hidden states.
         lstm_out, self.states = self.lstm(
             #   PyTorch requires the first two axes of its data input to have special
             # semantics. The length of first axis (or the number of rows) equals the
             # number of data vectors. The length of the second axis (or the number of
             # columns) equals to the number of mini batches, or 1 if mini batches are
             # not used. The third axis is the actual data and so its size equals the
             # dimension of the data vectors.
             #   The "sequence.view" function is reshaping the tensor to ensure the
             # "sequence" has the shape required by PyTorch. The third parameter "-1"
             # means the dimension of the third axis is inferred from the data.
             sequence.view(len(sequence), 1, -1), self.states)

         # Transforms the dimension of the output vectors.
         raw_label_scores = self.linear_dim_conversion(lstm_out.view(len(sequence), -1))

         #   Converts output vectors to probability distribution.
         #   Due to particular design of PyTorch (its NLLLoss does not do the logarithm),
         # we have to take logarithm of these probabilities here.
         label_scores = nnF.log_softmax(raw_label_scores, dim=1)

         return label_scores

Then we create an instance of :python:`LSTMClassfier` class, where :python:`DATA_DIM`,
:python:`HIDDEN_STATE_DIM`, :python:`LABEL_SET_SIZE` are parameters for the model
instance creation. We also create a loss function object and an
optimizer object.

.. code-block:: python

 model = LSTMClassifier(DATA_DIM, HIDDEN_STATE_DIM, LABEL_SET_SIZE) # gets the model object
 loss_function = nn.NLLLoss()  # gets the loss function object
 optimizer = optim.SGD(model.parameters(), lr=0.1)  # gets the stochastic gradient descent optimizer

Now suppose training sets :math:`X_{1},X_{2},\ldots` are provided in a Python list called
:python:`training_sets`, then each iteration of optimization runs through all
training sets and do gradient descent,

.. code-block:: python

 for _ in range(300):  # runs 300 iterations
     for sequence, labels in training_sets:  # runs through each training set

         # Clears previous computed gradients, required by PyTorch.
         model.zero_grad()

         # Each training set is a new sequence, so previous states should be cleared.
         # Therefore, re-initializes LSTM states for this training set.
         model.states = model.init_states()

         # Gets the output units of network
         label_scores = model(sequence)

         # Gets the loss object, which stores all necessary information for
         # back propagation and optimization
         loss = loss_function(label_scores, labels)

         # Do backward propagation (automatically calculating gradients)
         loss.backward()

         # Do gradient descent
         optimizer.step()

For prediction, suppose we want to label an unknown sequence represented by a PyTorch tensor variable called :python:`unlabeled_sequence`,
then the trained model estimates the labels as the following,

.. code-block:: python

 with torch.no_grad():  # This "with" statement is required by PyTorch

     # inputs the unlabeled sequence and gets the label scores
     estimated_label_scores = model(unlabeled_sequence)

     # uses argmax to get the best labels
     estimated_labels = torch.argmax(estimated_label_scores, dim=1)

     # prints out the labels
     print(estimated_labels)
