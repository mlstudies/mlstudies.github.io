.. role:: red
.. role:: def
.. role:: exdef
.. role:: emp
.. role:: bemp
.. role:: ititle
.. role:: ititle2
.. role:: conn1
.. role:: conn2
.. role:: conn3
.. role:: uline
.. role:: ibold
.. role:: strike

.. |list-div| raw:: html

   <ul style="left-margin:20px">

.. |item-div| raw:: html

   <li>

.. |item| raw:: html

   <li>

.. |end-list-div| raw:: html

   </ul>

.. |end-item-div| raw:: html

   </li>

.. |end-item| raw:: html

   </li>

.. |tip| raw:: html

   <span class="tooltip">

.. |tiptxt| raw:: html

   <span class="tooltiptext">

.. |theorem| raw:: html

   <span><span class="theorem-highlight">	

.. |result| raw:: html

   <span><span class="result-highlight">

.. |fact| raw:: html

   <span><span class="fact-highlight">

.. |comment| raw:: html

   <span><span class="comment-highlight">

.. |emperical| raw:: html

   <span><span class="emperical-highlight">

.. |strike| raw:: html

   <span><span class="strike">
   
.. |uline| raw:: html

   <span><span class="uline">
   
.. |ibold| raw:: html

   <span><span class="ibold">
   
.. |bold| raw:: html

   <span><span style="font-weight: bold;">
   
.. |italic| raw:: html

   <span><span style="font-style: italic;">
   
.. |para| raw:: html

   <span style="padding-left:20px"></span>

.. |def| raw:: html

   <span><span class="def">

.. |exdef| raw:: html

   <span><span class="exdef">

.. |hidden-cell| raw:: html

   <div class="hidden_cell" style="display:none">01

.. |indent-div| raw:: html

   <div><div style="text-indent: 20px;">

.. |end-div| raw:: html

   </div></div>

.. |span-center| raw:: html

   <span style="text-align:center;display:block">

.. |span-right| raw:: html

   <span style="text-align:right;display:block">

.. |end-span| raw:: html

   </span></span>
   
.. raw:: html

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { extensions: ["color.js","autoload-all.js"] }
    });

	MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
     var color = MathJax.Extension["TeX/color"];
     color.colors["theorem"] = color.getColor('RGB','255,229,153');
	 color.colors["result"] = color.getColor('RGB','189,214,238');
	 color.colors["fact"] = color.getColor('RGB','255,255,204');
	 color.colors["emperical"] = color.getColor('RGB','253,240,207');
	 color.colors["comment"] = color.getColor('RGB','204,255,204');
     color.colors["thm"] = color.getColor('RGB','255,229,153');
	 color.colors["rlt"] = color.getColor('RGB','189,214,238');
	 color.colors["emp"] = color.getColor('RGB','253,240,207');
	 color.colors["comm"] = color.getColor('RGB','204,255,204');
	 color.colors["conn1"] = color.getColor('RGB','255,0,255');
	 color.colors["conn2"] = color.getColor('RGB','237,125,49');
	 color.colors["conn3"] = color.getColor('RGB','112,48,160');
	});
  </script>

Recurrent Networks
-----------------

|para| This section introduces  |exdef| :index:`recurrent networks` |end-span|, another major category
of neural networks that primarily deal with  |exdef| :index:`sequential data` |end-span| like
time series and natural language texts. :emp:`Again`, our data
:math:`\left( \mathbf{X},\mathbf{Y} \right)` consist of the data vectors
:math:`\mathbf{X} = \left( \mathbf{x}_{1},\mathbf{x}_{2},\ldots\mathbf{x}_{N} \right)`
and corresponding target vectors
:math:`\mathbf{Y} = \left( \mathbf{y}_{1},\mathbf{y}_{2},\ldots,\mathbf{y}_{N} \right)`,
and the objective is to find a neural network :math:`\mathcal{N}` that
approximately solves the regression problem
:math:`\mathbf{y}_{1},\ldots,\mathbf{y}_{N}\mathcal{\approx N}\left( \mathbf{x}_{1},\mathbf{x}_{2},\ldots\mathbf{x}_{N} \right)`.
Specially, when :math:`\mathbf{y}_{1},\ldots,\mathbf{y}_{N}` are categorical
scalars, we have a classification problem. As mentioned before, a  |exdef| :index:`neural network` |end-span| :math:`\mathcal{N}`
is a function derived from a complicated composition of  |exdef| :index:`elementary functions` |end-span|.

|para| We say :math:`\left( \mathbf{X},\mathbf{Y} \right)` is  |def| :index:`sequential data` |end-span| if
we assume :math:`\mathbf{y}_{t}` is dependent only on the present and the
past, i.e. assuming there exists underlying functions :math:`𝒻_{t}`
s.t.
:math:`\mathbf{y}_{t} = 𝒻_{t}\left( \mathbf{x}_{1},\ldots,\mathbf{x}_{t} \right),t = 1,\ldots,N`.
A  |def| :index:`recurrent network` |end-span|, denoted by :math:`\mathcal{R}`, aims at approximating
:math:`\mathcal{R \approx}\left( 𝒻_{1},\ldots,𝒻_{N} \right)`
by the following recursive compositions

.. math::
 \begin{aligned}
 \mathbf{h}_{1} &= f\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1} \right),{\widehat{\mathbf{y}}}_{1} = g\left( \mathbf{h}_{1};𝛌_{1} \right) \\
 \mathbf{h}_{2} &= f\left( \mathbf{h}_{1},\mathbf{x}_{2};𝛉_{2} \right) = f\left( f\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1} \right),\mathbf{x}_{2};𝛉_{2} \right),{\widehat{\mathbf{y}}}_{2} = g\left( \mathbf{h}_{2};𝛌_{2} \right) \\
 \mathbf{h}_{3} &= f\left( \mathbf{h}_{2},\mathbf{x}_{3};𝛉_{3} \right) = f\left( f\left( f\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1} \right),\mathbf{x}_{2};𝛉_{2} \right),\mathbf{x}_{3};𝛉_{3} \right),{\widehat{\mathbf{y}}}_{3} = g\left( \mathbf{h}_{3};𝛌_{3} \right) \\
 & \ldots \\
 \mathbf{h}_{N} &= f\left( \mathbf{h}_{N - 1},\mathbf{x}_{N};𝛉_{N} \right),{\widehat{\mathbf{y}}}_{N} = g\left( \mathbf{h}_{N};𝛌_{N} \right)
 \end{aligned}
 :label: eq_rnn_general

where 1) the intermediate variables
:math:`\mathbf{h}_{1},\mathbf{h}_{2},\ldots,\mathbf{h}_{N}` are called  |def| :index:`hidden states` |end-span|
or collectively called the  |def| :index:`hidden layer` |end-span|, and the
derivation of :math:`\mathbf{h}_{t + 1}` from :math:`\mathbf{h}_{t}` is called  |def| :index:`hidden state transition` |end-span|;
since :math:`\mathbf{h}_{t}` is recursively dependent on :math:`\mathbf{h}_{\mathbf{t - 1}}` and :math:`\mathbf{x}_{t}`, then
clearly :math:`\mathbf{h}_{t}` is only dependent on
:math:`\mathbf{x}_{1},\ldots,\mathbf{x}_{t}`, the past and the present; 2)
:math:`{\widehat{\mathbf{y}}}_{1},\ldots,{\widehat{\mathbf{y}}}_{N}` are
called the  |exdef| :index:`regressed target values` |end-span| or collective called the  |def| :index:`output layer` |end-span|,
which we hope to be good approximation of
:math:`\mathbf{y}_{1},\ldots,\mathbf{y}_{N}`; 3)
:math:`f\left( \mathbf{h},\mathbf{x};𝛉 \right)` and
:math:`g\left( \mathbf{h};𝛌 \right)` are two function families
and we can see :math:`g\left( \mathbf{h}_{t};𝛌_{t} \right)` is
only dependent on the past and the present data
:math:`\mathbf{x}_{1},\ldots,\mathbf{x}_{t}` since :math:`\mathbf{h}_{t}` is only
dependent on them, so function
:math:`g \left( \mathbf{h};𝛌_{t} \right)`
is our approximation of the underlying true function :math:`𝒻_{t}`;
4) :math:`𝛉_{t},𝛌_{t},t = 1,2,\ldots` are
parameters to be inferred through optimization together with estimation
of :math:`\mathbf{h}_{t},{\widehat{\mathbf{y}}}_{t},t = 1,2,\ldots`
The model scheme in :eq:`eq_rnn_general` specifies the general architecture of RNN, and hence we also refer to it specifically as the general recurrent network
The plate diagram of model represented by :eq:`eq_rnn_general` is shown in .


.. raw:: html

   <div class="hidden_cell" style="display: none">$$cell_001$$

.. image:: ./basic_rnn_unfolded.png
  :scale: 100%
  

.. raw:: html

   $$end_cell_001$$</div>


.. raw:: html

   <div class="hidden_cell" style="display: none">$$cell_002$$

:math:`\Leftrightarrow`


.. raw:: html

   $$end_cell_002$$</div>


.. raw:: html

   <div class="hidden_cell" style="display: none">$$cell_003$$

.. image:: ./basic_rnn_folded.png
  :scale: 150%
  

.. raw:: html

   $$end_cell_003$$</div>


.. raw:: html

   <table class="colwidths - given docutils" style="background:none; border:none;"><colgroup><col width = "80%" /><col width = "2%" /><col width = "17%" /></colgroup><tbody><tr class="row-odd" style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;">$$cell_001$$</td><td  style="background:none; border:none;">$$cell_002$$</td><td  style="background:none; border:none;">$$cell_003$$</td></tr><tr style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;">(a)</td><td  style="background:none; border:none;"></td><td  style="background:none; border:none;">(b)</td></tr></tbody></table>


.. _figure_basic_rnn:

|span-center| |ibold| Figure 3-1 |end-span| The plate diagram of the recurrent network.
(a) the unfolded diagram, with back propagation direction illustrated;
(b) the folded diagram. |end-span|

|para| For :eq:`eq_rnn_general`, :emp:`note` very often we may use  |exdef| :index:`time-homogenous parameters` |end-span|,
i.e. letting :math:`𝛉_{t} \equiv 𝛉_{1}` and/or
:math:`𝛌_{t} \equiv 𝛌_{1}` to reduce model
complexity. :emp:`Also note` the dimensions of different
variables in the network: a data vector :math:`\mathbf{x}`, a hidden state
vector :math:`\mathbf{h}` and a target vector :math:`\mathbf{y}` may have different
dimensions; :emp:`even` a regressed vector :math:`{\widehat{\mathbf{y}}}_{t}`
and its corresponding target vector :math:`\mathbf{y}_{t}` can have the different dimensions,
as long as a proper differentiable loss function can be chosen for them, see the later
example of (6?€?11).

|para| To optimize our neural network, we must choose a  |exdef| :index:`loss function` |end-span|
:math:`ℓ`, which is a differentiable function dependent on the
regressed values :math:`\widehat{\mathbf{y}}` s and true target vectors
:math:`\mathbf{y}` s. For convenience, denote
:math:`f_{𝛉}\left( \mathbf{h},\mathbf{x} \right) := f\left( \mathbf{h,x};𝛉 \right)`
and
:math:`g_{𝛌}\left( \mathbf{h} \right) := \left( \mathbf{h};𝛌 \right)`.
After :math:`ℓ` is chosen, the  |exdef| :index:`back propagation` |end-span| for :eq:`eq_rnn_general` is the
following process of gradient calculation: 1) the gradients of
output-layer unknowns :math:`\widehat{\mathbf{y}}` s and parameters :math:`𝛌` s,

.. math::
 \begin{aligned}
 \color{conn1}{\frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}} &\ \text{calculated as itself},t = 1,\ldots,N \\
 \frac{\partialℓ}{\partial𝛌_{t}}
 &= \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}\frac{\partial{\widehat{\mathbf{y}}}_{t}}{\partial𝛌_{t}}
 = {\color{conn1}{\frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}}} \frac{\partial g_{𝛌_{t}}\left( \mathbf{h}_{t} \right)}{\partial𝛌_{t}},t = 1,\ldots,N
 \end{aligned}
 :label: eq_rnn_back_prop_output_layer

and 2) the gradients of the hidden states :math:`\mathbf{h}_{t}` and parameters :math:`𝛉_{t}` for :math:`t = N,N - 1` are

.. math::
 \begin{aligned}
 {\color{conn2}\frac{\partialℓ}{\partial\mathbf{h}_{N}}}
 &= \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{N}}\frac{\partial{\widehat{\mathbf{y}}}_{N}}{\partial\mathbf{h}_{N}}
 = {\color{conn1} \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{N}}}\frac{\partial g_{𝛌_{N}}\left( \mathbf{h}_{N} \right)}{\partial\mathbf{h}_{N}} \\
 \frac{\partialℓ}{\partial𝛉_{N}}
 &= {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{N}}}\frac{\partial\mathbf{h}_{N}}{\partial𝛉_{N}}
 = \frac{\partialℓ}{\partial\mathbf{h}_{N}}\frac{\partial f_{𝛉_{N}}\left( \mathbf{h}_{N - 1},\mathbf{x}_{N} \right)}{\partial𝛉_{N}} \\
 {\color{conn2}{\frac{\partialℓ}{\partial\mathbf{h}_{N - 1}}}}
 &= \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{N - 1}}\frac{\partial{\widehat{\mathbf{y}}}_{N - 1}}{\partial\mathbf{h}_{N - 1}}
 + \frac{\partialℓ}{\partial\mathbf{h}_{N}}\frac{\partial\mathbf{h}_{N}}{\partial\mathbf{h}_{N - 1}}
 = {\color{conn1} \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{N - 1}}}\frac{\partial g_{𝛌_{N-1}}\left(\mathbf{h}_{N-1}\right)}{\partial \mathbf{h}_{N-1}}
 + {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{N}}}\frac{\partial f_{𝛉_{N}}\left( \mathbf{h}_{N - 1},\mathbf{x}_{N} \right)}{\partial\mathbf{h}_{N - 1}} \\
 \frac{\partialℓ}{\partial𝛉_{N - 1}}
 &= \frac{\partialℓ}{\partial\mathbf{h}_{N - 1}}\frac{\partial\mathbf{h}_{N - 1}}{\partial𝛉_{N - 1}}
 = {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{N - 1}}}\frac{\partial f_{𝛉_{N - 1}}\left( \mathbf{h}_{N - 2},\mathbf{x}_{N - 1} \right)}{\partial𝛉_{N - 1}}
 \end{aligned}

and then it is easy to see generally

.. math::
 \begin{aligned}
 {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{t}}}
 &= {\color{conn1} \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}}\frac{\partial g_{𝛌_{t}}\left( \mathbf{h}_{t} \right)\ }{\partial\mathbf{h}_{t}}
 + {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{t + 1}}}\frac{\partial f_{𝛉_{t + 1}}\left( \mathbf{h}_{t},\mathbf{x}_{t + 1} \right)}{\partial\mathbf{h}_{t}}, t=1,\ldots,N-1 \\
 \frac{\partialℓ}{\partial𝛉_{t}}
 &= {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{t}}}\frac{\partial f_{𝛉_{t}}\left( \mathbf{h}_{t - 1},\mathbf{x}_{t} \right)}{\partial𝛉_{t}},t = 1,\ldots,N - 1
 \end{aligned}
 :label: eq_rnn_back_prop_hidden_layer

If the parameters are time-homogeneous, i.e.
:math:`𝛌_{t} \equiv 𝛌` and
:math:`𝛉_{t} \equiv 𝛉`, then :eq:`eq_rnn_back_prop_output_layer` and :eq:`eq_rnn_back_prop_hidden_layer` are
replaced by

.. math::
 \begin{aligned}
 {\color{conn1} \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}} &\ \text{calculated as itself},t = 1,\ldots,N \\
 \frac{\partialℓ}{\partial𝛌}
 &= \sum_{t = 1}^{N}{{\color{conn1} \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}}\frac{\partial g_{𝛌}\left( \mathbf{h}_{t} \right)}{\partial𝛌}} \\
 {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{t}}}
 &= {\color{conn1} \frac{\partialℓ}{\partial{\widehat{\mathbf{y}}}_{t}}}\frac{\partial g_{𝛌}}{\partial\mathbf{h}_{t}}
 + {\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{t + 1}}}\frac{\partial f_{𝛉}\left( \mathbf{h}_{t},\mathbf{x}_{t + 1} \right)}{\partial\mathbf{h}_{t}},t = 1,\ldots,N - 1 \\
 \frac{\partialℓ}{\partial𝛉}
 &= \sum_{t = 1}^{N}{{\color{conn2} \frac{\partialℓ}{\partial\mathbf{h}_{t}}}\frac{\partial f_{𝛉}\left( \mathbf{h}_{t - 1},\mathbf{x}_{t} \right)}{\partial𝛉}}
 \end{aligned}
 :label: eq_rnn_back_prop_homogeneous

|list-div|


|item| 


.. _model_lstm_single_gate:


:ititle:`Model 3-1.` :bemp:`LSTM with Single Forget Gate.`
^^^^^^^^^^^^^^^^^^^^
A well-known variant of RNN (6?€?1) is the  |def| :index:`long short-term memory` |end-span|
network (LSTM), whose :emp:`essential idea` is to introduce the
vector-valued  |def| :index:`forget rate function` |end-span| or  |def| :index:`forget gate function` |end-span| or  |def| :index:`damping factor function` |end-span|
:math:`σ_{𝛉^{\left( σ \right)}} = \left( σ_{1},\ldots,σ_{m}\  \right)_{𝛉^{\left( σ \right)}} \in \left\lbrack 0,1 \right\rbrack^{m}`
to the hidden states such as the following, where ":math:`\circ`" denotes element-wise product,

.. math::
 \begin{aligned}
 \mathbf{h}_{1}
 &= σ\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1}^{\left( σ \right)} \right) \circ f\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1} \right),{\widehat{\mathbf{y}}}_{1} = g\left( \mathbf{h}_{1};𝛌_{1} \right) \\
 \mathbf{h}_{2}
 &= σ\left( \mathbf{h}_{1},\mathbf{x}_{2};𝛉_{2}^{\left( σ \right)} \right) \circ f\left( \mathbf{h}_{1},\mathbf{x}_{2};𝛉_{2} \right),{\widehat{\mathbf{y}}}_{2} = g\left( \mathbf{h}_{2};𝛌_{2} \right) \\
 \mathbf{h}_{3}
 &= σ\left( \mathbf{h}_{2},\mathbf{x}_{3};𝛉_{3}^{\left( σ \right)} \right) \circ f\left( \mathbf{h}_{2},\mathbf{x}_{3};𝛉_{3} \right),{\widehat{\mathbf{y}}}_{3} = g\left( \mathbf{h}_{3};𝛌_{3} \right) \\
 \ldots \\
 \mathbf{h}_{N}
 &= σ\left( \mathbf{h}_{N - 1},\mathbf{x}_{N};𝛉_{N}^{\left( σ \right)} \right) \circ f\left( \mathbf{h}_{N - 1},\mathbf{x}_{N};𝛉_{N} \right),{\widehat{\mathbf{y}}}_{N} = g\left( \mathbf{h}_{N};𝛌_{N} \right)
 \end{aligned}
 :label: eq_lstm_sg_general

We often call the forget gate function as simply a  |def| :index:`forget gate` |end-span| for
simplicity and refer to the model represented by :eq:`eq_lstm_sg_general` as the  |def| :index:`LSTM with single forget gate` |end-span|.
It be viewed as a special design of the general form of RNN in :eq:`eq_rnn_general` by letting
:math:`f\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t} \right) := σ\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t}^{\left( σ \right)} \right)f\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t} \right)`;
or conversely you may also view RNN as a special case of the LSTM using constant
:math:`σ_{𝛉^{\left( σ \right)}} \equiv 1` as the
forgetting rate. The :emp:`philosophy` behind LSTM is so that
the effects of a hidden state :math:`\mathbf{h}_{t}` should have a limited
effect on a far-away future hidden states :math:`\mathbf{h}_{t + T},T \gg 0`
when more recent data are provided, or the network's current state
should gradually "forget" the far-away past and hence the "short-term"
memory. This philosophy intuitively makes sense for many applications,
e.g. today's stock price is likely to be more dependent on this
months' historical prices, rather than last month's price history. A
concrete example of such "history forgetting" is in :eq:`eq_lstm_sg_example_model` and :eq:`eq_lstm_sg_example_model_forgetting_illustration`.
The plate diagram of this general LSTM is given in Figure 6?€?2.


.. raw:: html

   <div class="hidden_cell" style="display: none">$$cell_011$$

.. image:: ./lstm_single_gate_unfolded.png
  :scale: 100%
  

.. raw:: html

   $$end_cell_011$$</div>


.. raw:: html

   <div class="hidden_cell" style="display: none">$$cell_012$$

:math:`\Leftrightarrow`


.. raw:: html

   $$end_cell_012$$</div>


.. raw:: html

   <div class="hidden_cell" style="display: none">$$cell_013$$

.. image:: ./lstm_single_gate_folded.png
  :scale: 150%
  

.. raw:: html

   $$end_cell_013$$</div>


.. raw:: html

   <table class="colwidths - given docutils" style="background:none; border:none;"><colgroup><col width = "80%" /><col width = "2%" /><col width = "17%" /></colgroup><tbody><tr class="row-odd" style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;">$$cell_011$$</td><td  style="background:none; border:none;">$$cell_012$$</td><td  style="background:none; border:none;">$$cell_013$$</td></tr><tr style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;">(a)</td><td  style="background:none; border:none;"></td><td  style="background:none; border:none;">(b)</td></tr></tbody></table>


.. _figure_lstm_single_gate:

|span-center| |ibold| Figure 3-2 |end-span| The unfolded and folded plate diagram of the LSTM with single forget gate.
LSTM can be viewed as a special case of general RNN such that it defines a forgetting rate function
to dampen the effects of far-away past on the current state; or conversely the general RNN can be viewed
as a special case of LSTM by setting forgetting rate as the constant 1. |end-span|

:emp:`For example`, if the all labels in
:math:`\mathbf{y}_{1},\ldots,\mathbf{y}_{N}` are in or can be normalized to
range :math:`\left\lbrack - 1,1 \right\rbrack` (the range of the :math:`\tanh`
function), then a widely used canonical design based on (6?€?4) with
time-homogeneous parameters is as the following :eq:`eq_lstm_sg_example_model`. We
:emp:`note` a forget gate usually depends on surrounding units
in the network, but there is no recognized guideline for choice. In
:eq:`eq_lstm_sg_example_model` the forget gate is a sigmoid function dependent on the previous
hidden state and the current input.

.. math::
 \begin{aligned}
 σ\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t}^{\left( σ \right)} \right)
 &:= \text{sigmoid}\left( \mathbf{b}^{\left( σ \right)} + \mathbf{U}^{\left( σ \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( σ \right)}\mathbf{x}_{t} \right)\
 \text{where } 𝛉_{t}^{\left( σ \right)} := \left( \mathbf{b}^{\left( σ \right)},\mathbf{U}^{\left( σ \right)},\mathbf{V}^{\left( σ \right)} \right)\\
 f\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t} \right)
 &:= \mathbf{h}_{t - 1} + \text{sigmoid}\left( \mathbf{b}^{\left( f \right)} + \mathbf{U}^{\left( f \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( f \right)}\mathbf{x}_{t} \right)\
 \text{where } 𝛉_{t} := \left( \mathbf{b}^{\left( f \right)},\mathbf{U}^{\left( f \right)},\mathbf{V}^{\left( f \right)} \right)\\
 g\left( \mathbf{h}_{t};𝛌_{t} \right)
 &:= \text{sigmoid}\left( \mathbf{b}^{\left( g \right)} + \mathbf{U}^{\left( g \right)}\mathbf{h}_{t} + \mathbf{V}^{\left( g \right)}\mathbf{x}_{t} \right) \circ \tanh\left( \mathbf{h}_{t} \right)\
 \text{where }𝛌_{t} := \left( \mathbf{b}^{\left( g \right)},\mathbf{U}^{\left( g \right)},\mathbf{V}^{\left( g \right)} \right)\\
 \end{aligned}
 :label: eq_lstm_sg_example_model

where :math:`\text{sigmoid}\left( \cdot \right)` applies sigmoid function to
each element of a vector. The hidden state transition is then

.. math::
 \mathbf{h}_{t} = \text{sigmoid}\left( \mathbf{b}^{\left( σ \right)} + \mathbf{U}^{\left( σ \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( σ \right)}\mathbf{x}_{t} \right) \circ \mathbf{h}_{t - 1} + \text{sigmoid}\left( \mathbf{b}^{\left( σ \right)} + \mathbf{U}^{\left( σ \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( σ \right)}\mathbf{x}_{t} \right) \circ \text{sigmoid}\left( \mathbf{b}^{\left( f \right)} + \mathbf{U}^{\left( f \right)}\mathbf{h}_{t - 1} + \mathbf{V}^{\left( f \right)}\mathbf{x}_{t} \right)
 :label: eq_lstm_sg_example_model_hidden_state_transition

Let
:math:`\mathbf{σ}_{t} := σ\left( \mathbf{h}_{t - 1},\mathbf{x}_{t};𝛉_{t}^{\left( σ \right)} \right)`,
based on above design, we have

.. math::
 \mathbf{h}_{t + 1} = \mathbf{σ}_{t + 1} \circ \mathbf{h}_{t} + \text{others}

.. math::
 \mathbf{h}_{t + 2} = \mathbf{σ}_{t + 2} \circ \mathbf{σ}_{t + 1} \circ \mathbf{h}_{t} + \text{others}

and so on, we find

.. math::
 \mathbf{h}_{t + T} = \mathbf{σ}_{t + T} \circ \mathbf{σ}_{t + T - 1} \circ \ldots \circ \mathbf{σ}_{t + 1} \circ \mathbf{h}_{t} + \text{others}
 :label: eq_lstm_sg_example_model_forgetting_illustration

will be a vector of very small quantities when :math:`T` is large, effectively
limiting the effect of :math:`\mathbf{h}_{t}` on :math:`\mathbf{h}_{t + T}` and
hence :math:`{\widehat{\mathbf{y}}}_{t + T}`.


|end-item|

|item| 


.. _model_lstm_multiple_gates:

:ititle:`Model 3-2.` :bemp:`LSTM with Multiple Forget Gate.`
^^^^^^^^^^^^^^^^^^^^
A further canonical development of :eq:`eq_lstm_sg_general` is to introduce multiple forget
gates to achieve more flexibility, as in (6?€?8), where
:math:`σ^{\left( j \right)},j = 1,\ldots,K` are forget gate functions,



.. raw:: html

   <div class="hidden_cell" style="display: none">$$cell_021$$

.. image:: ./lstm_multiple_gate_folded.png
  :scale: 100%
  

.. raw:: html

   $$end_cell_021$$</div>


.. raw:: html

   <div class="hidden_cell" style="display: none">$$cell_022$$

.. math::
 \begin{aligned}
 \mathbf{h}_{1}
 &= \sum_{j = 1}^{K}{σ^{\left( j \right)}\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1}^{\left( σ,j \right)} \right) \circ f^{\left( j \right)}\left( \mathbf{h}_{0},\mathbf{x}_{1};𝛉_{1}^{\left( j \right)} \right)},{\widehat{\mathbf{y}}}_{1} = g\left( \mathbf{h}_{1};𝛌_{1} \right) \\
 \mathbf{h}_{2}
 &= \sum_{j = 1}^{K}{σ^{\left( j \right)}\left( \mathbf{h}_{1},\mathbf{x}_{2};𝛉_{2}^{\left( σ,j \right)} \right) \circ f^{\left( j \right)}\left( \mathbf{h}_{1},\mathbf{x}_{2};𝛉_{2}^{\left( j \right)} \right)},{\widehat{\mathbf{y}}}_{2} = g\left( \mathbf{h}_{2};𝛌_{2} \right) \\
 \mathbf{h}_{3}
 &= \sum_{j = 1}^{K}{σ^{\left( j \right)}\left( \mathbf{h}_{2},\mathbf{x}_{3};𝛉_{3}^{\left( σ,j \right)} \right) \circ f^{\left( j \right)}\left( \mathbf{h}_{2},\mathbf{x}_{3};𝛉_{3}^{\left( j \right)} \right)},{\widehat{\mathbf{y}}}_{3} = g\left( \mathbf{h}_{3};𝛌_{3} \right) \\
 & \ldots \\
 \mathbf{h}_{N}
 &= \sum_{j = 1}^{K}{σ^{\left( j \right)}\left( \mathbf{h}_{N - 1},\mathbf{x}_{N};𝛉_{N}^{\left( σ,j \right)} \right) \circ f^{\left( j \right)}\left( \mathbf{h}_{N - 1},\mathbf{x}_{N};𝛉_{N}^{\left( j \right)} \right)},{\widehat{\mathbf{y}}}_{N} = g\left( \mathbf{h}_{N};𝛌_{N} \right)
 \end{aligned}
 :label: eq_lstm_mg_general


.. raw:: html

   $$end_cell_022$$</div>


.. raw:: html

   <div class="hidden_cell" style="display: none">$$cell_023$$


.. _figure_lstm_multiple_gates:

|span-center| |ibold| Figure -1 |end-span| The folded diagram of a 2-forget-gate LSTM. |end-span|



.. raw:: html

   $$end_cell_023$$</div>


.. raw:: html

   <table class="colwidths - given docutils" style="background:none; border:none;"><colgroup><col width = "25%" /><col width = "75%" /></colgroup><tbody><tr class="row-odd" style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;">$$cell_021$$</td><td  rowspan="2" style="background:none; border:none;">$$cell_022$$</td></tr><tr style="text-align:center; vertical-align:middle; "><td  style="background:none; border:none;">$$cell_023$$</td><td  style="background:none; border:none;"></td></tr></tbody></table>

To see why :eq:`eq_lstm_mg_general` makes sense, we can have a network with design similar to :eq:`eq_lstm_sg_example_model` as


|end-item| |end-list-div|

