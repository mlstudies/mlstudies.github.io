.. role:: red
.. role:: def
.. role:: exdef
.. role:: emp
.. role:: bemp
.. role:: ititle
.. role:: ititle2
.. role:: conn1
.. role:: conn2
.. role:: conn3
.. role:: uline
.. role:: ibold
.. role:: strike
.. role:: python(code)
   :language: python
.. role:: latex(code)
   :language: latex

.. |list-div| raw:: html

   <ul style="margin-left:20px">

.. |end-list-div| raw:: html

   </ul>

.. |enum-div| raw:: html

   <ol style="margin-left:20px">

.. |end-enum-div| raw:: html

   </ol>

.. |item-div| raw:: html

   <li>

.. |item| raw:: html

   <li>

.. |end-item-div| raw:: html

   </li>

.. |end-item| raw:: html

   </li>

.. |tip| raw:: html

   <span class="tooltip">

.. |tiptxt| raw:: html

   <span class="tooltiptext">

.. |theorem| raw:: html

   <span><span class="theorem-highlight">	

.. |result| raw:: html

   <span><span class="result-highlight">

.. |fact| raw:: html

   <span><span class="fact-highlight">

.. |comment| raw:: html

   <span><span class="comment-highlight">

.. |emperical| raw:: html

   <span><span class="emperical-highlight">

.. |strike| raw:: html

   <span><span class="strike">
   
.. |uline| raw:: html

   <span><span class="uline">
   
.. |ibold| raw:: html

   <span><span class="ibold">
   
.. |bold| raw:: html

   <span><span style="font-weight: bold;">
   
.. |italic| raw:: html

   <span><span style="font-style: italic;">
   
.. |para| raw:: html

   <span style="padding-left:20px"></span>

.. |def| raw:: html

   <span><span class="def">

.. |exdef| raw:: html

   <span><span class="exdef">

.. |hidden-cell| raw:: html

   <div class="hidden_cell" style="display:none">01

.. |indent-div| raw:: html

   <div><div style="text-indent: 20px;">

.. |end-div| raw:: html

   </div></div>

.. |span-center| raw:: html

   <span style="text-align:center;display:block">

.. |span-right| raw:: html

   <span style="text-align:right;display:block">

.. |span-justify| raw:: html

   <span style="text-align:justify">

.. |end-span| raw:: html

   </span></span>
   
.. raw:: html

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { extensions: ["color.js","autoload-all.js"] }
    });

	MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
     var color = MathJax.Extension["TeX/color"];
     color.colors["theorem"] = color.getColor('RGB','255,229,153');
	 color.colors["result"] = color.getColor('RGB','189,214,238');
	 color.colors["fact"] = color.getColor('RGB','255,255,204');
	 color.colors["emperical"] = color.getColor('RGB','253,240,207');
	 color.colors["comment"] = color.getColor('RGB','204,255,204');
     color.colors["thm"] = color.getColor('RGB','255,229,153');
	 color.colors["rlt"] = color.getColor('RGB','189,214,238');
	 color.colors["emp"] = color.getColor('RGB','253,240,207');
	 color.colors["comm"] = color.getColor('RGB','204,255,204');
	 color.colors["conn1"] = color.getColor('RGB','255,0,255');
	 color.colors["conn2"] = color.getColor('RGB','237,125,49');
	 color.colors["conn3"] = color.getColor('RGB','112,48,160');
	});
  </script>

Covariance Matrix
-----------------

|para| Given two scalar RVs :math:`X` and :math:`Y`, then the covariance
:math:`\colorbox{fact}{$\operatorname{cov} \left( {X,Y} \right) = \mathbb{E}\left[ {\left( {X - \mathbb{E}X} \right)\left( {Y - \mathbb{E}Y} \right)} \right]$}`.
Given a RV vector :math:`X = \left( {\begin{array}{*{20}{c}}{{X_1}} \\   \vdots  \\  {{X_M}}\end{array}} \right)`,
define :math:`\mathbb{E}X = \left( {\begin{array}{*{20}{c}}{\mathbb{E}{X_1}} \\\vdots  \\{\mathbb{E}{X_M}}\end{array}} \right)`
(and similarly the expectation of a RV matrix is to take expectation on each entry of that matrix), then the  |def| :index:`covariance matrix` |end-span| is defined as the following,

.. math::
 \begin{align}
 \Sigma \left( X \right) & = \left( {\begin{array}{*{20}{c}}
 {\operatorname{cov} \left( {{X_1},{X_1}} \right)}& \cdots &{\operatorname{cov} \left( {{X_1},{X_M}} \right)} \\
 \vdots & \ddots & \vdots  \\
 {\operatorname{cov} \left( {{X_M},{X_1}} \right)}& \cdots &{\operatorname{cov} \left( {{X_M},{X_M}} \right)}
 \end{array}} \right) \hfill \\
 & = \left( {\begin{array}{*{20}{c}}
 {\mathbb{E}\left[ {\left( {{X_1} - \mathbb{E}{X_1}} \right)\left( {{X_1} - \mathbb{E}{X_1}} \right)} \right]}& \cdots &{\mathbb{E}\left[ {\left( {{X_1} - \mathbb{E}{X_1}} \right)\left( {{X_M} - \mathbb{E}{X_M}} \right)} \right]} \\
 \vdots & \ddots & \vdots  \\
 {\mathbb{E}\left[ {\left( {{X_M} - \mathbb{E}{X_M}} \right)\left( {{X_1} - \mathbb{E}{X_1}} \right)} \right]}& \cdots &{\mathbb{E}\left[ {\left( {{X_M} - \mathbb{E}{X_M}} \right)\left( {{X_M} - \mathbb{E}{X_M}} \right)} \right]}
 \end{array}} \right) \hfill \\
 &=\colorbox{fact}{$\mathbb{E}\left[ {\left( {X - \mathbb{E}X} \right){{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}} \right]$} \hfill \\
 \end{align}
 :label: eq_rv_cov
where the diagonal elements are  |exdef| :index:`variances` |end-span| that can be denoted by :math:`\operatorname{var}\left( X_{i} \right) := \operatorname{cov}\left( X_{i},X_{i} \right),i = 1,\ldots,M`
. We :emp:`note` there is difference that for two scalar RVs :math:`X,Y`, :math:`\operatorname{cov}‚Å°(X,Y)` is a value, but :math:`\operatorname{\Sigma}\begin{pmatrix} X \\ Y \\ \end{pmatrix}` is a :math:`2√ó2` matrix.

|para| On the other hand, given two RVs :math:`X,Y` and draw samples :math:`{\mathbf{x}} = \left( {{x_1}, \ldots ,{x_N}} \right)\sim X`
and :math:`{\mathbf{y}} = \left( {{y_1}, \ldots ,{y_N}} \right)\sim Y`, then we define the  |def| :index:`sample covariance` |end-span| of them as
:math:`\colorbox{fact}{$\operatorname{cov} \left( {{\mathbf{x}},{\mathbf{y}}} \right) = \frac{1}{{N - 1}}{\left( {{\mathbf{x}} - {\mathbf{\bar x}}} \right)^{\text{T}}}\left( {{\mathbf{y}} - {\mathbf{\bar y}}} \right)$}`.
Given a RV vector :math:`X = \left( {\begin{array}{*{20}{c}}{{X_1}} \\\vdots  \\{{X_M}}\end{array}} \right)`,
we can draw  |def| :index:`samples` |end-span| :math:`{\mathbf{x}}_1^{\text{T}} = \left( {{x_{1,1}}, \ldots ,{x_{1,N}}} \right)\sim {X_1}, \ldots , {\mathbf{x}}_M^{\text{T}} = \left( {{x_{M,1}}, \ldots ,{x_{M,M}}} \right)\sim {X_M}`,
and form a  |exdef| :index:`sample matrix` |end-span| :math:`{\mathbf{X}} = \left( {\begin{array}{*{20}{c}}{{\mathbf{x}}_1^{\text{T}}} \\\vdots  \\{{\mathbf{x}}_M^{\text{T}}}\end{array}} \right)`.
In the machine learning context, the rows of :math:`ùêó` are also referred to as |tip|  |def| :index:`feature vectors` |end-span| |tiptxt| Feature vectors are column vectors even though they are rows in the data matrix. |end-span| because it treats the RVs :math:`X_1,‚Ä¶,X_M` as representing :math:`M` random features of a data point;
and the columns of :math:`ùêó` are called  |def| :index:`data entries` |end-span|, because they are actually observed data.
Then the  |def| :index:`sample covariance matrix` |end-span| is defined :red:`w.r.t. the feature vectors` as

.. math::
 \begin{aligned}
 {\Sigma }\left( {\mathbf{X}} \right) &= \left( {\begin{array}{*{20}{c}}
 {\operatorname{cov} \left( {{{\mathbf{x}}_1},{{\mathbf{x}}_1}} \right)}& \cdots &{\operatorname{cov} \left( {{{\mathbf{x}}_1},{{\mathbf{x}}_M}} \right)} \\
 \vdots & \ddots & \vdots  \\
 {\operatorname{cov} \left( {{{\mathbf{x}}_M},{{\mathbf{x}}_1}} \right)}& \cdots &{\operatorname{cov} \left( {{{\mathbf{x}}_M},{{\mathbf{x}}_M}} \right)}
 \end{array}} \right) \hfill \\
 &= \frac{1}{{N - 1}}\left( {\begin{array}{*{20}{c}}
 {{{\left( {{{\mathbf{x}}_1} - \overline {{{\mathbf{x}}_1}} } \right)}^{\text{T}}}\left( {{{\mathbf{x}}_1} - \overline {{{\mathbf{x}}_1}} } \right)}& \cdots &{{{\left( {{{\mathbf{x}}_1} - \overline {{{\mathbf{x}}_1}} } \right)}^{\text{T}}}\left( {{{\mathbf{x}}_M} - \overline {{{\mathbf{x}}_M}} } \right)} \\
 \vdots & \ddots & \vdots  \\
 {{{\left( {{{\mathbf{x}}_M} - \overline {{{\mathbf{x}}_M}} } \right)}^{\text{T}}}\left( {{{\mathbf{x}}_1} - \overline {{{\mathbf{x}}_1}} } \right)}& \cdots &{{{\left( {{{\mathbf{x}}_M} - \overline {{{\mathbf{x}}_M}} } \right)}^{\text{T}}}\left( {{{\mathbf{x}}_M} - \overline {{{\mathbf{x}}_M}} } \right)}
 \end{array}} \right) \hfill \\
 &= \frac{1}{{N - 1}}\left( {{\mathbf{X}} - {\mathbf{\bar X}}} \right){\left( {{\mathbf{X}} - {\mathbf{\bar X}}} \right)^{\text{T}}} \hfill \\
 \end{aligned} 
 :label: eq_sample_cov

where :math:`{\overline {{{\mathbf{x}}_i}} ^{\text{T}}} = \frac{1}{N}\mathop \sum \limits_{j = 1}^N {x_{i,j}}{1^{\text{T}}} = \left( {\frac{1}{N}\mathop \sum \limits_{j = 1}^N {x_{i,j}}, \ldots ,\frac{1}{N}\mathop \sum \limits_{j = 1}^N {x_{i,j}}} \right)`
(the same mean value repeating itself for :math:`N` times)
and :math:`{\mathbf{\bar X}} = \left( {\begin{array}{*{20}{c}}{\overline {{{\mathbf{x}}_1}} } \\\vdots  \\{\overline {{{\mathbf{x}}_M}} }\end{array}} \right)`,
and the diagonal elements are  |exdef| :index:`sample variances` |end-span| that can be denoted as :math:`\operatorname{var}\left( \mathbf{x}_{i} \right) := \operatorname{cov}\left( \mathbf{x}_{i},\mathbf{x}_{i} \right), i=1,...,M`.
The sum of variances, or the trace of the covariance matrix, is called the  |def| :index:`total variance` |end-span| of :math:`X`.
In addition, :emp:`note` :math:`\operatorname{cov} \left( {{\mathbf{x}},{\mathbf{y}}} \right)` is a value, while :math:`Œ£(x,y)` is a :math:`2√ó2` matrix.


|para|  |exdef| :index:`Pearson's correlation` |end-span| is the normalized version of covariance, defined as
:math:`\operatorname{corr}\left( X,Y \right) = \frac{\operatorname{cov}\left( X,Y \right)}{\sqrt{\operatorname{var}\left( X \right)}\sqrt{\operatorname{var}\left( Y \right)}}`
for two scalar RVs :math:`X,Y`, and
:math:`\operatorname{corr}\left( \mathbf{x},\mathbf{y} \right) = \frac{\operatorname{cov}\left( \mathbf{x},\mathbf{y} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x} \right)}\sqrt{\operatorname{var}\left( \mathbf{y} \right)}}`
for two samples :math:`\mathbf{x},\mathbf{y}`. A  |def| :index:`correlation matrix` |end-span| for a
RV vector :math:`X = \begin{pmatrix} X_{1} \\  \vdots \\ X_{M} \\ \end{pmatrix}` or a sample matrix :math:`\mathbf{X} = \begin{pmatrix} \mathbf{x}_{1}^{\rm{T}} \\  \vdots \\ \mathbf{x}_{M}^{\rm{T}} \\ \end{pmatrix}` are just replacing
:math:`\operatorname{cov}\left( \cdot , \cdot \right)` elements in (1?‚Ç¨?1) and
(1?‚Ç¨?2) by corresponding
:math:`\operatorname{corr}\left( \cdot , \cdot \right)`. Since
:math:`\operatorname{corr}\left( X,X \right) = 1` and
:math:`\operatorname{corr}\left( \mathbf{x},\mathbf{x} \right) = 1`, then |result| the diagonal elements of a correlation matrix will all be 1  |end-span|. Let
:math:`\Sigma_{\text{corr}}` denote a correlation matrix, we have

.. math::
 \operatorname{\Sigma_{\text{corr}}} \left( X \right) = \begin{pmatrix}
 1 & \cdots & \frac{\operatorname{cov}\left( X_{1},X_{M} \right)}{\sqrt{\operatorname{var}\left( X_{1} \right)}\sqrt{\operatorname{var}\left( X_{M} \right)}} \\
 \vdots & \ddots & \vdots \\
 \frac{\operatorname{cov}\left( X_{M},X_{1} \right)}{\sqrt{\operatorname{var}\left( X_{M} \right)}\sqrt{\operatorname{var}\left( X_{1} \right)}} & \cdots & 1 \\
 \end{pmatrix} = \begin{pmatrix}
 1 & \cdots & \operatorname{E}{\lbrack\frac{\left( X_{1}\mathbb{- E}X_{1} \right)}{\sqrt{\operatorname{var}\left( X_{1} \right)}}\frac{\left( X_{M}\mathbb{- E}X_{M} \right)}{\sqrt{\operatorname{var}\left( X_{M} \right)}}\rbrack} \\
 \vdots & \ddots & \vdots \\
 \operatorname{E}{\lbrack\frac{\left( X_{M}\mathbb{- E}X_{M} \right)}{\sqrt{\operatorname{var}\left( X_{M} \right)}}\frac{\left( X_{1}\mathbb{- E}X_{1} \right)}{\sqrt{\operatorname{var}\left( X_{1} \right)}}\rbrack} & \cdots & 1 \\
 \end{pmatrix}

.. math::
 \Sigma_{\text{corr}}\left( \mathbf{X} \right) = \begin{pmatrix}
 1 & \cdots & \frac{\operatorname{cov}\left( \mathbf{x}_{1},\mathbf{x}_{M} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{1} \right)}\sqrt{\operatorname{var}\left( \mathbf{x}_{M} \right)}} \\
 \vdots & \ddots & \vdots \\
 \frac{\operatorname{cov}\left( \mathbf{x}_{M},\mathbf{x}_{1} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{1} \right)}\sqrt{\operatorname{var}\left( \mathbf{x}_{M} \right)}} & \cdots & 1 \\
 \end{pmatrix} = \frac{1}{N - 1}\begin{pmatrix}
 1 & \cdots & \frac{\left( \mathbf{x}_{1} - \overline{\mathbf{x}_{1}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{1} \right)}}^{\rm{T}}\frac{\left( \mathbf{x}_{M} - \overline{\mathbf{x}_{M}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{M} \right)}} \\
 \vdots & \ddots & \vdots \\
 \frac{\left( \mathbf{x}_{M} - \overline{\mathbf{x}_{M}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{M} \right)}}^{\rm{T}}\frac{\left( \mathbf{x}_{1} - \overline{\mathbf{x}_{1}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x}_{1} \right)}} & \cdots & 1 \\
 \end{pmatrix}


Given a RV :math:`X`, we can define
:math:`\widetilde{X} = \frac{\left( X - \mathbb{E}X \right)}{\sqrt{\operatorname{var}\left( X \right)}}`
as the  |exdef| :index:`standardized RV` |end-span| (or  |exdef| :index:`normalized RV` |end-span|) of :math:`X` of zero
expectation and unit variance; given a sample :math:`\mathbf{x}`, we can
define
:math:`\widetilde{\mathbf{x}} = \frac{\left( \mathbf{x} - \overline{\mathbf{x}} \right)}{\sqrt{\operatorname{var}\left( \mathbf{x} \right)}}`
as the  |exdef| :index:`standardized sample` |end-span| (or  |exdef| :index:`normalized sample` |end-span|) of
:math:`\mathbf{x}` with zero mean and unit variance. We thus can define  |exdef| :index:`standardize RV vector` |end-span| :math:`\widetilde{X} = \begin{pmatrix} {\widetilde{X}}_{1} \\  \vdots \\ {\widetilde{X}}_{M} \\ \end{pmatrix}` and  |exdef| :index:`standardized sample matrix` |end-span|
:math:`\widetilde{\mathbf{X}} = \begin{pmatrix} {\widetilde{\mathbf{x}}}_{1}^{\rm{T}} \\  \vdots \\ {\widetilde{\mathbf{x}}}_{M}^{\rm{T}} \\ \end{pmatrix}`, and therefore
|result| :math:`\operatorname{}\left( X \right) = \operatorname{E}{\lbrack{\widetilde{X}\widetilde{X}}^{\rm{T}}\rbrack},\Sigma_{\text{corr}}\left( \mathbf{X} \right) = \frac{1}{N - 1}\widetilde{\mathbf{X}}{\widetilde{\mathbf{X}}}^{\rm{T}}`,
where we see correlation matrix is just the concept of covariance matrix applied on standardize (normalized) RV or sample. |end-span|

|para|  |def| :index:`Cross-covariance matrix` |end-span| and  |def| :index:`cross-correlation matrix` |end-span| is a generalized concept of covariance matrix and correlation matrix that consider two RV vectors :math:`X,Y` or two sample matrices :math:`\mathbf{X},\mathbf{Y}`. For example, suppose :math:`X,Y` are of lengths :math:`M_{1},M_{2}`, following :eq:`eq_rv_cov`, the cross-covariance matrix for :math:`X,Y` is defined as a :math:`M_{1} \times M_{2}` matrix (:emp:`note` need not be a square matrix),

.. math::
 \begin{aligned} \operatorname{\Sigma}\left( X,Y \right)
 &= \begin{pmatrix}
 \operatorname{cov}{(X_{1},Y_{1})} & \cdots & \operatorname{cov}{(X_{1},Y_{M_{2}})} \\
 \vdots & \ddots & \vdots \\
 \operatorname{cov}{(X_{M_{1}},Y_{1})} & \cdots & \operatorname{cov}{(X_{M_{1}},Y_{M_{2}})} \\
 \end{pmatrix} \\
 &= \begin{pmatrix}
 \operatorname{E}{\lbrack\left( X_{1}\mathbb{- E}X_{1} \right)\left( Y_{1}\mathbb{- E}Y_{1} \right)\rbrack} & \cdots & \operatorname{E}{\lbrack\left( X_{1}\mathbb{- E}X_{1} \right)\left( Y_{M_{2}}\mathbb{- E}Y_{M_{2}} \right)\rbrack} \\
 \vdots & \ddots & \vdots \\
 \operatorname{E}{\lbrack\left( X_{M_{1}}\mathbb{- E}X_{M_{1}} \right)\left( Y_{1}\mathbb{- E}Y_{1} \right)\rbrack} & \cdots & \operatorname{E}{\lbrack\left( X_{M_{1}}\mathbb{- E}X_{M_{1}} \right)\left( Y_{M_{2}}\mathbb{- E}Y_{M_{2}} \right)\rbrack} \\
 \end{pmatrix} \\
 &= \colorbox{fact}{$\operatorname{E}{\lbrack{\left( X - \mathbb{E}X \right)\left( Y - \mathbb{E}Y \right)}^{\rm{T}}\rbrack}$}
 \end{aligned}

And all others can be defined in the same way, summarized as :math:`\colorbox{fact}{$\Sigma\left( \mathbf{X,Y} \right) = \frac{1}{N - 1}\left( \mathbf{X} - \overline{\mathbf{X}} \right)\left( \mathbf{Y} - \overline{\mathbf{Y}} \right)^{\rm{T}}$}`,
:math:`\colorbox{fact}{$\operatorname{}\left( X,Y \right) = \operatorname{E}{\lbrack{\widetilde{X}\widetilde{Y}}^{\rm{T}}\rbrack}$}`, and :math:`\colorbox{fact}{$\Sigma_{\text{corr}}\left( \mathbf{X,Y} \right) = \frac{1}{N - 1}\widetilde{\mathbf{X}}{\widetilde{\mathbf{Y}}}^{\rm{T}}$}`


.. caution:: 
 In machine learning problems, we are often given a data matrix :math:`{\mathbf{X}} = \left( {{{\mathbf{x}}_1}, \ldots ,{{\mathbf{x}}_N}} \right)`
 with the columns :math:`{{\mathbf{x}}_1}, \ldots ,{{\mathbf{x}}_N}` as data entries.
 The bold small-letter symbol ":math:`\mathbf{x}`" very often represents a data entry in the content of machine learning,
 but in statistics it often instead represents a feature vector (as in above discussion), and sometimes this causes confusion.
 Therefore, we note it is necessary to understand what ‚Äú:math:`\mathbf{x}`‚Äù represents in the context.

 The other possible confusion is about the "samples". It is possible both the data entries and feature vectors are referred to as samples in different contexts.
 We again :emp:`note` :red:`sample covariance is always w.r.t. the feature vectors, not data entries`,
 because it studies how the quantities of different features vary with each other.
 Therefore, the "samples" in :eq:`eq_sample_cov` refers to feature vectors.



|list-div|

|item| 


.. _property_cov_to_expectation:

:ititle:`Property 1-4.` :bemp:`Classic representation of covariance by expectation (or mean).`
^^^^^^^^^^^^^^^^^^^^
Using the fact that :math:`\colorbox{fact}{$\operatorname{cov}\left( X,Y \right)\mathbb{= E}XY - \mathbb{E}X\mathbb{E}Y$}` for scalar RVs :math:`X,Y`, the covariance matrix has another form

.. math::
 \operatorname{\Sigma}\left( X \right) = \begin{pmatrix}
 \mathbb{E}X_{1}^{2} - \mathbb{E}^{2}X_{1} & \cdots & \mathbb{E}{X_{1}X_{M}}\mathbb{- E}X_{1}\mathbb{E}X_{M} \\
 \vdots & \ddots & \vdots \\
 \mathbb{E}{X_{M}X_{1}}\mathbb{- E}X_{M}\mathbb{E}X_{1} & \cdots & \mathbb{E}X_{M}^{2} - \mathbb{E}^{2}X_{M} \\
 \end{pmatrix} = \colorbox{rlt}{$\mathbb{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}X\mathbb{E}^{\mathrm{T}}X$}

For two samples :math:`\mathbf{x}\sim X,\mathbf{y}\sim Y` where
:math:`\mathbf{x =}\left( x_{1}\mathbf{,\ldots,}x_{N} \right)\mathbf{,}\mathbf{y = (}y_{1}\mathbf{,\ldots,}y_{N}\mathbf{)}`,
we have

.. math::
 \left( \mathbf{x} - \overline{\mathbf{x}} \right)^{\mathrm{T}}\left( \mathbf{y} - \overline{\mathbf{y}} \right) = \mathbf{x}^{\mathrm{T}}\mathbf{y} - \mathbf{x}^{\mathrm{T}}\overline{\mathbf{y}} - {\overline{\mathbf{x}}}^{\mathrm{T}}\mathbf{y} + {\overline{\mathbf{x}}}^{\mathrm{T}}\overline{\mathbf{y}}

Let :math:`\overline{x} = \frac{\sum_{i = 1}^{N}{\mathbf{x(}i\mathbf{)}}}{N}`
and :math:`\overline{y} = \frac{\sum_{i = 1}^{N}{\mathbf{y(}i\mathbf{)}}}{N}`,
then

.. math::
 {\mathbf{x}^{\mathrm{T}}\overline{\mathbf{y}} = \sum_{i = 1}^{N}{\overline{y}\mathbf{x(}i\mathbf{)}} = \overline{y}\sum_{i = 1}^{N}{\mathbf{x(}i\mathbf{)}} = N\overline{x}\overline{y}}

.. math::
 {{\overline{\mathbf{x}}}^{\mathrm{T}}\mathbf{y} = \sum_{i = 1}^{N}{\overline{x}\mathbf{y(}i\mathbf{)}} = \overline{x}\sum_{i = 1}^{N}{\mathbf{y(}i\mathbf{)}} = N\overline{x}\overline{y}}

.. math::
 {{\overline{\mathbf{x}}}^{\mathrm{T}}\overline{\mathbf{y}} = \sum_{i = 1}^{N}{\overline{x}\overline{y}} = N\overline{x}\overline{y}}

Thus

.. math::
 \left( \mathbf{x} - \overline{\mathbf{x}} \right)^{\mathrm{T}}\left( \mathbf{y} - \overline{\mathbf{y}} \right) = \mathbf{x}^{\mathrm{T}}\mathbf{x +}N\overline{x}\overline{y} - 2N\overline{x}\overline{y} = \mathbf{x}^{\mathrm{T}}\mathbf{x -}N\overline{x}\overline{y} = \mathbf{x}^{\mathrm{T}}\mathbf{y -}{\overline{\mathbf{x}}}^{\mathrm{T}}\overline{\mathbf{y}}

which implies

.. math::
 \Sigma(\mathbf{X}) = \frac{1}{N - 1}\begin{pmatrix}
 \mathbf{x}_{1}^{\mathrm{T}}\mathbf{x}_{1}{\bf -}{\overline{\mathbf{x}_{1}}}^{\mathrm{T}}\overline{\mathbf{x}_{1}} & \cdots & \mathbf{x}_{1}^{\mathrm{T}}\mathbf{x}_{M}{\bf -}{\overline{\mathbf{x}_{1}}}^{\mathrm{T}}\overline{\mathbf{x}_{M}} \\
 \vdots & \ddots & \vdots \\
 \mathbf{x}_{M}^{\mathrm{T}}\mathbf{x}_{1}{\bf -}{\overline{\mathbf{x}_{M}}}^{\mathrm{T}}\overline{\mathbf{x}_{1}} & \cdots & \mathbf{x}_{M}^{\mathrm{T}}\mathbf{x}_{M}{\bf -}{\overline{\mathbf{x}_{M}}}^{\mathrm{T}}\overline{\mathbf{x}_{M}} \\
 \end{pmatrix} = \colorbox{rlt}{$\frac{1}{N - 1}\left( \mathbf{X}\mathbf{X}^{\mathrm{T}}{\bf -}\overline{\mathbf{X}}{\overline{\mathbf{X}}}^{\mathrm{T}} \right)$}

We can verify above inference directly works for cross-covariance, and therefore

.. math::
 \colorbox{result}{$\operatorname{\Sigma}\left( X,Y \right) = \mathbb{E}\mathrm{\lbrack}XY^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}X\mathbb{E}^{\rm{T}}Y,\Sigma\left( \mathbf{X,Y} \right) = \frac{1}{N - 1}\left( \mathbf{X}\mathbf{Y}^{\rm{T}}\mathbf{-}\overline{\mathbf{X}}{\overline{\mathbf{Y}}}^{\rm{T}} \right)$}


|end-item|

|item| 


.. _property_cov_invariant_to_centralization:

:ititle:`Property 1-5.` :bemp:`Invariance to centralization.`
^^^^^^^^^^^^^^^^^^^^
For any random vector :math:`X`, we have :math:`\colorbox{result}{$\Sigma\left( X - \mathbb{E}X \right) = \Sigma(X)$}`,
since :math:`\mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack = \mathbf{0}` and

.. math::
 \Sigma\left( X - \mathbb{E}X \right)
 = \mathbb{E}{\lbrack{\left( X - \mathbb{E}X - \mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack \right)\left( X - \mathbb{E}X - \mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack \right)}^{\mathrm{T}}\rbrack}
 = \mathbb{E}{\lbrack{\left( X - \mathbb{E}X \right)\left( X - \mathbb{E}X \right)}^{\mathrm{T}}\rbrack} = \Sigma(X)

Similarly :math:`\colorbox{result}{$\Sigma\left( \mathbf{X} - \overline{\mathbf{X}} \right) = \Sigma\left( \mathbf{X} \right)$}`,
because :math:`\mathbf{x} - \overline{\mathbf{x}} - \overline{\mathbf{x} - \overline{\mathbf{x}}} = \mathbf{x} - \overline{\mathbf{x}}`
for any sample :math:`\mathbf{x}`, and the result following by applying this on :eq:`eq_sample_cov`. For cross-covariance matrix, we have
:math:`\colorbox{result}{$\Sigma\left( X - \mathbb{E}X,Y - \mathbb{E}Y \right) = \Sigma\left( X,Y \right)$}`
and :math:`\colorbox{result}{$\Sigma\left( \mathbf{X} - \overline{\mathbf{X}},\mathbf{Y} - \overline{\mathbf{Y}} \right) = \Sigma\left( \mathbf{X,}\mathbf{Y} \right)$}`
for exactly the same reason.


|end-item|

|item| 


.. _theorem_cov_matrix_arithmetic_rules:

:ititle:`Theorem 1-5.` :bemp:`Matrix arithmetics of covariance matrix.`
^^^^^^^^^^^^^^^^^^^^
|theorem| Given :math:`X = \left( X_{1},\ldots,X_{n} \right)^{\mathrm{T}}`,
:math:`\operatorname{var}\left( \mathbf{Œ±}^{\rm{T}}X \right) = \operatorname{var}\left( X^{\rm{T}}\mathbf{Œ±} \right) = \mathbf{Œ±}^{\rm{T}}\operatorname{\Sigma}\left( X \right)\mathbf{Œ±}`  |end-span|.
:emp:`Note` :math:`Œ±^{\rm{T}}\Sigma{X}` is a scalar random variable, and

.. math::
 \mathbb{E}\left( \mathrm{\mathbf{Œ±}}^{\mathrm{T}}X \right)\mathbb{= E}\left( X^{\mathrm{T}}\mathrm{\mathbf{Œ±}} \right) = \mathrm{\mathbf{Œ±}}^{\mathrm{T}}\mathbb{E}X = \left( \mathbb{E}^{\mathrm{T}}X \right)\mathrm{\mathbf{Œ±}}

Also :math:`\mathrm{\mathbf{Œ±}}^{\mathrm{T}}X` is a scalar RV, and so
:math:`\left( \mathrm{\mathbf{Œ±}}^{\mathrm{T}}X \right)^{2}={\left( \mathrm{\mathbf{Œ±}}^{\mathrm{T}}X \right)\left( \mathrm{\mathbf{Œ±}}^{\mathrm{T}}X \right)}^{\mathrm{T}} = \mathrm{\mathbf{Œ±}}^{\mathrm{T}}XX^{\mathrm{T}}\mathrm{\mathbf{Œ±}}`.
Recall :math:`\operatorname{var} \left( X \right)=\mathbb{E}X^{2} - \mathbb{E}^{2}X`, then using :ref:`Property 1-4 <property_cov_to_expectation>`, we have

.. math::
 \operatorname{var} \left( \mathrm{\mathbf{Œ±}}^{\mathrm{T}}X \right) = \mathbb{E}\mathrm{\lbrack}\mathrm{\mathbf{Œ±}}^{\mathrm{T}}X\left( \mathrm{\mathbf{Œ±}}^{\mathrm{T}}X \right)^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}\left\lbrack \mathrm{\mathbf{Œ±}}^{\mathrm{T}}X \right\rbrack\mathbb{E}^{\mathrm{T}}\left\lbrack \mathrm{\mathbf{Œ±}}^{\mathrm{T}}X \right\rbrack = \mathrm{\mathbf{Œ±}}^{\mathrm{T}}\mathbb{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack}\mathrm{\mathbf{Œ±}} - \mathrm{\mathbf{Œ±}}^{\mathrm{T}}\mathbb{E}X\mathbb{E}^{\mathrm{T}}X\mathrm{\mathbf{Œ±}}=\mathrm{\mathbf{Œ±}}^{\mathrm{T}}\mathbf{(}\mathbb{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}X\mathbb{E}^{\mathrm{T}}X\mathrm{)}\mathrm{\mathbf{Œ±}}=\mathrm{\mathbf{Œ±}}^{\mathrm{T}}\operatorname{\Sigma}\left( X \right)\mathrm{\mathbf{Œ±}}

Similarly, using
:math:`\operatorname{cov} \left( X,Y \right)\mathbb{= E}XY - \mathbb{E}X\mathbb{E}Y`, we have
:math:`\colorbox{theorem}{$\operatorname{cov} \left( \mathrm{\mathbf{Œ±}}^{\mathrm{T}}X,\mathbf{Œ≤}^\mathrm{T}Y \right) = \mathrm{\mathbf{Œ±}}^\mathrm{T}\operatorname{\Sigma}\left( X \right)\mathbf{Œ≤}$}`.
Further, if we let
:math:`\mathbf{A}\mathbf{= (}\mathbf{a}_{1}\mathbf{,\ldots,}\mathbf{a}_{n}\mathbf{)}`,
then
:math:`\colorbox{theorem}{$\Sigma\left( \mathbf{A}^\mathrm{T}X \right) = \mathbf{A}^\mathrm{T}\operatorname{\Sigma}\left( X \right)\mathbf{A}$}`,
since
:math:`\Sigma\left( \mathrm{\mathbf{Œ±}}_{i}X\mathbf{,}\mathrm{\mathbf{Œ±}}_{j}X \right) = \mathrm{\mathbf{Œ±}}_{i}^\mathrm{T}\operatorname{\Sigma}\left( X \right)\mathrm{\mathbf{Œ±}}_{j}`; for the same reason, we have
:math:`\Sigma\left( \mathbf{A}^{\rm{T}}X,\mathbf{B}^{\rm{T}}Y \right) = \mathbf{A}^{\rm{T}}\operatorname{\Sigma}\left( X \right)\mathbf{B}`
for cross-covariance.

On the other hand, given
:math:`\mathbf{X}=(\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\mathbf{)}`, we have that
:math:`\colorbox{theorem}{$\operatorname{var}\left( \mathbf{Œ±}^{\rm{T}}\mathbf{X} \right) = \operatorname{var}\left( \mathbf{X}\mathbf{Œ±} \right) = \mathbf{Œ±}^{\rm{T}}\operatorname{\Sigma}\left( \mathbf{X} \right)\mathbf{Œ±}$}`.
First check

.. math::
 \left\{ \begin{matrix}
 \overline{\mathbf{\text{XŒ±}}} = \overline{\sum_{i = 1}^{n}{\mathrm{\mathbf{Œ±}}\left( i \right)\mathbf{x}_{i}}} = \frac{1}{m}\sum_{j = 1}^{m}{\sum_{i = 1}^{n}{\mathrm{\mathbf{Œ±}}\left( i \right)\mathbf{x}_{i}(j)}} \\
 \overline{\mathbf{X}}\mathrm{\mathbf{Œ±}}=\sum_{i = 1}^{n}{\mathrm{\mathbf{Œ±}}\left( i \right)\overline{\mathbf{x}_{i}}} = \sum_{i = 1}^{n}\left( \mathrm{\mathbf{Œ±}}\left( i \right) \times \frac{1}{m}\sum_{j = 1}^{m}{\mathbf{x}_{i}\left( j \right)} \right) = \frac{1}{m}\sum_{i = 1}^{n}\left( \sum_{j = 1}^{m}{\mathrm{\mathbf{Œ±}}\left( i \right)\mathbf{x}_{i}\left( j \right)} \right) \\
 \end{matrix} \Rightarrow \colorbox{rlt}{$\overline{\mathbf{\text{XŒ±}}} = \overline{\mathbf{X}}\mathrm{\mathbf{Œ±}}$} \right. 

Then we have

.. math::
 \operatorname{var} \left( \mathbf{\text{XŒ±}} \right) = \frac{1}{n + 1}\left( \left( \mathbf{\text{XŒ±}} \right)\mathrm{T}\left( \mathbf{\text{XŒ±}} \right)\mathbf{-}{\overline{\mathbf{\text{XŒ±}}}}^\mathrm{T}\overline{\mathbf{\text{XŒ±}}} \right) = \frac{1}{n + 1}\left( \mathrm{\mathbf{Œ±}}^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{XŒ± -}\mathrm{\mathbf{Œ±}}^\mathrm{T}{\overline{\mathbf{X}}}^\mathrm{T}\overline{\mathbf{X}}\mathrm{\mathbf{Œ±}} \right) = \mathrm{\mathbf{Œ±}}^\mathrm{T}\Sigma\left( \mathbf{X} \right)\mathrm{\mathbf{Œ±}}

By similar calculation,
:math:`\operatorname{cov} \left( \mathbf{XŒ±,XŒ≤} \right) = \mathrm{\mathbf{Œ±}}^\mathrm{T}\Sigma\left( \mathbf{X} \right)\mathbf{Œ≤}`.
Let :math:`\mathbf{Y} = \mathbf{\text{XA}}` for any matrix
:math:`\mathbf{A} = (\mathbf{a}_{1}\mathbf{,\ldots,}\mathbf{a}_{n}\mathbf{)}`,
then
:math:`\colorbox{theorem}{$\Sigma(\mathbf{\text{A}}^{\rm{T}}\mathbf{\mathrm{X}}) = \mathbf{A}^\mathrm{T}\Sigma(\mathbf{X})\mathbf{A}$}`,
since :math:`\ \Sigma\left( \mathbf{a}_{i}^{\rm{T}}\mathbf{X}\mathbf{,}\mathbf{a}_{j}^{\rm{T}}\mathbf{X} \right) = \mathbf{a}_{i}^{\rm{T}}\operatorname{\Sigma}\left( \mathbf{X} \right)\mathbf{a}_{j}`;
of course we also have |theorem| :math:`\Sigma(\mathbf{A}^{\text{T}}\mathbf{X,}\mathbf{B}^{\rm{T}}\mathbf{Y}) = \mathbf{A}^{\rm{T}}\Sigma(\mathbf{X,Y})\mathbf{B}` for cross-covariance  |end-span|.

For the same reason, |theorem| a square cross-covariance matrix is positive-semidefinite |end-span|.



|end-item|

|item| 


.. _theorem_cov_semipositiveness:

:ititle:`Theorem 1-6.` :bemp:`Positive definiteness of covariance matrix.`
^^^^^^^^^^^^^^^^^^^^
|theorem| Covariance matrix is clearly symmetric, and moreover they are semi-positive definite |end-span|, since for any constant vector :math:`\mathbf{Œ±}`, using :ref:`Theorem 1-5 <theorem_cov_matrix_arithmetic_rules>`, we have

.. math::
 \begin{aligned}
 {{\mathbf{Œ± }}^{\text{T}}}\left( {\Sigma \left( X \right)} \right){\mathbf{Œ± }} &= \Sigma \left( {{{\mathbf{Œ± }}^{\text{T}}}X} \right) \hfill \\
 &= \mathbb{E}\left[ {\left( {{{\mathbf{Œ± }}^{\text{T}}}X - \mathbb{E}{{\mathbf{Œ± }}^{\text{T}}}X} \right){{\left( {{{\mathbf{Œ± }}^{\text{T}}}X - \mathbb{E}{{\mathbf{Œ± }}^{\text{T}}}X} \right)}^{\text{T}}}} \right] = \mathbb{E}\left[ {\left( {{{\mathbf{Œ± }}^{\text{T}}}X - {{\mathbf{Œ± }}^{\text{T}}}\mathbb{E}X} \right){{\left( {{{\mathbf{Œ± }}^{\text{T}}}X - {{\mathbf{Œ± }}^{\text{T}}}\mathbb{E}X} \right)}^{\text{T}}}} \right] \hfill \\
 &= \mathbb{E}\left[ {{{\mathbf{Œ± }}^{\text{T}}}\left( {X - \mathbb{E}X} \right){{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}{\mathbf{Œ± }}} \right] = \mathbb{E}\left[ {{{\left( {{{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}{\mathbf{Œ± }}} \right)}^{\text{T}}}\left( {{{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}{\mathbf{Œ± }}} \right)} \right] \geqslant 0 \hfill \\
 \end{aligned} 

For sample covariance matrix, check that (omiting the coefficient)

.. math::
 \mathbf{Œ±}^{\rm{T}}\left( \Sigma\left( \mathbf{X} \right) \right)\mathbf{Œ±}
 =\Sigma\left( \mathbf{\text{XŒ±}} \right) \propto \left( \mathbf{\text{XŒ±}} - \overline{\mathbf{\text{XŒ±}}} \right)^{\rm{T}}\left( \mathbf{\text{XŒ±}} - \overline{\mathbf{\text{XŒ±}}} \right) \geq 0


|end-item|

|item| 


.. _property_cov_as_rank1_sum:

:ititle:`Property 1-6.` :bemp:`Sample covariance represneted by rank-1 sum.`
^^^^^^^^^^^^^^^^^^^^
Recall :math:`\mathbf{X} = \begin{pmatrix} \mathbf{x}_{1}^{\rm{T}} \\  \vdots \\ \mathbf{x}_{M}^{\rm{T}} \\ \end{pmatrix}` are feature vectors, and the covariance matrix in :eq:`eq_sample_cov` is defined w.r.t. the feature vectors. Suppose
:math:`\mathbf{X} = \left( \mathbf{ùìç}_{1},\ldots,\mathbf{ùìç}_{N} \right)`
where :math:`\mathbf{ùìç}_{1},\ldots,\mathbf{ùìç}_{N}` are columns of
:math:`\mathbf{X}` as data entries, and similarly :math:`\mathbf{Y} = \left( \mathbf{ùìé}_{1},\ldots,\mathbf{ùìé}_{N} \right)`
and let :math:`\overline{\mathbf{ùìç}} = \frac{1}{N}\sum_{i = 1}^{N}\mathbf{ùìç}_{i}` and :math:`\overline{\mathbf{ùìé}} = \frac{1}{N}\sum_{i = 1}^{N}\mathbf{ùìé}_{j}` be the mean vector of all data entries. Then we can show
:math:`\Sigma\left( \mathbf{X} \right)` or :math:`\Sigma\left( \mathbf{X,Y} \right)` can also be represented by
sum of rank-1 addends dependent on the data entries (rather than the feature vectors) as

.. math::
 \Sigma\left( \mathbf{X,Y} \right) = \frac{1}{N - 1}\sum_{k = 1}^{N}{\left( \mathbf{ùìç}_{k} - \overline{\mathbf{ùìç}} \right)\left( \mathbf{ùìé}_{k} - \overline{\mathbf{ùìé}} \right)^{\rm{T}}}
 :label: eq_cov_rank1_data_entry_representation

Let :math:`\mathbf{\Sigma} := \Sigma\left( \mathbf{X},\mathbf{Y} \right)` for convenience.
By :eq:`eq_sample_cov`, we have (omitting the coefficient)

.. math::
 \mathbf{\Sigma}\left( i,j \right) \propto \left( \mathbf{x}_{i} - \overline{\mathbf{x}_{i}} \right)^{\rm{T}}\left( \mathbf{y}_{j} - \overline{\mathbf{y}_{j}} \right)

Note :math:`\mathbf{ùìç}_{j}\left( i \right) = x_{i,j} = \mathbf{x}_{i}\left( j \right)`
and :math:`\overline{\mathbf{ùìç}}\left( i \right)\mathbf{=}\overline{x_{i}} = \frac{1}{N}\sum_{k = 1}^{N}x_{i,k}`, :math:`\overline{\mathbf{ùìé}}\left( j \right)\mathbf{=}\overline{y_{j}} = \frac{1}{N}\sum_{j = 1}^{N}y_{j,k}`,
we have

.. math::
 \begin{aligned}
 & \mathbf{\Sigma}\left( i,j \right) \propto \left( \mathbf{x}_{i} - \overline{\mathbf{x}_{i}} \right)^{\rm{T}}\left( \mathbf{y}_{j} - \overline{\mathbf{y}_{j}} \right)
 = \sum_{k = 1}^{N}{\left( \mathbf{x}_{i}\left( k \right) - \overline{x_{i}} \right)\left( \mathbf{y}_{j}\left( k \right) - \overline{y_{j}} \right)} \\
 &= \sum_{k = 1}^{N}{\left( \mathbf{ùìç}_{k}\left( i \right) - \overline{x_{i}} \right)\left( \mathbf{ùìé}_{k}\left( j \right) - \overline{y_{j}} \right)}
 = \sum_{k = 1}^{N}{\left( \mathbf{ùìç}_{k}\left( i \right) - \overline{\mathbf{ùìç}}\left( i \right) \right)\left( \mathbf{ùìé}_{k}\left( j \right) - \overline{\mathbf{ùìé}}\left( j \right) \right)} \\
 &= \sum_{k = 1}^{N}{\left( \left( \mathbf{ùìç}_{k} - \overline{\mathbf{ùìç}} \right)\left( \mathbf{ùìé}_{k} - \overline{\mathbf{ùìé}} \right)^{\rm{T}} \right)\left( i,j \right)} \end{aligned}
 :label: eq_cov_elementwise_representation_by_data_entries

The above identity immediately implies :eq:`eq_cov_rank1_data_entry_representation`.


.. _corollary_cov_as_mixed:

:ititle2:`Corollary 1-4.` :math:`\colorbox{result}{$\mathbf{X}\mathbf{X}^{\rm{T}} = \left( N - 1 \right)\Sigma\left( \mathbf{X} \right) + N\overline{\mathbf{ùìç}}{\overline{\mathbf{ùìç}}}^{\rm{T}}$}`.
Check that :math:`\mathbf{\text{XX}}^{\rm{T}} = \begin{pmatrix} \mathbf{x}_{1}^{\rm{T}}\mathbf{}_{1} & \cdots & \mathbf{x}_{1}^{\rm{T}}\mathbf{x}_{M} \\  \vdots & \ddots & \vdots \\ \mathbf{x}_{M}^{\rm{T}}\mathbf{x}_{1} & \cdots & \mathbf{x}_{M}^{  T}\mathbf{x}_{M} \\ \end{pmatrix}`, and by the same inference as :eq:`eq_cov_elementwise_representation_by_data_entries`
we have |tip| :math:`\mathbf{\text{XX}}^{\rm{T}} = \sum_{k = 1}^{N}{\mathbf{ùìç}_{k}{\mathbf{ùìç}_{k}}^{\rm{T}}}` |tiptxt| This is the rank-1 decomposition of any symmetric matrix in linear algebra. |end-span|,
and then use :eq:`eq_cov_rank1_data_entry_representation`, we have

.. math::
 \begin{aligned} \mathbf{\text{XX}}^{\rm{T}} &= \sum_{k = 1}^{N}{\mathbf{ùìç}_{k}{\mathbf{ùìç}_{k}}^{\rm{T}}} \\
 &= \sum_{k = 1}^{N}{\left( \mathbf{ùìç}_{k} - \overline{\mathbf{ùìç}} \right)({\mathbf{ùìç}_{k} - \overline{\mathbf{ùìç}})}^{\rm{T}}} + \sum_{k = 1}^{N}\left( \overline{\mathbf{ùìç}}\mathbf{ùìç}_{k}^{\rm{T}} + \mathbf{ùìç}_{k}{\overline{\mathbf{ùìç}}}^{\rm{T}} - \overline{\mathbf{ùìç}}{\overline{\mathbf{ùìç}}}^{\rm{T}} \right) \\
 &= \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)\mathbf{+}\overline{\mathbf{ùìç}}\left( \sum_{k = 1}^{N}\mathbf{ùìç}_{k}^{\rm{T}} \right) + \left( \sum_{k = 1}^{N}\mathbf{ùìç}_{k} \right){\overline{\mathbf{ùìç}}}^{\rm{T}} - N\overline{\mathbf{ùìç}}{\overline{\mathbf{ùìç}}}^{\rm{T}} \\
 &= \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)\mathbf{+}N\overline{\mathbf{ùìç}}{\overline{\mathbf{ùìç}}}^{\rm{T}} + N\overline{\mathbf{ùìç}}{\overline{\mathbf{ùìç}}}^{\rm{T}} - N\overline{\mathbf{ùìç}}{\overline{\mathbf{ùìç}}}^{\rm{T}} \\
 &= \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)\mathbf{+}N\overline{\mathbf{ùìç}} {\overline{\mathbf{ùìç}}}^{\rm{T}} \end{aligned}


|end-item|

|item| 


.. _theorem_cov_as_rank1_sum:

:ititle:`Theorem 1-7.` :bemp:`Block decomposition of covariance matrix.`
^^^^^^^^^^^^^^^^^^^^
Again consider :math:`\mathbf{X} = \left( \mathbf{ùìç}_{1},\ldots,\mathbf{ùìç}_{N} \right)`
where :math:`\mathbf{ùìç}_{1},\ldots,\mathbf{ùìç}_{N}` are data entries, and
:math:`\overline{\mathbf{ùìç}} = \frac{1}{N}\sum_{i = 1}^{N}\mathbf{ùìç}_{i}`.
Suppose :math:`\mathbf{ùìç}_{1},\ldots,\mathbf{ùìç}_{N}` are categorized into
:math:`K` non-overlapping groups :math:`G_{1},\ldots,G_{k}`. Let
:math:`N_{1},\ldots,N_{k}` be the size of the groups, and
:math:`{\overline{\mathbf{ùìç}}}^{k} = \frac{1}{N_{k}}\sum_{\mathbf{ùìç} \in G_{k}}^{}\mathbf{ùìç}`, then


.. math::
 \colorbox{theorem}{$\left( N - 1 \right)\Sigma\left( \mathbf{X} \right)
 = \color{conn1}{\sum_{k = 1}^{K}{\left( N_{k} - 1 \right)\Sigma\left( \mathbf{X}_{k} \right)}}
 + \color{conn2}{\sum_{k = 1}^{K}{N_{k}\left( {\overline{\mathbf{ùìç}}}^{k} - \overline{\mathbf{ùìç}} \right)\left( {\overline{\mathbf{ùìç}}}^{k} - \overline{\mathbf{ùìç}} \right)^{\rm{T}}}}$}
 :label: eq_cov_block_decomposition

This is because

.. math::
 \begin{aligned}
 \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)
 &= \sum_{j = 1}^{N}{\left( \mathbf{ùìç}_{j} - \overline{\mathbf{ùìç}} \right)\left( \mathbf{ùìç}_{j} - \overline{\mathbf{ùìç}} \right)^{\rm{T}}} \\
 &= \sum_{k = 1}^{K}{\sum_{\mathbf{ùìç} \in G_{k}}^{}{\left( \mathbf{ùìç} - \overline{\mathbf{ùìç}} \right)\left( \mathbf{ùìç} - \overline{\mathbf{ùìç}} \right)^{\rm{T}}}} \\
 &= \sum_{k = 1}^{K}{\sum_{\mathbf{ùìç} \in G_{k}}^{}{\left( \mathbf{ùìç} - {\overline{\mathbf{ùìç}}}^{k}\mathbf{+}{\overline{\mathbf{ùìç}}}^{k}\mathbf{-}\overline{\mathbf{ùìç}} \right)\left( \mathbf{ùìç} - {\overline{\mathbf{ùìç}}}^{k}\mathbf{+}{\overline{\mathbf{ùìç}}}^{k}\mathbf{-}\overline{\mathbf{ùìç}} \right)^{\rm{T}}}} \\
 &= \sum_{k = 1}^{K}{\sum_{\mathbf{ùìç} \in G_{k}}^{}{\left( \mathbf{ùìç} - {\overline{\mathbf{ùìç}}}^{k} \right)\left( \mathbf{ùìç} - {\overline{\mathbf{ùìç}}}^{k} \right)^{\rm{T}}}} + \sum_{k = 1}^{K}{\sum_{\mathbf{ùìç} \in G_{k}}^{}{\left( {\overline{\mathbf{ùìç}}}^{k}\mathbf{-}\overline{\mathbf{ùìç}} \right)\left( {\overline{\mathbf{ùìç}}}^{k}\mathbf{-}\overline{\mathbf{ùìç}} \right)^{\rm{T}}}} + \sum_{k = 1}^{K}{\sum_{\mathbf{ùìç} \in G_{k}}^{}{\left( \mathbf{ùìç} - {\overline{\mathbf{ùìç}}}^{k} \right)\left( {\overline{\mathbf{ùìç}}}^{k}\mathbf{-}\overline{\mathbf{ùìç}} \right)^{\rm{T}}}} + \sum_{k = 1}^{K}{\sum_{\mathbf{ùìç} \in G_{k}}^{}{\left( {\overline{\mathbf{ùìç}}}^{k}\mathbf{-}\overline{\mathbf{ùìç}} \right)\left( \mathbf{ùìç} - {\overline{\mathbf{ùìç}}}^{k} \right)^{\rm{T}}}}
 \end{aligned}
 

where we have

.. math::
 \sum_{k = 1}^{K}{\sum_{\mathbf{ùìç} \in G_{k}}^{}{\left( \mathbf{ùìç} - {\overline{\mathbf{ùìç}}}^{k} \right)\left( \mathbf{ùìç} - {\overline{\mathbf{ùìç}}}^{k} \right)^{\rm{T}}}}
 = \color{conn1}{\sum_{k = 1}^{K}{\left( N_{k} - 1 \right)\Sigma\left( \mathbf{X}_{k} \right)}}

.. math::
 \sum_{k = 1}^{K}{\sum_{\mathbf{ùìç} \in G_{k}}^{}{\left( {\overline{\mathbf{ùìç}}}^{k}\mathbf{-}\overline{\mathbf{ùìç}} \right)\left( {\overline{\mathbf{ùìç}}}^{k}\mathbf{-}\overline{\mathbf{ùìç}} \right)^{\rm{T}}}}
 = \color{conn2}{\sum_{k = 1}^{K}{N_{k}\left( {\overline{\mathbf{ùìç}}}^{k} - \overline{\mathbf{ùìç}} \right)\left( {\overline{\mathbf{ùìç}}}^{k} - \overline{\mathbf{ùìç}} \right)^{\rm{T}}}}

The other two summations are zero matrices. For example,

.. math::
 \sum_{k = 1}^{K}{\sum_{\mathbf{ùìç} \in G_{k}}^{}{\left( \mathbf{ùìç} - {\overline{\mathbf{ùìç}}}^{k} \right)\left( {\overline{\mathbf{ùìç}}}^{k}\mathbf{-}\overline{\mathbf{ùìç}} \right)^{\rm{T}}}}
 = \sum_{k = 1}^{K}\left( \left( \sum_{\mathbf{ùìç} \in G_{k}}^{}\left( \mathbf{ùìç} - {\overline{\mathbf{ùìç}}}^{k} \right) \right)\left( {\overline{\mathbf{ùìç}}}^{k}\mathbf{-}\overline{\mathbf{ùìç}} \right)^{\rm{T}} \right)
 = \sum_{k = 1}^{K}{\left( N_{k}{\overline{\mathbf{ùìç}}}^{k}\mathbf{-}N_{k}{\overline{\mathbf{ùìç}}}^{k} \right)\left( {\overline{\mathbf{ùìç}}}^{k}\mathbf{-}\overline{\mathbf{ùìç}} \right)^{\rm{T}}}
 = \mathbf{O}
 

Now the identity of :eq:`eq_cov_block_decomposition` is obvious.

|end-item| |end-list-div|


